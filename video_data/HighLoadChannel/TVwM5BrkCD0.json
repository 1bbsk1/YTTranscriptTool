{
  "video_id": "TVwM5BrkCD0",
  "channel": "HighLoadChannel",
  "title": "MinIO — что изменилось за год / Алексей Плетнёв (Базис-Центр+)",
  "views": 2850,
  "duration": 2409,
  "published": "2024-04-17T01:10:17-07:00",
  "text": "соответственно группа будет размазываться по всем 16 нодам и на самом деле это выглядит логично только по той причине что тот кто писал документацию подобрал вот такой красивый пример я подобрал другой пример менее красивый здесь у нас 6 нот каждой Ноди подключено полсотни дисков и того у нас получается 30 их соответственно строка для запуска будет выглядеть следующим образом группы здесь выберутся автоматически по 15 дисков то есть выбирается просто мы делим 300 на 16 получаем 18,75 не целая делим на 15 Ага 20 целые соответственно у нас будет 20 групп наших дисков и совсем не понятно абсолютно непонятно как эти диски Будут распределяться между нодами и может ли вообще произойти такая ситуация что все диски в группе 15 окажутся на одной ноте которая в самый неподходящий момент ляжет документация на этот вопрос ответ не даёт разработчики на githaby тоже на этот вопрос ответ не дают при условии оплаты подписки минимум 1.000 долларов в месяц они могут провести архитектурный реввью и подсказать что как лучше сделать но это не наш случай То есть если мы выбираем минимум и пытаемся развернуть своими силами нам бы проще было уже отнести деньги к какому-нибудь облачному провайдеру Чем платить 1.000 долларов и ещё разбираться что-то докручивать что мы сделали мы начали использовать zfs мы разбили все группы все диски которые у нас есть на группы таким образом чтобы количество групп на каждой ноде не превышала четырех и в случае когда мы используем 4 нода получится максимум 16 Сейчас дальше я вам покажу как это собственно выглядит Но прежде расскажу почему именно за FS потому что ко мне приходила на почту вопрос почему именно за tfs мы выбрали а не например lv и рассматривали ли мы лвм да конечно рассматривали и более того в документации минимум прямо сказано Э что лучше не использовать никаких прослоек между ними и дисками в виде всяких рейдов каких-то ZF посредников лучше использовать э чистый там xfs-системы соответственно мы использовали xfs поверх lvm Но для меню выбрали zfs все-таки во-первых по рекомендациям куратор моего предыдущего прошлогоднего доклада Ивана гарков сказал что у них в компании ztfs подменил уже успешно эксплуатируется и в принципе рекомендовал мне его использовать Я на тот момент вообще не знал что такое zfs начал в него погружаться и меня захватила его такая идеологическая простота то есть э-э для того чтобы в нём что-то сделать собрать пул посмотреть статистику достаточно использовать одну всего лишь команду Ну и в-третьих ztfs изначально создан это коммерческая файловая система которая Создана для больших данных она название не так расшифровывается байт файл собственно нам Она по всем параметрам подошла а вот это вот скрин моей шпаргалки с компьютера потому что до сих пор без неё я не могу на самом деле расширять лвм дома то есть на самом деле люди даже на хаббре статьи пишут на тему Как расширить вон там для zfs мне такого не пришлось писать просто запул и дальше в хелпе всё понятно А если в системе можно 100 ручек там крутилок заменить одной кнопкой ты на неё нажимаешь и понятно что происходит меня Такое устраивает больше Ну и еще один минус lvm с которым мы естественно у нас есть другие проекты есть другие сервера на которых мы используем lvm у него есть динамический ресайзинг но очень часто такое случается что ты расширил диск сервис работает работает и перестаёт работать Почему конкретно мы пока не разобрались Но для обмена своих написали некий гайд который говорит что по возможности после расширения диска лучше сервер всё-таки ребутнуть ещё одна плюшка ztfs которая мне очень понравилась э так как у нас речь идёт всё-таки о хранилище для хранилища одним из важных показателей является и Ops и мне хочется быстро посмотреть насколько загружены мои диски я вот лично в линуксе команд которые позволяют посмотреть и опции не знаю простейшие там А чтоб её топ а топ они этого не показывают Они показывают поток записи то есть сколько в мегабайтах секунд пишется но и опция не показывают Вот вы видите что одной командой здесь я могу посмотреть и опции для каждой из моих групп Ну собственно не для того чтобы ztfs восхвалять ей сюда вышел просто объяснил почему именно она здесь Некоторые из её возможности дополнительные мы их используем соответственно мы используем raidz Следующий вопрос который мне задавали Как лучше проводить обновление кластера Ведь я говорил что нельзя обновлять узлы один за другим необходимо только остановить обновить всё перезапустить но согласитесь надолго наш кластер Production останавливать не хочется и у меня есть одна родная утилита она называется MC через неё выполняется большинство операций из командной строки и у неё есть команда MC админ апдейт и соответственно название нашего кластера кажется всё просто она обновляет скачивает обновление на все узлы перезапускает и мы имеем новую версию видео но Есть несколько но Первое это скорость скачивания дистрибутива на минимум в гитхабе уже давно он Давно висит задача по поводу того что надо что-то с этим делать они меняли своего облачного провайдера с АВС на овh но Профит это не принесло и люди по всему миру продолжает жаловаться на то что там 100 мб инсталляции можно сидеть и качать полчаса и согласитесь неприемлемо когда на разные ноды он будет приходить с абсолютно разные ещё и такой медленной скоростью второй момент после того как вы в первый раз запустите МЦ админ апдейт он скорее у вас не пройдёт а происходит Это потому что минимум по умолчанию из коробки ставится в папку с бинарниками то есть usr локал Бин естественно прав на запись у него туда нету И вот посылки которые здесь ведётся обсуждение которое сводится вкратце к тому э что обычно надо используется то есть да это дырка в безопасности давать минимум писать папку с бинарниками с одной стороны с другой стороны как правило нода на которой стоит минимум используются исключительно для меня и не Для чего больше Поэтому для вас не должно составить труда дать минимум полные права на запись в эту папку Следующий вопрос который был мы собрали кластер в котором у нас определенное количество нот каждый Ноди подключено какое-то количество дисков установили параметр ecm Согласно калькулятору но при падении одной из нот падает у нас весь кластер почему такое происходит Ну первое что я хотел ответить ребятам наверное собрали что-то не так потом подумал что это ответ больше достойный первой линии техподдержки чем Наверное ведущего программиста и попросил у них логи попросил конфигурацию кластера вроде бы выглядела всё нормально тогда я решил попробовать всё это дело на своём кластере попробовал упала То есть я выключил одну из нот прошло 2-3 минуты минимум перестал на запрос мне отвечать почему такое произошло оказывается 3 или 4 месяца в репозитории висела ошибка из-за которой действительно потеря одной ноды приводила к тому что весь кластер не просто переходил в ритон для режима вообще останавливался Ну собственно и совсем недавно поправили и и я просто установил обновление ребятам тоже посоветовал установить обновление после этого кластер начал нормально работать То есть можно выключать любую из нот всё продолжает работать после этого я решил принять на вооружение опыт больших компаний таких как Например Яндекс где ребята проводят учения и там раз в месяц например отключают один дата-центр для того чтобы посмотреть что произойдёт теперь после каждого обновления минимум мы делаем то же самое то есть ночью когда минимальная нагрузка отключаем ноду и смотрим чтобы весь наш кластер где у распределённой нить играть не деградировал Следующий вопрос развернули кластер включили компрессию идём на этот кластерные Можно ли какими-то средствами посмотреть объём который занимали не распакованные данные то есть до того как мы их упаковали и залили туда минимум напомню поддерживает а-а сжатие на лету собственно вот ответ на этот вопрос дискуссии на эту тему ведутся но так как минимум используют свой там пролетарный алгоритм сжатия пока нету информации наверх куда-то вынести не смогли и когда мы туда загружаем данные мы не можем посмотреть до сих пор даже не можем понять это сжатые данные или это разжатые данные какой Единственное что мы можем посмотреть сколько места нам ещё доступно для того чтобы мы туда могли что-то залить далее Далее вопрос почему при выпадении нот начинает сильно тупить веб-консоль проблема в том что в консоли Web есть мониторинг и этот мониторинг при первом логине Начинает опрашивать все узлы в нашем кластере соответственно если какой-то из узлов недоступен то время схода у нас увеличивается на тайм-аут ожидания от каждой из недоступных нот в такие моменты обычно приходится пользоваться больше теряем Я не знаю почему сделаны именно так и почему не перенесли опрос нот с момента входа на момент перехода на вкладку мониторинга Но вот как-то так оно собственно работает и далее главный наверное вопрос Способен ли минимум работать как геораспределенная система хранения или все-таки место ему только в одной стойке может быть в нескольких стойках но стоящих рядом и Для начала я напомню что у нас было какой кластер о котором я рассказывал в прошлый раз по сути это три сервера один располагается в нашей серверной в городе Коломна Московской области один располагался в Питере и один в Казани каждому серверу подключено 4 обычных HDD диска 3 SSD диска и все сервера объединены между собой сетью по 500 мегабит со средним временем отклика 20-25 миллисекунд то есть вот такой вот треугольник у нас получается за прошедший год мы преобразовали эту схему такую То есть у нас остался один сервер в Московской области один в Казани один из Питера мы перевезли в Самару и один еще новый добавили в Новосибирске для того чтобы быть поближе к нашим любимым пользователям из Урала Кроме того мы отказались от использования горева То есть у нас раньше было разделение на горячее хранилище где под которые мы использовали SSD диски и холодные под которые мы использовали хдд дальше расскажу почему Смысл в том что пинги между надо для данного слайда основная мысль пинги между серверами в Коломне то есть Подмосковье в Самаре и в Казани так и остались примерно 20 25 секунд а вот до Новосибирска как ни крути физику не обманешь пакеты ходят туда-обратно уже около 50 секунд соответственно мы собрали кластер такой и первым делом решили посмотреть что будет и да как я сказал мы отказались от горячего холодного хранилища потому что как выяснилось минимум всё-таки работает одновременно с большим количеством дисков и все наши нагрузочные тесты показали что он без проблем может утилизировать э производительность дисковых полок которые мы ему подсовываем э-э и одна дисковая полка на 25 дисков Вполне себе заменяет ssd-шник э вот некоторые тест здесь мы перекачивали со старого кластера на новый информацию У меня введено 50 мегабайт в секунду эту цифру надо умножить на 2 потому что кластер грубо говоря находится и источник и приемник в одном и том же месте поэтому фактическая скорость за чтение записи с одного кластер на другой составила 100 мб/с нам такой производительности вполне достаточно Да и не сказал сеть мы вот здесь вот тоже проапгрейдере было 500 Мбит стало по Гигабит между до центрами Почему мы отказались ещё от SSD Мы у себя использовали конкретные модели накопители это intels и 4608 по 6,4 ТБ и перешли на дисковые полки 25 дисков в каждой э-э диски Саске 15.000 оборотов в секунду То есть ничего нового а единственное скорость у нас не упала но при этом стоимость хранения цена за гигабайт Как вы видите снизилась с 39 руб до 19 руб за ГБ При таком подходе Кроме того такой кластер стала удобным масштабировать То есть если раньше Сервер это все-таки какое-то ограниченное пространство куда новых дисков может быть и некуда уже ставить в данном случае мы используем дисковые полки они могут подключаться каскадам И для нас сейчас расширение кластера на минимум это просто приехал в дата-центр Вставил новую полку подключил её к предыдущей через Дэдпул расширил пул и всё меню сразу на лету увидел что место у нас добавилось единственное есть опять пару нюансов во-первых я бы не стал делать слишком большие пулы zfs и в моём понимании большие это Больше шести ТБ сейчас у нас получается пулы по три ТБ 3.2 Почему Потому что э Я сказал у нас получается сейчас четыре сервера на каждом по четыре диска но диски - это фактически четыре Пула то есть физически это 96 дисков из 96 дисков какой-нибудь нет да вылетит соответственно после этого необходимо пересобрать пул и перенаполнить его информацией Это займёт определённое количество времени в течение которого кластер будет подвержен фактором риска что может выйти из строя что-то ещё поэтому меню рекомендует в меню нельзя просто так сказать Вот тебе новые диски возьми их нужно создать новый кластер и подключить этот кластер к старому в этом случае меню начнет все новые данные писать на новый кластер до тех пор пока размеры занятого пространства в кластерах не сравняются после этого он начнет равномерно записывать уже на 2 и соответственно скорость записи точно также повысится в два раза то есть по мере того как мы масштабируем кластер нашу скорость также масштабируется и растёт еще один интересный фактор после того как три дата-центра превратилась в 4 мы смогли повысить не только отказы устойчивость но и полезное пространство Почему было у нас я сейчас говорю про обычных HD диски 12 дисков группа dc-4 то есть мы можем потерять 4 диска или одну ноду целиком Таким образом получается что полезный объём это 12 - 4 8 дисков стало у нас 16 дисков то есть 16 групп фактор еси мы настроили равным шести и теперь можно потерять шесть дисков то есть одну ноту плюс ещё два диска и таким образом полезный объем теперь уже равен 10 но как выяснилось дата-центры могут выключаться не только ввиду того что произошла какая-то авария катастрофа наводнение еще что-то дата-центр может выключаться потому что мы его захотели выключить например как это произошло в нашем случае наш провайдер наш дата-центр где мы располагали свои сервера в какой-то момент прислал нам письмо ребята завтрашнего дня все цены на размещение оборудования X2 мы сказали Ну окей приехали выключили все оборудование упаковали привезли в Деловые линии и за неделю оно переехала из Питера в Самару где мы его включили и кластер начал догоняться соответственно Ему нужно было догнать Ту самую неделю в которой он был недоступен и немного о железе расскажу и все это время я подчеркнул кластер работал работал и мы не потеряли отказы устойчивость потому что мы по факту потеряли из 16 дисков 4 могли потерять ещё два то есть в момент пока сервера с дисками полками ехали в коробке из одного города в другой мы могли потерять ещё диски и при этом бы не потеряли в производительности вообще под наши задачи хватает вот такого вот железа Это виртуальные машины на каждые 8 виртуальных ядер 16 Гб И соответственно по 25 дисков Таким образом мы получаем 4 ноды с общей производительностью 348 запросов в секунду что удобно менее показывает Какое количество запросов в секунду он может обработать уже при старте кластера и зависит это напрямую От количества оперативы то есть нам 348 достаточно пожалуйста кому недостаточно можно умножить 16 Гб на 2 32 Гб И соответственно количество запросов увеличится так же и что примечательно э эксплуатирует он нагружает в основном память и сеть при этом задержки для него оказались не принципиальные от того что мы одну из ноты Разместили в Новосибирске медленнее это работать не стало То есть как бы теоретически где-то скорее всего стало физику уже не обманешь но мы этого никак не замечаем и наши пользователи тоже этого никак не замечают а дальше после того как мы все это перенесли переразместили нужно было данные с одного кластера перелить на другой со старого на новый всего надо было у нас не такой большой объем еще данных там потому что мы в меню только начинаем подключать наши продукты нам нужно было перелить 12 терабайт и для того чтобы не мешать другим сервисам работать скорость мы ограничили до 20 мегабайт в секунду реальная нагрузка на сеть как я уже сказал удваивается по той причине что старый кластер и новый кластер по сути работают на одних и тех же каналах Это просто два рядом стоящих сервера но данный же переливаются не с одного сервера в другой они переливаются с кластера на кластер кластер между собой взаимодействует соответственно нагрузка на сетьфицируется Таким образом мы потратили на переливку 2 недели человек с калькулятором спросит А что так долго вроде С такой скоростью должно перелиться больше быстрее объясняю Нам же не просто необходимо взять данные с одного кластера и перелить на другой кластер работает соответственно нам нужно сначала их перелить а потом сделать так чтобы новый кластер догнал старый То есть все данные которые за время переливки переноса изменились тоже до синхронизировать и нам нужна была какая-то утилита вроде Ring только которая может работать С3 хранилищами PS3 протоколу первое чем мы решили воспользоваться стандартной утилитой MC в комплекте minio у него есть команда Mirror которая позволяет сделать именно то что нам надо то есть синхронизировать старый кластер с новым я запустил антимир пошел спать с утра проснулся увидел вот такое на самом деле не надо вглядываться в этот скриншот здесь просто самое важное видно это количество ошибок которые произошло за ночь проблема в том что когда мы поднимали первый кластер меню несовершенную его систему мы там умудрились потерять пару тысяч файлов побились мы их никак не вычистили и нашим всемир об это дело споткнулся Я сначала хотел вам всем миром поискать не хотела искал какой-то флаг который бы сказал ему пропускать битые файлы и как-то их отмечать но у него такого флага не оказалось поэтому я нашёл другую интересную она называется Клон собственно запустил Клон он также позволил мне указать ограничение на производительность указать сколько потоков одновременно для файлов я могу использовать он начал данные копировать Все отлично Я опять пошёл спать с утра просыпаюсь Вот такая картина то есть битый файл он пропускает все нормально но при этом скорость копирования почему-то упала до 256 килобит в секунду я подумал Ну может быть первый раз что-то пошло не так перезапустил через день история повторилась то есть скорость копирования с одного кластера в другой не превышала скорости черепахи Хотя я проверил оба кластера нормальные сетью с производительностью проблем нет а-а И здесь нужно сделать отступление рассказать как работают объектные хранилище Мы привыкли все файлики у нас видеть в какое-то древовидной структуре то есть папка внутри другая папка внутри третий папка внутри файлы а-а хранилище объектные хранят объекты плоско То есть все объекты хранятся э как бы в одном пространстве Но для удобства восприятия нами э-э утилиты которыми мы пользуемся отображают их нам в виде какого-то дерева Я здесь делаю небольшое отступление если в зале есть Ребята с Яндекс облака вы Привет техподдержки своей передайте Это я им потроллил немножко чтобы красивый слайд получился вопрос задал собственно говоря в чем проблема сами объектные хранилища без проблем переваривают такой большой объем данных они могут хранить там десятки миллионов объектов плоско но это не могут делать клиенты вот Обычный батит в котором там 10 терабайт и чуть больше полумиллиона объектов вызывает то что различные утилиты родная Вот эта скриншот - Это родная консолью предыдущие Это был эрклон как я уже сказал мы же не просто берём данные из одного места и копируем в другое Нам необходимо построить некий диск И затем по этому дифу действовать так вот построение дифа для таких больших объёмов Да и даже просто чтение таких больших объёмов утилит вызывает некоторые сложности мы приходим к тому что само объектное хранилище не тормозит но ничего работать с ним в таком формате не может на наше счастье мы до сих пор по старой привычке в объектном хранилище храним наши файлики по принципу разбиения имени на части то есть вот Файлик 1 2 3 4 5 6 7 будет лежать в папке 12 34 Ну и дальше соответственно со своим именем Используя это просто написали небольшой скрипт который перебирал одну папку за другой и в этом случае копирование уже пошло нормально то есть мы перенесли все данные достаточно быстро но вот с такой сложностью столкнулись то есть попытка скопировать один Бакет огромный другой Бакет приводит к тому что все начинает подвисать и тормозить Хотя потом с этим байкером можно нормально работать то есть мы запрашивали просто дай нам один файл из этого большого байке это проблема не возникало проблемы возникало из-за того что нам нужно всё это дело синхронизировать Ну и на данный момент мы пришли к такому кластеру То есть у нас четыре ноты располагаются в четырех датах всего у нас используется 96 дисков Кстати если кто заметил я сказал что в дисковых полках по 25 дисков и это соответственно должно быть умножить на четыре 100 Но на самом деле мы используем 96 потому что это во-первых удобно делится на четыре во-вторых в этом случае если где-то один диск выходит из строя моего держим как то есть мы можем его быстренько подменить на другой и потом уже у нас будет время на решение возникшие проблемы всё-таки для нас от каждого устойчивость важна и важно как можно быстрее вывести кластер из такого состояния при котором появляется риск потерять в нём какие-то данные на самом деле я Пока готовился выступлению это презентация Она же сделана заранее еще одна проблемка с меню возникла я вам про неё отдельно расскажу без слайда она в принципе наглядная в старой версии minio до того как я ее обновил мы использовали возможность хранения метаданных прямо прикреплённых файлу то есть к файлику можно в протоколе S3 цеплять различные данные мы их туда напихали начали радостно использовать после того как минимум обновилось Э прибежали ко мне разработчики и говорят А что-то у нас метаданные с файла не читают я О'кей давай посмотрим начинаю смотреть А после обновления половины метаданных нету Начинаю разбираться в чём дело Хотя исходники миньо э закидывая в него старые методы данные запуская процесс обновления и выясняется что э минимум метаданные хранил в каком-то своём формате и принимал их в какой-то момент бы посмотрели Ага что-то наш формат не соответствует тому формату который описан для S3 с самим амазоном и решили его привести в соответствие и что они сделали когда объекты переезжают с одного хранилища в другое либо когда происходит обновление они берут прогоняют старые методы данные через реджекс если эти метаданные реджекс не проходят до свидания с этим тоже нужно быть аккуратными собственно все дальше готов услышать вопросы Добрый день Меня зовут Василий Спасибо за доклад вопрос на поверхности как на мой взгляд у вас растущий кластер S3 и сейчас он пока еще очень маленький 12 миллиона объекта это мало но почему минимум А нецев почему меня не цепь Почему не цепь Просто если мастер продолжит расти у меню есть все-таки Вот то что сейчас было перечислено у него еще всякие мелкие болячки которые Ну изначально не задумывался как система там чтобы кластеризовать он очень долго не умел кластер в то же время есть которая отлично работает прекрасно предоставляет ис-3 и держит большие объемы хорошо кластеризуются и так далее притом железная конфигурация которая была приведена она отлично подходит для того чтобы тот же пластик сделать Непонятно чего Год назад было понятно отвечу вопросом на вопрос используете У меня два человека его обслуживают это кластер с 9 нот 150 терабайт данных 200 терабайт полезного места Ну там могу еще характеристики у нас меню не обслуживает никто его запустили и он работает я на самом деле когда выбирал чего использовать просто надо вбить в Яндексе цепь и как его восстановить и там есть куча куча просто историй от ребят из того же digit lotion когда у них посыпался этот цепь и на то чтобы восстановить им чуть ли там не пришлось реально с бубном вокруг этого цепь кластера скакать добавлять память в ноду он у них не стартовал и таких историй в интернете про церковь просто десятки то есть на мой взгляд минимум он также просто как из тфс по сравнению с lvm сюда микрофон Можно пожалуйста вот здесь первый ряд ваш следующий будет хорошо Здравствуйте спасибо за доклад очень интересно у меня один вопрос А вот zfs у вас дал какой-то оверхед по нагрузке то есть там вы его заметили то есть на процессор там еще не заметили вообще то есть вообще никаких точек То есть даже в момент копирования ничего он такого не прибавлял хотя неоднократно слышал заявление о том что за FS чуть ли там не 20% может сажать не 20 но обычного видно то есть так чуть-чуть то есть вы нету нету я говорю вот такой конфигурации хватает процессоры практически простаивает то есть основная нагрузка всегда идёт памяти Угу спасибо вот там вот ещё была рука Да да да микрофон пожалуйста день спасибо за доклад у меня несколько вопросов первый вопрос по zfs Почему не используется они нам не нужны потому что те объекты то есть здесь надо немножко тогда углубиться мы используем минимум двумя путями первое S3 хранилище второе это свой цдм то есть мы подумали Ага У нас есть четыре ноты в различных городах чтобы нам из этого не собрать цдн Да пожалуйста мы соответственно на инжинсе настроили два доменных имени одно у нас называется S3 Базис of True другое седиент Базис софтру единственных отличий в том что всё что ходит через три не кэшируется эти данные должны постоянно обновляться всё что ходят в сиде- через сидень соответственно попадает в кэш и этим у нас занимается сам джинсы Ясно Хорошо Тогда как будет боретесь с ресиверингом э-э при добавлении новых слов с чем болимся ресиверинг ещё раз в зифэсе это процедура переезда блоков при добавлении дисков либо при замене их Ну вышло из строя там начинается процедура пересильверинга это в рокковых терминах воплин dfs Я не знаю как это называется смысл я понял вопрос Смысл в том что если у нас какой-то диск из дэдпула вывалится мы просто этот zetpool Перри соберём Потому что э-э от того что мы диск у нас же получается рейд 0 по факту и от того что мы один диск изымаем весь Рейтер рассыпается мы заменяем его другим диском перебираем пул и минимум его переливает данные поэтому эта проблема Просто никто не актуальна Я понял Тогда э-э последний вопрос Вот э-э вы указывали на проблемы с обновлением А этот mcd А в чём проблема подменить адрес с э-э там Китая или что-то там на свой один раз его скачать и дальше хоть по 100 гигабитам раздавать ну мы это собственно Так и делаем но это как бы гайд для тех кто с этим столкнётся впервые понятно что он для себя найдёт какой-то оптимальное решение как распространять Я говорю что минимум из коробки работает вот так и на это нужно обратить внимание понятно хорошо спасибо друзья еще вопросы будут поднимать руку Вот вот там вот Здравствуйте меня зовут Евгений Спасибо за доклад zfs на самом деле есть еще штука Как компрессия не знаю вы упоминали что вы используете мини-компрессию так понимаю но не пробовали в zfc просто ее включить она достаточно хорошо работает не пробовали и намеренно даже не Будем пробовать потому что zfs мы используем чисто как слой для объединения в пулы мы не хотим навешивать на него чего-то лишнего вот есть мини-компрессия мы её используем мы zfs используем просто как хранилку нам какие-то дополнительные там сверх возможности её как вот только что отвечал на вопрос касательно э замены битых дисков не нужны ну принят Спасибо ещё вопрос У кого-нибудь Привет чувак клад та часть презентации где ты рассказывала что вам пришлось писать собственный скрипт так как arclone не вывозил вы при использовании этого скрипта использовали все равно airclone и какой примерно скорость вам удалось добиться что использовали примерно использовали А дальше ты спросил И какой скорости вам удалось добиться примерно при перекачке данных 50 мб/с мы её отожрали практически целиком потому что кластер старая нода новые ноды они стоят в одной стойке но при этом все остальные ноды стоят в стойках по другим городам соответственно когда ты начинаешь с кластер мини читать он читает не с одной ноды он информацию собирается со всех и когда он начинает писать на другой кластер вот эту информацию он любой блок данных делится на мелкие блоки эти блоки рассасываются по нодам в разных городах соответственно любая запись на ноль либо тени с ноды вызывает обмен интернета трафик то есть обмен трафиком между нодами соответственно мы можем утилизировать целиком сеть и здесь нам приходится ее ограничивать И делить пополам чтобы у нас все остальные сервисы не стали А кроме сетевого опирались в CPU или память при не упирались то есть вот эти вот параметры которые я привел для нот они для нас даже немного избыточны мы их оставили чтобы потом в них не лазить по мере того как нагрузка будет расти Спасибо вот здесь вот ещё была рука я ещё отвечаю на этот вопрос вспомнил э мини используют как я сказал свой алгоритм компрессии и на их тестах и этот алгоритм компрессии на одном ядре цпу переваривает торопит данных Ну я сам соответственно с такими данными не сталкивался Я опираюсь только на то что приводит они в свои документации в своих примерах прошу ваш вопрос Лёш Спасибо за доклад Вопрос немножко нубский ты с одной стороны говоришь про полки из там 30 дисков И как мы добавляем рядом на соседнем слайде ты говоришь что вот у нас нода на 4 диска Это какие-то другие диски это пулы то есть в полке 25 дисков один мы вычитаем оставляем подход спр остаётся 24 оставшиеся 24 мы делим на четыре Пула То есть это четыре Пула по шесть дисков мы делаем так чтобы minio не видел больше чем 16 дисков в этом случае мы можем гарантированно контролировать что у нас различные пулы окажутся на различных нодах не получится Так что все плыл на одной ноге Спасибо это нужно для того чтобы обойти вот это их ограничение с размером EC вот там вот только сзади Ты рассказывал как вы боретесь с битами файлами А у меня еще есть такая особенность что у него бывает недобитые файлы это когда ты с него что-то удалил что-то пошло не так и у тебя эти файлы лежат на файловой системе и менее начинает А у вас тут уже лежит я ничего записать не могу Да бывает такое по таким на самом деле сейчас меньше когда мы в первый раз втыкались в это просто отписывались в github Это довольно оперативно нравится ну здесь на самом деле у меня такой подход что если несколько раз ему сделать РМ он все-таки его удалит это тоже какого-то момента появилась Я помню мы много раз пытались не получалось сейчас с этим уже проблем нет вот ту тысячу битых файлов там пару тысяч мы как раз получили вследствие Вот таких то недоударений то ещё чего-нибудь не до надо понимать ещё такой момент проект Меню развивается 6 лет всего и пишут его там буквально 4-5 человек э и соответственно он очень динамично развивается Мне он то есть мы готовы мириться с косяками которые у нас возникают в угоду того что он динамично развивается и ребята активно принимает идеи по тому что что можно в нём сделать лучше Угу Спасибо друзья у нас есть вопросы с чата Иван Ковач спрашивает диски в полках объединены в какой-либо рейд либо далее объединение только через дтф только через zfs мы не используем возможности контроллеров по объединению В рейд потому что в противном случае мы А можем упереться в какие-то проблемы с контроллером б можем теряться проблемы с этим сами массивом и в мы на всех серверах используем разные контроллеры то есть для меня не принципиально какой контроллер купить лишь бы он мог эту полку увидеть воткнул погнали спасибо Вот рука Да все на микрофон пожалуйста Спасибо за прекрасную историю более страдания У меня вопрос Вы до каких пределов собираетесь жить На прекрасном вашем решении Где ваш следующий порог масштабирования в который вы планируете упереться по сети по количеству объектов и по утилизации запросами О мы будем жить в этом окружении до тех пор пока оно не начнёт тормозить либо возникнут какие-то вот прямо нерешаемые проблемы и тогда мы будем уже задуматься о переходе куда-то Ну вот по вашей оценке Это сколько там миллион объектов или там ну миллионов всех у нас уже лежит Я думаю по моей оценки это проблемы начнутся после того как мы перевалим за хотя бы терабайт ой терабайт оптимистичненько хорошо встретимся в следующем году мне уже многие говорят что в следующем году надо приходить с докладом минимум ещё год спустя Привет коллеги есть у кого-нибудь еще вопросы Из зала вот сюда Да здравствуйте Спасибо Алексей за доклад подскажите вот вы говорили этом точнее Вот продолжение молодого человека его вопросы вот промасштабирование какое количество нот Ну минимальное можно добавлять пол Можно ли добавить одну или там какое-то определенное количество добавить мод то есть меню можно стартовать вообще На одном сервере с одним диском Но для того чтобы добиться отказа устойчивости минимальной допустимой а-а необходимо четыре Четыре года Четыре ноты по одному диску в каждый потому что тогда будет достигнут минимальный размер 10 группы 4 диска два диска выделится по данной в этом случае два диска выделятся под блоки чётности здесь используются обычные коды и до Соломона для восстановления данных соответственно нам нужно половина от занятого места для того чтобы эти данные восстанавливать то есть минимально четыре устойчивость но мы не собираемся больше трёх дел нам этого не надо соответственно мы эти ноды просто будем масштабировать до тех пор пока у нас будут влезать полки А дальше уже будем смотреть еще вопросы что если на этом всё Алексей образов было много Возможно это будет сложно но тебе нужно выбрать много но подарки наверное отдал вот молодому человеку хорошо поднимите пожалуйста руку Helper вот сюда пожалуйста подарок а давайте поблагодарим Алексея замечательный доклад аплодисменты и Алексей подарки даже для тебя подарок от атончика и подарок от Группы компаний ВК умная колонка Маруся О спасибо Спасибо"
}