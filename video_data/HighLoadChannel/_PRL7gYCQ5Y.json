{
  "video_id": "_PRL7gYCQ5Y",
  "channel": "HighLoadChannel",
  "title": "AP и CP пытаемся усидеть на двух стульях и боремся с последствиями / Сергей Петренко (Tarantool, VK)",
  "views": 120,
  "duration": 2057,
  "published": "2023-10-06T07:21:13-07:00",
  "text": "сначала пару слов о себе Меня зовут Сергей Петренко последние четыре года я программист тарантул занимаюсь в основном репликацией в частности участвовал в разработке нашего алгоритма синхронной репликации и выборов лидера он у нас основан на рафт о чем мы сегодня будем говорить я начну с того что напомню перед каким вообще выбором Мы стоим в случае если у нас сеть ненадежная это либо обеспечивать консистентность либо доступность системы дальше я поговорю Каким образом мы пытаемся прыгать между этими двумя режимами консистентности и доступности если вдруг нам это потребовалось расскажу каким проблемам это приводит И как мы эти проблемы пытаемся решать для начала Давайте вспомним про кап-теорему что в ней говорится в ней говорится что в присутствии разделения сети система может обеспечивать либо доступность либо консистентность данных здесь его доступности и консистентности есть вполне определенные определения поэтому часто Когда система себя как-то относятся к теоремы Они любят говорить что они либо CP либо AP в зависимости от того что они обеспечивают на худой конец можно быть просто п Но обычно так себя никто не рекламирует так вот консистентность означает что любой запрос на чтение должен возвращать все данные которые были подтверждены до этого запроса это по-другому еще называется линияризуемость а доступность означает что любой здоровых узлов должен отвечать на все пришедшие ему запросы за конечное время это значит что например этот узел если его оторвало от остальных не может пересидеть разделение сети чтобы ответить самыми актуальными данными Нам же хочется всего и сразу давайте убедимся что не получается и консистентности доступности обеспечивать вместе вот пусть у нас есть кластер из трех узлов один из них оторвало от остальных и к одному из них приходит клиент И просит записать что-то например фура внутри доступность от нас требует сразу же этому клиенту ответить А вот другой клиент или может быть тот же самый пришел уже на оторванный от всех серверов хочет прочитать Чему равно Фу консистентность от нас требует ответить что фура внутри но этот сервер этого не знает и узнать Только когда разделение сети полечится доступность требует ответить сразу что мы делаем Отвечая что мы не знаем чему равно как правило системы которые выбирают гарантировать либо CP либо AP пользуется стандартными подходами в частности либо асинхронной либо синхронной репликацией системы которые борются за доступность обычно используют асинхронную репликацию и не заморачиваются синхронной а системы которые пытаются обеспечивать консистентность обычно используют синхронную репликацию это сразу же делает их недоступными потому что если у нас остался один живой узел ему не хватит кворума чтобы подтверждать транзакции а доступность требует от него этого делать но одной только синхронной репликации для консистентности недостаточно нам нужно вообще это не только писать по кворуму и читать по квораму для того чтобы обеспечивать консистентность классический пример ЦП алгоритма Это рафт здесь и синхронная репликация или не реализуемое чтения про которые я уже говорил Ну и про выборы лидера так еще есть немножко и кворум для всех этих активностей большинство Давайте теперь поговорим про нашу реализацию raft чем она отличается от канонической вообще рафт всем хорош особенно если вы на него смотрите только на бумаге то вам вообще нравится его описание что Что еще можно хотеть вот у вас консистентность обеспечивается всегда всегда будет Лидер выбираться замечательно Ну пока большинство узлов жива и как-то никто не думает А что будет если большинство умрет вроде как непонятно что с этим делать на самом деле если большинство умрет то мы не сможем ничего описать мы не сможем выбирать нового лидера и перейдем в режим недоступности поэтому на этот случай у нас можно выбрать между синхронной и асинхронной репликацией то есть Лидер все еще один мы используем рад чтобы выбирать лидера А в зависимости от таблицы в которую он пишет репликация может быть либо синхронная либо синхронная если вы вот очень любите консистентность то вы можете везде писать синхронно все будет в полном соответствии с ростом Но если вы хотите чтобы когда Лидер остался один Вы все еще могли что-то писать Вы можете выбрать асинхронную репликацию и помимо этого мы разрешаем настраивать кворумы для выборов лидера и для подтверждения синхронных транзакций по умолчанию на скором большинства как и положено но опять же Что делать если у вас большинство умерло вместе со старым лидером и вся система сидит в редунгли Вы можете среди оставшихся живых узлов кворум понизить чтобы они могли выбрать нового лидера и продолжить работать на этом можно было бы заканчивать что вот мы и консистентность вроде как обеспечиваем захотели обеспечить доступность обеспечили Но все не так просто рано или поздно все аварии заканчиваются и наверное захотим восстановить гору мы снова обеспечивать консистентность это сделать как оказывается не так легко Давайте Сначала посмотрим в чем проблемы с асинхронными транзакциями А дальше поговорим что и синхронными транзакциями все не очень хорошо а можно мне сделать слайд на весь экран пожалуйста так вот асинхронные транзакции кому вообще может прийти в голову использовать асинхронную репликацию вместе с ростом какой в этом смысл рафт гарантирует консистентность только если используется синхронная репликация смысл очень простой люди как-то больше любят асинхронную репликацию потому что с ней задержки меньше и вроде как действительно все умерли Лидер остался дальше может писать Здорово Ну вот каким это проблемам приводит предположим что у нас был кластер сорафтом был выбран Лидер в какой-то момент лидера оторвало от остальных Ну он это не замечает он продолжает принимать какие-то транзакции репликация асинхронная соответственно он спокойно на эти транзакции отвечает так тем временем большинство выбирает нового лидера туда тоже приходит какие-то клиенты что-то пишут новый Лидер тоже им отвечает и таким образом система работает с двумя независимыми лидерами Да эти два лидера в разных формах то есть гарантии Там они нарушили но тем не менее клиент может прийти на одного и на второго и что-то записать асинхронно затем наконец разделение сети лечится старый Лидер завидев нового складывает полномочия и бывший лидеры новый Лидер начинает синхронизировать свои данные тут может возникнуть вопрос почему вообще новый лидеры принимают транзакции из старого терма Это вроде как рафту противоречит на самом деле нет тут все транзакции которыми мы обмениваемся они уже подтверждены потому что они асинхронные для их записи к вам не нужен рафт если что и отбрасывает из старого термо то только не подтвержденные транзакции в раб ведь только синхронная репликация и там не может возникнуть ситуации когда вы что-то подтвердили а потом выкинули его никогда не выкидываете то что уже подтверждено и мы это сделать здесь не можем мы уже ответили клиенту что да Мы записали фура внутри и фуравноду 2 Мы записали Мы обязаны когда связь восстановится между узлами этими данными обменяться мы не можем просто выкинуть то что сказал черный сервер или то что сказал фиолетовый сервер Ну и в результате это нас приводит к такой ситуации когда на серверах данные разъехались на одном уровне 2 на другом уровне 3 этот инцидент реально произошел у нас где-то весной и вот тут два сервера работали всего полторы минуты вот так Независимо но в результате чтобы восстановить консистентность наш инженер потратил порядка 9 часов он по одному эти сервера отключал и записи в их журналах упорядочивал В каком-то общем порядке который ему нравился чтобы консистентность восстановить нам бы этого Понятно хотелось избежать то есть мы не можем запретить двум серверам работать Независимо Раз уж асинхронная репликация используется поэтому все что мы можем это не дать им воссоединиться после того как они вот так Независимо поработали Это и будет нашей целью Ну как мы это делаем я расскажу чуть попозже Да вот еще один может возникнуть вопрос почему же нам не помог чек-корум у нас же рафт чек корм для тех кто не знает это такая надстройка рафт которая заставляет лидера сложить полномочия как только он видит что потерял большинство про это вчера подробно в своем докладе рассказывал Сергей астаневич правда у него это называлось фенсинг мы не до конца терминологии определились более каноническое чек-кору так вот во-первых весной у нас еще этого Рома не было Сейчас он есть во-вторых он бы не помог как он работает вот вы уже потеряли связь с остальными У вас есть какой-то тайм-аут на то чтобы убедиться что действительно соединение пока этот не прошел реально уже пакеты от вас большинство не идут а вы все еще думаете что вы лидеры Можете писать соответственно у нас бы старый Лидер в течение нескольких секунд продолжал принимать транзакции уже не увидит новый Лидер на момент своего становления лидером Проблема была та же самая что два лидера Независимо что-то покормители просто это ободлилось не полторы минуты а несколько секунд и разбирать вручную бы это пришлось но всего час а не 9 часов в любом случае мы бы хотели этого избежать нужно как-то еще проблему решать Но если бы проблемы были только с асинхронной репликацией как я уже ну чем я Вас пытаюсь удивить взять авто выкинуть из него синхру взять асинхру и вот надо же консистентность потерялась но синхронно и репликации все тоже не очень хорошо все не очень хорошо по той причине что мы разрешаем конфигурировать вором мы делаем Это не просто так но дело в том что есть вполне конкретные паттерные установки базы данных при которых у вас потеря определенного числа узлов сразу ведет к недоступности вообще все кворумные системы любят работать когда есть нечетное число точек отказа или точек доступности то есть у вас должно быть три дата-центра три узла Ну 35 нечетное количество всего на самом деле же весь бизнес у нас сидит в двух дата-центрах практически это нас автоматически приводит к симметричному расположению кластера когда у нас в одном своде несколько узлов в другом своде столько же узлов если с этим цодом что-то происходит А мы используем ruft мы сразу же теряем доступность все квору у нас нет Писать мы ничего не можем это нас вынуждает сразу же снижать курум что уже нарушает Ну либо если мы такие стойки не боимся терять кучу денег из-за недоступности долгой мы можем понижать и ждать пока цод становится почему-то не все выбирают быть стойкими более правильный вариант это решить сделать третий сот такой более дешевый для голосования в нем не будут храниться данные но зато Тарантул установленный в нем все равно будет голосовать и подтверждать синхронные транзакции тогда у нас потеря любого из трех содов хоть любого из двух основных дополнительного не приведет к потере кворума мы можем не теряя консистентности не снижая кворум продолжать работать вот это было бы правильно но мы пока так не умеем у нас нет узлов которые одновременно и не содержат в себе данные не принимают их но при этом умеют голосовать может быть когда-нибудь появится но пока возвращаемся к установке в двух содах как я уже сказал кворам охватывает оба цода стоит одному из содов пропасть вором теряется тоже и пользователь вынужден приходить и вручную снижать кворум чтобы оставшиеся в живых узлы продолжили работать Ну Как и любая ручная операция никто в этом случае как с Любой ручной операцией никто не застрахован от ошибок в этом случае и Вполне может получиться такая ситуация когда на самом деле оба сода живы здоровы Но между ними потерялась связь и более того два админа которые дежурят в этих складах поссорились из-за девушки не разговаривают каждый в своем своде понизил кворум и вроде как довольный что Работа сделана пошел отдыхать клиенты каждый клиент ходит в свой любимый цвет каждый клиент ходит в свой любимый цод и что-то пишет только здесь уже репликация не асинхронная асинхронная то есть мы а синхронная то есть мы коммитим синхронные транзакции просто со сниженным кворумом и вот связь восстанавливается эти цода точно также обмениваются данными мы опять не можем отбрасывать уже закомеченные транзакции Мы уже ответили клиенту что мы все сохранили получили ту же ситуацию что для асинхронных данных уже синхрой понятно что это тоже нужно решать Ну и даже если бы авария не происходили раз появилась ручка снизить кворум кто-нибудь ее обязательно нажмет не вовремя поэтому вообще надо Быть готовым к тому что может появиться два лидера по ошибке А значит от нас требуется кластер от этой ситуации как-то Защищать не давать этим двум лидерам смешать свои данные после того как они какое-то время поработали Независимо Давайте смотреть как мы это делаем мне для этого потребуется рассказать про устройство нашего журнала и первый пример будет про mastermaster репликацию потому что по ней будет понятнее всего вот пусть до какого-то момента журнала всех узлов совпадают например что-либо писал только третий узел дальше первый второй узлы Независимо что-то комитет и репликация по репликации все это разъезжается на все узлы к власти Как видно порядок операции на в журнале каждого из узлов разный это как раз и говорит о том что эти операции черные фиолетовые выполнены Независимо то есть фиолетовая операция выполнялась Когда не было никакой информации о черной и наоборот это ровно то чего мы пытаемся избежать в рафте Давайте посмотрим как раз этим справляется здесь все нормально кворум Хороший у нас третий узел был лидером затем почему-то избрался первый узел тоже что-то там на всех записал после этого избрался второй узел тоже там на всех все записал Как видно порядок операции на всех узлах совпадает это кстати одна из гарантий рафта там прямо прописано под каким-то пунктом что в журналах всех узлов на одной и той же позиции содержится одна и та же операция посмотрим что будет в случае Сплит Брейна опять третий узел был лидером затем почему-то у нас появилась сразу два лидера и первый и второй им не обязательно это делать одновременно главное что между ними не было связи Вот когда они появились потом связь восстановилась они обмениваются данными и мы получаем ровно ту же картинку что и в мастер-мастере то есть порядок транзакции опять разный получается ровно это мы хотим детектировать чтобы отличить нормальную работу рафта От случая когда либо по ошибке либо из-за использования асинхронной репликации данные коммитились Независимо несколькими узлами нам достаточно посмотреть в каком порядке уложены транзакции в журналах каждого из узлов если порядок везде одинаковый то все хорошо если порядок на каких-то двух узлах разный значит они работали Независимо этого нам нужно избежать вот как мы это обнаруживаем каждый Лидер у нас помимо того что поровство он становится лидером и всем это сообщает предваряет своей транзакции специальные записью промоут он фактически завладевает журналом на весь свой Терм в этом промоуте указано Кто лидер В каком Терме он стал лидером и также содержится некоторая информация о предыдущем Лиде Кто он был и какую транзакцию последнюю мы от него видели вместе нам эта информация достаточно для того чтобы выстроить линейную историю вот этих промоутов они попадают прямо в журнал на каждый из нот они точно также реплицируются они предваряют все транзакции записанные текущим лидером и когда все в порядке есть Брейна Нет эти промоуты выстраиваются в линейную историю то есть в журналах всех узлов одни и те же записи промоуд расположенные в одном и том же порядке и от одного промоута к другому можно Проследить как лидерство в кластере переходило а вот что будет когда все плохо опять уже Надоевший нам кластер третий узел Лидер в пятом Терме о чем Он сообщил всем и даже написал это в свой журнал чтобы увековечить свое лидерство Затем он по какой-то причине умер первый Лидер Это первый Узел это заметил стал лидером в шестом Терме записал что до него лидером был третий узел пока все нормально но по какой-то причине видимо кворум неправильно сконфигурирован да еще и связь между первым и вторым оборвалась второй узел тоже становится лидером в каком-то вообще произвольном Терме это может быть тоже 6 термо это может быть 100500 и Терм главное что больше чем 5 второй узел тоже думает что все нормально Вот он стал лидером в новом Терме вот до него лидером был третий Давайте посмотрим что будет если мы попробуем эти два узла соединить Когда со второго узла на первый приходит вот эта запись промоуд нам нужно понять что она конфликтует с нашим промоутом чёрным ну здесь все Казалось бы нормально термо больше чем термо черного промоута но фиолетовым промоуте указано что предыдущим лидером был третий узел А черный это знает что предыдущем лидером был он он этот промоуд отвергает рвется единение со вторым узлом и в принципе никакой репликацию со второго узла больше не принимает то есть мы оставляем черный узел жить в своем мирке в котором он и существовал пока ему этот промоут не приехал в обратную сторону Здесь просто проверка терма не проходит фиолетовый узел знает что уже идет 999 Терм А до него доехал какая-то запись из шестого тюрьма раз она до него доехала и он пытается ее применить значит он ее до сих пор не видел то есть это совершенно точно последствия независимой работы двух узлов он по этой же причине этот промоуд выкидывает И никакие транзакции от чёрного узла не принимает Таким образом мы их сохранили каждый в своем мирке но этот пример простой здесь по первой же операции видно что случился Сплит Брайн Давайте посмотрим на более частые случаи вот предположим у нас первая нода была лидером что-то писала написала до 50 транзакции а затем почему-то вторая нода избралась лидером она пишет правильные промоут вот предыдущие Лидер был черный тебя я Лидер и последние транзакция что я от него видела 50 а черный продолжает работать он не согласен что он предыдущий Лидер более того он не просто продолжает работать он продолжает подтверждать транзакции то есть либо к ворам настроен неправильно либо синхронная репликация используется а вот через какое-то время они пытаются соединиться фиолетовый промоут попадает на черный узел фиолетовым промоуте сказано что фиолетовая нода стала лидером когда последнее что она знала это пятидесятая транзакция от чёрного узла а чёрный знает что он уже 51 подтвердил этих причин ему достаточно чтобы выкинуть и фиолетовый промоутер и разорвать соединение с фиолетовой нотой в обратную сторону вообще все проще Я уже говорил что промоуд устанавливает владение журналом и после фиолетового промоута в журнал могут попадать только фиолетовые операции либо более новые подходящий промоуд тут к нам пришла транзакция которую мы явно не видели раз она до нас доехала и мы пытаемся ее применить Ну все это маркер Сплит Брейна тоже нужно рвать соединение и ничего черного узла не принимать тут важно что в обоих случаях когда мы находим конфликт по записи промоута когда мы смотрим как конфликтует подтвержденная транзакция с чужим промоутом мы Рвем соединение на первой же конфликтующие операции то есть мы оба сервера оставляем точно в таком состоянии как будто они вообще друг с другом не соединялись как и было пока они работали Независимо ну и наконец Давайте посмотрим как эта ситуация выглядит когда все хорошо забудем что черный что-то там Независимо накопил предположим что у нас и репликация только синхронная и кворум настроен правильно вот фиолетовый стал новым лидером на черный пытается что-то писать он не может эти транзакции подтвердить потому что у него нет кворума при условии что новый Лидер уже выбран и если такие неподтвержденные транзакции попадают на нового лидера это ничему не противоречит мы знаем что последнее подтвержденная транзакция пятидесятая от того что нам приехало 51 не подтвержденная ничего страшного в этом нет мы просто игнорируем и соединение не Рвем в обратную сторону то же самое черный Лидер помнит что последнее подтвержденная транзакция 50 неважно сколько у него там выше напихано не подтвержденных транзакций если ему приходит промоуд в котором подтвержденная транзакция тоже пятидесятая он его спокойно применяет все что у него не подтверждено откатывает консистентность сохраняется и дальше уже фиолетовая нода продолжает спокойно писать они все номеруют транзакции Независимо поэтому фиолетовый все начинается с единички опять да то есть все что мы делаем Это не даем соединиться двум нодам если мы обнаружили конфликт между ними Почему мы так делаем Я уже частично ответил в процессе доклада потому что обе эти ноды в процессе независимой работы комители своей транзакции то есть мы не можем после коммита просто взять и одну из двух версий выбросить на каком-то клиенту ответили что на это сохранили о каком-то что это сохранили соответственно все что нам остается это не дать этим нодом соединиться и перемешать транзакции чтобы получить вообще непредсказуемые последствия Ну помимо этого вообще причина по которой происходит splitbrain это пользовательское вмешательство соответственно мы на уровне системы не можем понять какая версия данных пользователю нравится больше какой он считает правильный поэтому мы оставляем решение за пользователем Давайте посмотрим что мы сегодня узнали во-первых если вместе с алгоритмом выборов лидера использовать асинхронную репликацию то консистентности не достичь по крайней мере ситуация будет очень похожа на мастер для меня это был сюрприз в свое время казалось раз у нас единственный Лидер всегда то значит и гарантии должно быть такие как будто Лидер всегда единственный но это не совсем так Ну побуду капитаном Очевидность если снизить кворум то и для синхронных транзакций все может стать плохо лидеры у вас два может появиться И транзакции конфликтующие появятся Значит все что нам остается это как-то защищать данные от такого независимого изменения несколькими узлами и сейчас все что мы делаем Это не даем этим узлам соединиться но думаем про то чтобы как-то упростить для пользователя задачу Воссоединения вот этих двух версий данных на этом У меня все спасибо за внимание по QR коду можно проголосовать за доклад пожалуйста ваши вопросы поднимайте руки и вас увидят и вот у нас первый вопрос большое за доклад можете рассказать как сейчас выглядит процедура наведения порядка Ну вот когда у вас два лидера не соединились у него написали данные и дальше что кроме администратора который от ветра головы свои я что-то делает сейчас это выглядит так что у вас образуется две половинки кластера рано или поздно каждый узел начинает верить либо в одну версию данных либо в другую в зависимости от того что ему первое пришло и между этими двумя половинками нет ни одной связи то есть не только между лидерами А между всеми если вот я поверил в одну версию данных я от других ничего принимать не буду дальше только ручное вмешательство То есть самый простой способ это вытереть данные на одной половине и заново её загрузить с половиной которую считаешь здоровой Так что пока что так понял я второй вопрос вот внутри Тарантула репликация Логан авто она включает в себя данные или это только у вас выбор какого-то узла куда наливаем данное Да Лок рафта включает в себя данные У нас есть один журнал в нем хранятся и данные и вот эти можно сказать что это событие рафт вот эти записи промоуд их в настоящем рафте нет но у нас есть Спасибо Следующий вопрос Спасибо за доклад сейчас насколько я понимаю восстановление работы это работа администратора человека а в некоторых сценариях например там бизнес логика может быть такая что по логом я могу определить Ну как-то замерзнуть две Ну два состояния которые разные Планируется ли как-то там дать пользователю возможность в некоторых сценариях например накрутить логику чтобы их автоматически без ручного вмешательства восстановить Ну там я так понимаю например lastride wins да можно сделать что-нибудь такое наподобие Ну допустим меня там инкременты какие-то их просто заплюсовал понятно Да мы про это думаем Но пока что только руками к сожалению главное что это в планах а добрый день по сути вы рассказываете о том что ну как бы это конечно надстройка на драп там но по-хорошему ну в некотором плане потеря обратной совместимости думаю вы как-то распространяет свои практики на Ну как обобщать грубо говоря общий стандарт Трамп потеря обратной совместимости можете пояснить в чем в том что мы разрешаем менять кворум например или используем асинхронную репликацию Ну запись промоут допустим понимаю стандартном рафте нет да стандартном рафте их нет но они нам приносят какую-то пользу только когда мы пользуемся рафтом нестандартно То есть если если мы оставляем кворум большинство строго если мы используем только синхронную репликацию запись про мол для нас будет просто какой-то детали реализации внутренней которая на работу рафта не влияет А вот когда мы отклоняемся от канона когда мы понижаем кворум либо асинхронно что-то комите мы благодаря этим записям промоута можем восстановить историю лидерства понять В какой момент у нас разделение произошло этих двух веток и какой-то свою логику а поверх этого накрутить Ну вот конкретно сейчас просто не дать этим двум конфликтующим версиям соединиться Я думаю нет мы не планируем это как-то предлагать в канонический рафт Надеюсь ответил такой вопрос а рафт это надстройка над картриджем или часть его ни то ни другое это внутренности Тарантула самой коры в картридже недавно появилась опция выбора лидера с помощью рофта Вот то есть до сих пор это как картридж как оркестратор фактически назначал лидера А сейчас появилась настройка чтобы картридж смотрел как Тарантул внутри себя выбрал лидера и запоминал текущего сейчас плохо слышно было Ну то есть это все-таки будет устроено в картридж А вообще это будет доступно это уже доступно вот самые последние вещи про которые я сегодня говорил появились в Мастере в июне чуть больше двух лет в Мастере Так что да это доступно opensource Спасибо спасибо если у нас еще вопросы вопросы раз вопросы два Ну если у вас были вопросы то теперь вы не можете задать их спикеру потом Но сейчас они не будут участвовать в выборе лучшего вопроса все так мне наверное понравился второй вопрос больше всего про логику для разруливания вот этих конфликтов автоматически поднимите руку чтобы ваш подарок смог найти Ну что ж Спасибо за вам за ваше внимание и за ваши вопросы Спасибо спикеру за его прекрасный доклад Давайте поблагодарим его Спасибо Я надеюсь получилось проснуться Мы тебя тоже благодарим вот этим вот прекрасным подарком Спасибо еще раз для первого доклада это было Замечательно а дальше Мы уходим на перерыв и ждем вас на следующих докладах Спасибо"
}