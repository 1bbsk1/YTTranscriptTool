{
  "video_id": "exj0328IBDk",
  "channel": "HighLoadChannel",
  "title": "Сервис по оптимальному управлению складскими запасами на площадках ПАО НЛМК / В.Лепин, А.Ядров",
  "views": 107,
  "duration": 884,
  "published": "2024-04-17T00:59:46-07:00",
  "text": "коллеги Добрый день Меня зовут Владимир Лепин я главный специалист по анализу данных управление целями поставлен к со мной с докладчик Артем ядров Мой коллега мы занимаемся внедрением цифровых проектов продажи сегодня хотели бы рассказать вам про систему управления запасами шляпов на дочерних предприятии по войну к нам как Побег Артем тебе слово Привет коллеги Всем добрый день и на лунка клабек производит сталь для строительства судостроения энергетики транспорта и разных других отраслей основным Сырьем являются шляпы это металлический бруски определенной формы и химического состава они представлены на фото в последние годы появились проблемы с логистикой от Липецка доклабека нужно произвести слябы в Липецке затем отправить их по железной дороге в порт но в порту могут быть забастовки Потом пандемий проблемы с поиском судна после еще нужно дождаться шляпа для других европейских дивизионов чтобы погрузить их в одно судно также похожие истории на принимающей стороне еще бывают проблемы сфера сплавами в Липецке И это тоже влияет на клавиа сталь заказчиком нужна как можно быстрее поэтому есть два варианта либо держать на складе полуфабрикаты всех типов в большом количестве либо прогнозировать спрос и держать то что оптимально соответствует Этому спросу поэтому надо решать задачу прогнозирования стандартные модели прогноза дают очень плохие результаты но задача просто необходимо иметь на складе запас шляпов нужного сортамента на срок поставки срок поставки до начала кризисов был 6 недель с появлением проблем с поиском судов он увеличился некоторые поставки идут до трех месяцев Казалось бы достаточно заполнить склад всем нужным максимум и проблема решена но нет Это очень дорого слева нужно производить по запросу под конкретные большие заказы мощности в Липецке под высокой внутренней конкуренцией и просто так сделать большой запас впрок не выйдет да экономически это не лучшая идея просто взять и заморозить деньги на складе поэтому задача сводится к тому чтобы предсказывать спрос на конкретные марки металла на площадке ведется книга заказов есть коммитмент по планам площадки на производство и на закупку шляпов а также данные о плановых ремонтах оборудования Но даже при этом сохраняется огромное неопределенность ddmrp это методика моделирования планирование и управления цепями поставок для защиты и обеспечения потока правильных материалов и информации требования к ddmorpic к которым мы пришли подход ddmirp должен базироваться на обеспечение и защите потока релевантной информации и материалов это есть связь с лучшим возвратом на инвестиции должен обеспечить рассоединение для смягчения вариабельности в спросе и поставках а также сжатие летаем должен использовать самую релевантную информацию о спросе фактический спрос также должен обеспечить четкий и легкий интерпретации сигналы для всех ресурсов также подход ddrp должен обеспечить синхронизацию сложенной и динамической среды у подхода ddmrp есть несколько недостатков первый это очень большая неопределенность в прогнозе среднедневного потребления эдю второй большой разброс спроса До недавнего времени ediu рассчитывался о скользящему среднему потребление за три месяца этот метод не учитывает сильного влияния множество различных факторов наша модель использует множество факторов для составления более точного прогноза все они показаны в таблице модель которую мы используем это либо Случайный лес скалёр на Рандом Форест либо бустинг кадбуст где данных меньше и они более шумные там применяется более устойчивый лес в других случаях бустинг В некоторых случаях Также можно использовать ансамблевые модели оценка качества производилась на отложенный по времени выборки не более фиксированного числа раз Чтобы избежать проблем при множественной проверке гипотез а логика нашей модели обучены на большой выбор исторических данных хорошо согласуется с общей логикой менеджеров по снабжению важно чтобы результаты нашей модели можно было интерпретировать например высокие значения спроса в книге заказов и планов производства приводит к большему потреблению например Давайте посмотрим как модель анализирует одну точку данных на слайде вы видите интерпретацию с помощью известного фреймворка Шап чтобы оценить логику модели так результат работы нашей модели на тестовых данных отложенный по времени выборки показаны на этом слайде Вы можете видеть что с помощью модели машинного обучения можно значительно улучшить наши прогнозы на графике видно что модель на отложенной выборке в худшем случае достигает результата прогноза по среднему а в остальных случаях работает гораздо лучше иногда в разы далее далее нашу презентацию продолжит Владимир Да коллеги соответственно возникает вопрос что дальше делать с нашими замечательными моделями которые мы построили которые действительно по отложенной по времени выборки обеспечивают более низкую ошибку Казалось бы да берем ошибку моделей подставляем в известную формулу фетра для расчета страхового запаса и подставляем да непосредственно также стандартное отклонение или Time которое у нас есть по историческим данным и считаем страховой запас стоит Напомнить Да откуда данная формула берется данная формула выводилась из предположения что как Диман распределение ошибки Диман так и расстояние ошибки или тайма являются нормальными независимыми друг от друга распределениями поэтому их сумма также является нормальным распределением и по формуле свертки можно оценить также Вот этот посчитать плотность суммы двух независимых нормальных распределений получается тоже нормальное распределение так ли это в реальной жизни Конечно нет вот и в этом можно сказать основное ноу-хау нашего продукта том что мы не старались а проксимироваться реальное распределение по отложенный строка По времени выборки невозможно добиться нормального распределения ошибки прогноза модели факту потребления то есть Это какой-то недостижимый идеал потому что ну продажи Это сложный динамический процесс зависит от кучи факторов Невозможно его так смоделировать чтобы ошибка была нормально поэтому мы решили честно взять реальное распределение ошибки отложенный выборки взять распределение лет тайма можно было бы да аналитически посчитать также свертку этих двух распределений для этого надо было бы апраксимировать их там например гаус вами смесями Или другими подходами но неизбежно мы бы потеряли какое-то количество данных при этом подходе поэтому мы решили провести сотни симуляций и данных симуляциях каждый месяц для каждого из Каю ошибка прогноза случайно сэмплировалась из распределения ошибок прогноза а летаем доставки также случайным образом сэмплировался из распределения летаймов проведя таким образом сотни симуляции стоков для каждого из Каю мы смогли оптимальным образом установить страховой запас для целевого уровня сервиса для каждого из скайринной площадки примеры таких моделирований представлен на этом слайде понятно что это какой-то один разрез То есть если мы например для СК ю под условным номером 1115 провели бы еще одну симуляцию там была бы чуть другая ошибка чуть другое было бы время доставки Вот но видно что с помощью такого подхода мы можем довольно точно оценить какой в среднем необходимо страховой запас чтобы покрыть неопределенность и добиться необходимо целевого уровня сервиса заданном заранее проценте случае также стоит отметить что в компании НЛМК цикл разработки модели не заканчивается просто каким-то Black Box или например даже дотер контейнером с описанным API который отдали дальше разработчику и использовать его как угодно компания nonka целью является по разработке моделей Потому что таким образом гораздо проще интегрировать новых людей подключать к разработке поддерживать данное решение поэтому все что начинается от первичного загрузки им YouTube был слоя данных в аналитическое хранилище до непосредственно уже контейнеризации разделено на стандартные компоненты все эксперименты версионируются через также аналитическое хранилище и в итоге мы получаем ML pipeline и далее мы можем подключать людей к разработке например какой-то мало его части например немного улучшить модель человеку не нужно погружаться весь контекст тянуть все решение он подключается к одному компоненту у этого компонента есть входы и выходы определенные далее будет производить версионирование его экспериментов любой эксперимент всегда можем воспроизвести и таким образом достигается гораздо более короткий скажем так бас-фактор и гораздо проще и дешевле поддерживать все наши модели для контейнерализации используем Framework Bent eml с открытым исходным кодом который поддерживает все основные библиотеки его можно докрутить добавить туда и к от Boost все что угодно Да на основе также сканер это делается скальт артефактов Вот данный фреймворк уже позволяет создавать контейнеры с готовым restop и сервисом для того чтобы в дальнейшем подключать либо токер компост файл для разработчика либо кластер куберетесь где уже развернут какой-то Back and приложения и использовать при этом имели инженера достаточно иметь несколько инженеров на всю компанию они в каждой продуктовой команде Потому что им нужно настроить универсальный pipeline по разработке ml-моделей а далее каждый даты сантис там почитав документацию может сам создавать контейнеры которые будут в корпоративном репозитории сохранятся и откуда разработчик всегда может их поднять архитектура итогу продукта представлен на сайте Да у нас есть две пазы данных менуэз Это для того чтобы версионировать Excel файлы с площадок которые могут в которых типа данных могут плавать К сожалению до сих пор такое встречается То есть например могут прислать файл Где в одном из полей тип данных сменился на стринг Да там поставили что-то через дефис добавили какую-нибудь букву такое бывает Поэтому Чтобы избежать ошибок мы частично храним данные минус далее конечно используем уже после того как Чистим от таких ошибок и укладываем в одном кластере cubernets у нас развернуты непосредственно вот все поды с модельками также Back and и frontent приложение там соответственно общаются и также снаружи пределами небольшой такой сервис наш для мониторинга моделей чтобы отслеживать даты дрифты концепт дрифт Да собирать данные моделях которые уже работают продакшене и чтобы можно было оперативно вмешаться переобучить добавить новые данные вот поэтому конечно используем коллеги Спасибо за внимание если какие-то вопросы остались будем рады ответить"
}