{
  "video_id": "16qnYKQRqY8",
  "channel": "HighLoadChannel",
  "title": "Хочу знать, сколько посетителей было на сайте в прошлом /  Константин Игнатов (Qrator Labs)",
  "views": 424,
  "duration": 2580,
  "published": "2017-04-22T14:48:23-07:00",
  "text": "мы сегодня будем говорить о том как считайте элементы в множествах ну в программе у нас написано что я там хочу посчитать количество уникальных посетителей моем сайте за любой промежуток времени в прошлом ну мы будем в общем то решать чуть более общие более интересную задачку эту конечно как частный случай но тоже охватим более подробно мы будем значит смотреть на такую заречье нас есть поток элементов которые в нас летят что-то там они нам показывают и мы хотим знать сколько был без повторов естественно не могут повторяться в этом потоке мы тем не менее вот хотим как-то так посчитать значит это мог где нужно практически везде где мы можем написать цен аккаунт distinct или там sort of поэту и не книгу н но как правило здесь в этих задачах далеко не всегда нам нужно прям иметь точный ответ и даже если мы слишком не точно получим слишком плохой ответ вполне возможно что нам у нас устроит поэтому например в таких задачах как там при веб-аналитики или при оптимизации запросов к базе данных пример чтобы понять предварительную сколько там находится элементов в joy не да мы можем взять и сделать более быстрый какой то проход чем нежели использовать какой-нибудь сорт минус n и вот тут я и хочу только добавить 1 что пол беды когда у нас есть какая-то большое множество и в нам нужно что-то посчитать это вот реально полбеды настоящий проблема возникает тогда когда нас таких множеств несколько нас есть по сути декартовой опередим произведение их и нам нужно посчитать какие-нибудь сочетания пары группы уникальные такое часто встречается в при поиске паттернов там в да это майнинге в графах и так далее и вот это по-настоящему больно вот прям очень никакие большие данные уже практически не работать тем более что даже если это можно решить он прямо очень сложно значит давайте чуть подробнее как вот точнее формальные да что мы будем делать значит у нас есть поток из элементов есть некоторые универсально множество куб в нем есть собственно подмножество тех элементов которые мы увидим и есть оценка количества элементов это множество точная принятые обозначать как f0 и мы очень хотим ее найти ну пол века примерно уже существует быстрый хороший алгоритм того чтобы считать это точно например вы можете это делать при помощи списка отсортирована во или создать большущую убитого you маску ну понятное дело что рано или поздно все равно упретесь какой-то предел да и как только эта возможность возникает не при этом все-таки хочется что-то такое посчитать мы естественно переходим к желанию попробовать сделать это приблизительно приблизительно можно попробовать записать вот тогда то есть у нас есть какая-то оценка которая на какую-то небольшую величину меньше там плюс минус эпсилон точнее говоря до отличается от настоящей оценки но проблема в том что это не работает то есть можно теоретически показать что любой алгоритм который будет делать ровным счетом вот это вот либо будет потреблять столько же памяти стоя сколько потребляет настоящий полноценный до термини раваны алгоритм до либо будет увеличивать оценку и то есть мы не сможем и точнее говоря гарантировать точность она будет увеличиваться экспоненциально с уменьшением количества памяти которые мы потребляем поэтому возникает возникли разные идеи как это делать первая статья появилась в 1983 году один из авторов и и это филипп сажали такой известный французский наверно можно сказать первый вообще в мире человек который можно назвать да это санте ст и задача довольно быстро активно развивалась что мне вообще в ней нравится в этой задачи до что она относительно кажется такой простой и понятный но с другой стороны до сих пор не решена и вот прямо на наших глазах ее решают например то что я вам только что сказал по поводу того что приблизительном алгоритм но детерминированным решить ее нельзя это было доказано формально только в девяносто шестом году с другой стороны именно какие-то теоретические обоснования того какой должен быть оптимальный алгоритм для этой задачи появилась только в 2003 году алгоритме который сейчас везде используется тот же хитрил облако появилось только 2007 на его и его модификации которые сейчас уже стали более менее нормально местами появились только в тринадцатом при этом алгоритм с заявленными оптимальными свойствами но некоторым варианте которому мы сегодня будем говорить появился только в 10 в одиннадцатом году филипп вложили umirov наверное он так и не увидел что все представлял в итоге развития его алгоритмы там для гугла ну и как как они его улучшили но еще я хочу заметить что вот в двенадцатом году например появился алгоритм который может быть не совсем такой как бы идеальный но с другой стороны он не использует ничего из того что предыдущие авторы из того что мы сегодня будем обсуждать использует использовали когда-либо то есть это значит что эта задача все равно еще до сих пор вот до конца не решен не лишена это значит что кто-нибудь не там из нас например вполне возможно через какое-то время возьмёт и решит история развития математики алгоритмов происходит у нас на глазах интересно значит какой наш план действий я края кратко расскажу про этот алгоритм чем кратным почему пройдемся по его этапом кратко да потому что почти во всех алгоритмах которые работают с структурами вероятностями собирает алгоритмы и очень простые чисто по тому какие этапы они делают но очень сложно понять почему это работает и вот по этому больше внимания уделим на то почему это работает мы посмотрим какие из каких элементов это этот алгоритм состоит и как бонус мы получаем то что эти элементы которые мы рассмотрим работает и в других алгоритмах то есть есть другие задачи где есть другие алгоритмы и в них используются те же самые алгоритм и те же самые элементы так что придя домой вы сможете разобраться в целом ряде других задач в том числе например в задаче поиска наиболее часто встречающихся элементов или оценки чистоты самого встречающегося элемента мы таких задач на самом деле много и но прежде чем наверное разбираться и даже прежде чем разбираться с алгоритмом который мы сегодня будем ждать обсуждать я бы хотел поговорить чуть чуть фильтров это такое множество которое позволяет нам от который по сути это элемент который имитирует поведение множество он появился еще еще раньше чем тот алгоритм который мы говорим даже как задача да она появилась в 70-ом году ну дом фильтры это такие квадратики столиками да это как какой-то объем маленький кусочек памяти в котором вначале написано 0 и потом мы добавляем какой-то элемент полностью его туда копируем когда добавляем следующие элементы копируем их при помощи операции или то есть если в каком-то месте в каком-нибудь элементы стоит единица то мы копируем эту единицу углом фильтры если там до сих пор остаются yall 0 ч нам это даёт ну например если мы берем какой потом уже посчитав все элементы до брюс фильтры берем какой-то новый элемент мы можем неизвестным был он там или нет мы можем оценить был он или нет посмотрев на то в каких единицах в каких местах у него стоят единицы если фильтры во всех везде где стоят единицы в нашем представлении этого объекта и в самом будете видеть единицы совпадают то мы считаем что мы видели этот элемент ну понятное дело что эта штука может ошибаться я думаю не сложно привести пример да но когда она не ошибается такая это конечно говорит что мы такой элемент не видели едем будем фильтр очень интересным ну то есть если где то стоит единица в фильтр был ноль то начнут точно не видели на сто процентов подскажите как вы думаете можно жилье до убирать элементы из глухих города tucson имени имитирует поведение множество а можем из множества мы обычно можем убрать элемент отсюда можем нет действительно стандартной реализации не можем как можем ну мы самое простое что можем сделать это вместо вот этих единичек иметь счетчики сколько раз мы увидели такую единицы на такой позиции в этом случае мы спокойно можем удалить один элемент просто сделал в декремент да это тоже вероятностью структура мы можем удалить лишние так далее но работает что отсюда нужно вытащить это идею того что такое скетч скетч это некоторое представление о том что мы можем и сохранять не всю информацию которую содержит который нам дали да а только то что необходимо для того чтобы с нужной вероятностью решить эту задачу которую нам нужно решить в контекст вероятностных алгоритмов как правило это означает что у нас есть некоторые свойства каких-то элементов мы его только это свойство сохраняем каким-то образом обереги раем и его главная особенность в том что мы потом берем это свойство считаем от его функции от него математическое ожидание получаем сразу ответ того что нам нужно с какой-то вероятностью поэтому делать что раз это вероятность а значит нас интересуют еще и дисперсии этой величины понятное дело что вот она как раз эпсилон тем самым которое мы только что писали связано есть это вот минимально какое-то такое представление занимает гораздо меньше памяти может ошибаться но дает ровно то что нужно поэтому давайте ли это минут теперь давайте раз дам какой-то вот алгоритм я его очень очень сильно упростил вот насколько вообще было возможно вот ровно счетом для того чтобы он давал какой-то результат при заданной точности которые я тут даже зафиксировал мы выбираем какое то случайным образом простое число это может быть например 2 6 1 минус 1 ну или здесь у друга и не обязательно прям точно и простое число можно приблизительно я потом задаем 3 вот таких функций в них числа заданное тоже случайно и создаем массивчик м зависит от ип силуэтом и еще потом посмотрим вас еще и делаем вот так просто значит считаем этот зверь хэш-функции считаем это последне хэш-функции листы гнев к мбит то есть это такое грубо говоря количество нулей с конца бинарного представления этого хочу этого хочу из сохраняем в массив значений а если она что написала больше так мы добавляем каждый элемент как только нам нужно будет оценить сколько же у нас элементов вы просто берем и считаем буквально по формуле получаем оценку с некоторой вероятностью то есть во первых с какой-то ошибкой во вторых с вероятностью еще больше шипица есть это в общем то свойство всех практически вероятностных алгоритмов они из себя представляют ровным счетом вот что у нас есть оценка мы можем в ней ошибиться и можем добиться еще сильнее с какой-то вероятностью это называется эпсилон-дельта аппроксимация ну я думаю понятно что мы можем вот эту вероятность которой машина ошибаемся уменьшить произвольный да за счет того чтобы будем повторять элементы как этому делается ну мы тоже немножко поговорим но я думаю в принципе вы догадываетесь мы возьмём несколько раз запустим это дело и в данном случае нужно будет посчитать медиану но вероятность того что мы получили правильный ответ быстро увеличиваются на самом деле не очень быстро но при большом он стремится к единице теперь давайте начнем говорить об элементах алгоритма ну самый большой и такой наверное слон который здесь была та хэш-функции да вот эти вот странные функции какие-то были мне понятны числа написанные ну хэш-функции это просто отображение из этого нашим универсального множество у на множество чисел от 0 до им или от нуля до единицы при том что их какое-то но конечное количество это не криптографических функций нас не интересует многие функции многие возможности которые нам предоставляют криптографии но главное для нас чтобы в общем то они считали быстро и имели определенные свойства чем они нам нужны вообще эти хэш-функции дело в том что для того чтобы наш алгоритм как я потом еще покажу нормально работал нам нужно чтобы распределение элементов из универсального множество до было одинаковым равномерным если рада встретить каждый элемент должно быть одинаковым такого не бывает например если мы считаем количество уникальных айпи адресов которые посещали пам пам его ли на вашем роутере то скорее всего вы больше увидите опять ирисов из россии несмотря на то что в россии может быть даже ip-адресов ты поменьше чем у некоторых других странах откуда вы их оттуда этих айпи адресов никогда не увидите вообще хэш-функции пытаются решить именно это они пытаются сделать такое преобразование чтобы у нас получился равномерное распределение и в этом случае мы сможем обращаться с этими вещами как с случайными речевыми и дальше все красиво и здорово анализировать можно добиться этого например так да взять большую щий векторы с размером в количество всех возможных уникальных возможных элементов из множества ум посчитать до каждого из них какую-нибудь случайную величину и все это сохранить это очень много памяти но это будет работать на практике конечно нам еще часто нужно несколько хэш-функции они должны быть независимы друг от друга ну и поэтому конечно используется что то другое мы берем некоторые детали минированную функцию считаем для неё фиксируем нее несколько параметров если она многопараметрические до оставляем один параметр и получаем хэш-функцию и для того чтобы сохранить такую хэш-функцию нам достаточно только сохранить вот эти параметры как правило это столько же сколько занимает самого саванны сама значения хэш-функции что давайте я на картинках под покажу что общем то имел ссуду под этим равномерным распределением у нас есть какие-то элементы здесь в каждом столбике по горизонтали да может попасть как некоторое количество элементов тут по моему сотни высота столбика означает сколько на самом деле их туда попала ну и в случае вот как вот наверху мы так хотим мы хотим чтобы у нас изначально была вот так и в этом случае никакая хэш-функция нам не нужно на практике у нас могут быть разного рода пики и простые даже очень простые хэш-функции здесь нас спасут причем они будут работать до довольно таки с разных видов пиков довольно странных распределений для мультимодальных распределений как правило будут но конечно всегда всему есть предел если распределение уж очень неравномерная если где то уж очень есть большой пик томанах даже на преображение михаил фишман сам хэш-функция мы то увидим ну как бы здесь без вариантов то или иное искажение произойдет а если вот мы возьмем такой вот такую хэш-функцию если это в принципе линейная функция в которой мы взяли два числа и взяли по модулю от 3 давайте посмотрим ну для равномерного почти равномерно получилось для пиков ну в принципе тоже для разных пиков со вроде работает даже внук мыть модель все отлично ну да все равно если из большой пик все равно не работает точно также так зачем тогда использовать какие-то более сложные хэш-функции вообще почему это заработало какие здесь есть проблемы давайте подумаем в отцу о чем что нам дает универсальная у него не просил прошу прощения абсолютно случайно хэш-функция потом счет она нам говорит что если мы возьмем произвольное количество элементов и попытаемся угадать для них хэш то вероятность для этого это того что нам это удастся будет единица на количество элементов в множестве на которые отображают хэш-функция в степени сколько раз мы попытались сколько элементов мы пытались угадать это очень крутое свойства настолько крутое что но на самом деле оно не нужно вот просто она на мне сдалось честно поэтому потребление памяти абсолютной случайно хэш-функции нам просто не нужно вместо этого на практике нам как правило достаточно того что мы возьмем нет 2 там 34 х чувствах и шеи и попытавшись угадать их одновременно получим вероятность единицы на там 2 в степени k на m в степени k прошу прощения никаких гарантий относительно того что будет если мы пытаемся угадать одновременно несколько хэш-функций нет здесь например вы можете увидеть что поскольку это линейная функция из от двух по сути переменных если мы попытаемся дать три хэши там мы сможем спокойно это сделать мы просто можем выразить третьих аж через два предыдущих может быть нам в некоторых случаях когда вы достаточно может быть нет но есть раз есть естественное расширение для практически произвольного количества переменных которые мы должны угадать но свойства здесь интересное заключается в том что это будет работать только в том случае если мы вот эту хэш-функцию выбрали из некоторого семейства и выбрали ее случайным образом это не тоже самое что мы взяли свою абсолютно случайно хэш-функцию вместо этого взяли из некоторого семейства в апреле случайную посчитали и получилось что-то интересное и мы действительно можем это использовать на практике ну и наверное вы уже заметили что я вот говорю про поймать хэш-функций про повторения и вы понимаете что речь идет о том что мы пытаемся взаимодействовать уменьшать либо по либо ошибку либо вероятность того что мы ошибёмся и для этого против этого в общем то играет памяти к правило ну не только ну например при павку попытки увели уменьшить вероятность ошибки у нас еще и и уменьшаются а увеличивается затраты цепи ум да то есть будем нам нужно более больше операций проделать а вот если нам нужно увеличить точность то как правило это не так сильно заметно это интересным тогда мы будем использовать несколько хэш-функций для этого давайте ещё поясню что вот мы конкретным понимаем под туре потреблением памяти наш вот этот алгоритм он потоковый это значит что по идее мы можем в любой момент переключить его выполнение с одного исполнителя на другого и вот у нас была какой-то исполнитель алиса которая получала все планы все элементы она считала какую-то внутреннюю machinery you do пыталась оценить сколько элементов в множестве находятся а потом мы берем мы говорим и и вот алиса давая отправляй богу теперь всю информацию которую тебя есть и дальше бог будет видеть все эти элементы и вот алиса должна при этом отправить одно сообщение богу это односторонняя связь размер этого сообщения это и есть потребление памяти нашего алгоритма то есть она должна передать внутреннее состояние и отсюда я думаю вы даже понимать что мы сразу имеем отличные возможности для того чтобы проверить эти алгоритмы они замечательным образом должны работать на сколько угодно и сколько угодно в количестве исполнителей ровным счетом потому что мы можем вот этого делать и мы знаем сколько нужно передать информации они эти алгоритмы созданы такими теоретический предел такой значит единиц написано квадрат плюс логарифм н логарифм н это столько сколько хватит для записи 1 элемент из множества у а единиц на epson faded это количество хэш-функций который мы будем использовать текущей алгоритм который везде и сейчас используется имеет свою имеет потреблением памяти единиц на epson квадрат логарифм логарифм логарифм логарифм это память это этой памяти хватит на то чтобы сохранить индекс бита в некотором числе из множества давайте вернёмся теперь собственно подсчета элементов для этого я бы хотела задать пару вопросов относительно будем фильтра я думаю вы сейчас сидели много них думали вот сколько элементов могло создать вот такой был у mfilter но один ровно никаких других вариантов быть не может а вот здесь сколько кто считает что два кто считает 103 то есть что один на самом деле все это все ответы верны то есть это может быть один элемент может быть два может быть 34 может быть 5 нет не может быть потому что если бы это было так то у нас появился бы какое-то другое число здесь одна ну произвольной длины какой-то это был фильтр мы взяли бинарном представлении сохранили его сюда одну единицу могу добавить один элемент еще одну мог добавить еще один элемент и спокойно два элемента могло быть до могли добавить в две единицы мог давайте еще один элемент ну мы не считаем что существует но легкий но проблема здесь в том что при помощи будем фильтр не очень получается считать элементы да если у нас к единице заполнены то мы имеем только верхнюю границу это все еще по-прежнему могут быть один элемент могло быть ну и там на чьи-то единственно просто 2 в степени k 1 кажется но и проблема в том что даже если все они заполнены это все еще могло быть мог быть один или единственный элемент а верхняя граница все равно получается огромные не очень работает ну так давайте попробуем сломать этот будем фильтр таким образом чтобы он чуть лучше выполняла наша задача значит мы будем перезаписывать не целое бинарное представление а вместо этого мы возьмем только последний значимый бит из него и запишем на одну эту единственную единичку если соответственно мы берем больше элементов то мы записываем единички из них на все остальные элементы мы не смотрим и вот здесь мы уже имеем во-первых нижнюю границу да мы знаем точно что если есть три единицы то как минимум три элементы были записаны на с другой стороны как мы сейчас будем смотреть мы еще можем лучше судить о том сколько элементов было записано смотрите у нас было некоторое множество всех всех возможных бинарных представлений который мы записывали в плен фильтр мы считаем что они равномерно распределены вопрос сколько существует хэш в значений у которых там 40 а в конце 1 если есть 10 таких элементов половину понятно да если два твоих уже четыре единицы на 2 в квадрате ну если к нулей в конце таких элементов всего-то навсего 1 на 2 в степени k то есть это очень редкие элементы вероятность встретить такой элемент очень не были невелика открыл же то есть мы можем посмотреть единицы на 20 паника-это вероятность встретить этот элемент единица минус единица на 2 в степени k вероятность его не встретить если мы возведем последние в степень n то получим вероятность сегодня не встретить за просмотр в за инком попадание элементов ну и единицу минус это наш ответ хорошо есть известные алгоритм который сейчас используется который использует это свойство понятно наверное что ответ должен быть пропорционален единицы 2 в степени k мы просто 25k отель будет 20 паника на какую-то константу с другой стороны понятно также что здесь будет будут ответа пропорциональные степени двойки чтобы нам здесь что-то решать нужно сделать что-то похитрее ok давайте посмотрим в какую константу насчитал мессия слышали от ну блин чего чего вы хотите вложили основатель аналитической комбинаторики у google за кстати говоря немножко другие константы и они считают что вам зависит она не зависит от м ну и очень долгое время вот собственно говоря до десятого года возникали вопросы относительно того можно ли с этим что-то сделать об этом чуть позже здесь проблема еще в том что у нас нам нужно провести эксперимент несколько раз при этом мы бы не хотели повторять поток мы не мы не им бы мы не хотели его несколько раз считать действительно для этого мы можем завести еще одно когда можем решить и эту проблему мы можем завести еще одна хэш-функцию которая как при балансировке нагрузки только не раунд роббеном будет раскладывать в некоторые корзинки элементы из потока и в результате каждый в каждой корзинки обязательно будут пока попадать одни и те же элементы одинаковые элементы будут попадать в одну корзинку и мы можем теперь рассматривать каждую из них как отдельную не зависть мы поток в котором можем мы выйдем который мы можем запустить алгоритм и вот посчитать единицу 2 в степени k окей так вот небольшое отвлечение посмотрим на то что происходит когда поток только начинать к нам поступать вот он к нам начал поступать к нам прилетела примерно но не м примерно м значение данным пускай даже м вопрос сколько корзинок после этого осталось пустым какое какому какого математическое ожидание пустого количество корзинок ну я здесь расписал там принцип абсолютно такой же в принципе как вот мы уже два раза рассматривали здесь получится такая просто формула из которой мы потом хочешь можем сделать но пока это просто отвлечение здесь мы смотрим вот с нешто у нас есть значит алгоритм мы его разделили зделе теперь из каждого из этих потоков средняя оценку и получили уже что-то непропорциональные степени двойки вроде неплохо и точность получится эмпирически как установленную 078 на корень из m корень из m это количество хэш-функций память будет единиц нравится накладывается логарифм н напомню сейчас алгоритм который наиболее часто используется имеет логарифм логарифм н в этом месте ну а заменив в игре арифметическое средние на геометрическое ушло кстати говоря несколько лет на то чтобы человечество могло это придумать мы увеличиваем цен качество наших предсказаний ладно давайте сделаем пару наблюдению на которой я на глаз наводил нам не нужно во-первых естественно хранить сами и регистры то есть те самые значения в которых тех самых двух фильтров нам достаточно помнить самый старший бит потому что это и есть самая редкая птица и вот как только мы это делаем и нас получается место если мы вместо этого они такую штуку то у нас получается нужная нашим нам сложность мне сложность на потребление памяти space complexity еще одно наблюдение у нас есть массив чисел которые могут в этих регистров записали то теперь там находится максимальный индекс у нас был равномерно распределенный поток он равномерно попадал во все корзинки это значит чтобы большинство элементов этих корзинках будут примерно одинаковыми ну значит вам вместо того чтобы это делать вместо того чтобы хранить все это дело можем взять и сохранить минимум все остальное сохранить как он велик как количество которые отстают от этого минимума и здесь вдруг оказывается что существуют опять же не так давно изобретен и и компактные структуры данных которые позволяют сохранить вот этот вот массив в виде некоторыми ты в маске размером с пропорциональным размер который будет пропорционален сумме регистров ну и если посмотреть на практике просто запустив много раз этот алгоритм то мы увидим что она не зависит от количества н не зависит ни от чего не зависит от количества м зависит только от количества хэш-функций но это где-то 3-4 из них для сравнения для того чтобы хранить сами регистры без сжатия да нам нужны в современном мире 66 бит это логарифм от 64 индекс в 6 64-битного числе должен помещаться в 6 бит здесь мы соответственно вроде как можем хранить 34 беда из некоторые можем сказать наша нашему алгоритму что теперь если ты вдруг должен сохранить больше чем вот столько не сохраняя говорит и не смог справиться и в этом случаем мы получаем что 99 процентов случаях все будет работать в одном каком-то случае мы будем вылазить за этот предел и тем самым мы будем оказываться в неплохой ситуацию до когда наш алгоритм не сработал он вероятность это все равно может произойти а тут veritas и так маленькая вот этот мы вспоминаем ту форму которую я только что нам упоминал и в общем то мы можем посчитать ее не только для начала потока ну и в принципе для произвольного моментов потоки если у нас есть некоторые представления о том как у сколько находится элементов не обязательно точно и но какое то как только она у нас есть мы можем выразить математическое ожидание логарифма гарем математическое ожидание количество пустых корзинок а из них мы можем вытащить мы можем вытащить из комы нам f0 это будет та самая формула которая была на странице суд алгоритмом я не буду показывать потому что это не для слабонервных все таки вот теперь вернемся к тому что у нас написано в расписании если я все-таки хочу считать за любой произвольный момент времени в прошлом что я могу сделать ну у нас хранятся все элементы мы можем их сохранять раз несколько секунд объединять по принципу максимум или минимум зависимости от того какой вариации алгоритм мы используем и получать результат это мы можем сделать не только для произвольного интервала времени но для произвольного вообще объединения любых интервалов ну и наверное некоторым я думаю очень сильно интересно где это вообще что такое на практике реализовано на практике больше чаще всего реализован либо гипер log dog либо его версия от гугла я расскажу про редис где просто есть отдельные структуры данных которое позволяет хранить то что они называют гипер но блок юмор она ведет себя точно тоже так как мы все не обсуждали это множество в которые можно что-то добавлять а потом и оценивает сколько мы элементов добавили и второй такой распространенный на сегодняшний день система это treehouse увлекался все еще лучше там есть отдельная структура данных отдельный тип данных прошу прощения которые означают состоянии вероятностного алгоритма вы можете во первых призы использованием при запросах вообще использовать вероятностный алгоритмы напрямую больше того некоторые в некоторых случаях они используются по умолчанию и их состоянием можете напрямую сохранять а потом объединять так что на этом мне пожалуйста все я очень хочу ответим на все ваши вопросы задавайте пожалуйста спасибо за доклад очень интересный вопрос если какой-нибудь просит от использования нелинейных семей с хэш-функции и оказать на линейных семейства вообще насколько оцени мой вероятности зависят число параметров вопрос в том какие хэш-функции используют использовать да ну и на этом и дайте из вытекает из теории я не просто не стал это немного разбирать дело в том что мы вот здесь пропустили некоторые моменты как доказать что то или иное математическое ожидание будет работать при вот этих к как называемых к независимо хэш функциях в от ровным счетом из вот этого предела зная где какое количество конечно функций мы даже использовать подробно оттуда и вытекает к который нам необходимо те хэш-функций которые я показал линейные они действительно не единственные которые бывают существует несколько разных видов таких хэш-функций и нам нас интересует только вот этот параметр к это совсем необязательно степень полинома хотя вот здесь это действительно так почему слышь кстати спасибо за доклад откуда там взывать вот десятичная степень по модулю в этих функциях потому что зачем переходить в поле вычетов в общем то понятно там ну все на этом построена а зачем вот там какая-то достичь ность по модулю в двоичной машине откуда она там взялась и вобщем-то ну значит вопрос про записи с модулями да во первых олег там просто напросто берется итак мы берем модуль 10 штук это просто корзинка у нас 10 корзина к нам нужно распихать по 10 корзинку штата в этот вот ровно ровным счетом отсюда берется 10 в друга и в другом случае модуль там это 10 в кубе там просто так получается что нам нужно сначала поместить туда потом туда просто для того чтобы нас не получилась слишком большая слишком сложная хэш-функции вот по поводу того что это бинарные машины я тут есть пара известных слайдов наверное интереснее будет брать по модулю на самом деле дизайна удобно когда он раз там какая-то степень двойки это просто взяли сместили вот некоторых других случаях нам все равно нафиг вариантом все равно не нужно ничего делить не переживайте это все равно простые операции которые мы можем достичь того же самого нам ран в любом случае нам не нужно делить нам нужно просто расположить элемент в разные корзинки так или иначе то есть если какая-то функция ведет себя очень похоже на деление но при этом это на самом деле не остаток деления но вот вот этим вот свойствам раскладывания в корзинке одинаковым равномерным она обладает у нас это устраивает вот например последняя хэш-функции которая здесь написано по-моему сейчас используется в тендер flow кажется то есть совсем недавно ее туда добавили очень сильно увеличили скорость вычислений буквально просто потому что как раз перестали использовать деление по модулю спасибо можно еще такой молить его партнера какие то вот конкретные числа новый стоит стоит задача с которой мы начали что у нас есть сайтом я не знаю миллион посетителей в день каждую секунду мы храним вектор состояния такой-то длины там но у вас нет пример какого-то вот или конкретных чисел потому что формула достаточно запутанные это как оценить как бы вот на глаз сложно хорошо значит смотрите что нужно для того чтобы представлять да я рассказал примерно потребление памяти но по действительно очень теоретическое на практике для того чтобы сохранить с точностью вот которым я там указывал одна треть имея возможность ошибиться на плюс-минус треть нам нужно сохранить десять-двенадцать хэш-функций каждая из них соответственно занимает 6 бит это в случае hyperloop блога плюс значение хэш-функций вот столько мы должны хранить каждым в каждой манга ветвей момент времени которые нас будет интересовать и ответил спасибо пожалуйста 1 вопрос еще вопрос есть погуще очень интересного рассказать если хотите про предыдущая версия алгоритмов например спасибо за заклад сразу скажу что я не все понял честно говоря но я вам хотел задать вопрос вот вы говорили как вот например каждую секунду происходит какая-то запись в максимальных и минимальных параметров ну какая то нам нормализация данных а вот предположим условий по которым нам нужно вот сохранять этих ну как бы интервалов очень много например не просто количество пользователей просто сколько зашло а например там взошло с определенной стороны имела какие-то параметры в url и вот тогда получается у нас имеется ввиду большое количество параметров большое количество интервалов и все это дело я не знаю вот и использование этой получается методики уже как бы не целесообразно ну если вы просто хотите посчитать количество уникальных ну да руб да то есть вы хотите знать сколько было человек людей оттуда при таких условиях при таких условиях и два человека например из россии которые сидят с мобильного интернета из россии которые сидят с домашнего интернет например для вас разные и вот то в этом случае у вас как раз призвук происходит то о чем я говорил и сначала у вас большие множество происходит их декартова произведения вам в них нужно считать перемножении в этом случае эти алгоритмы как раз ради практически единственное что вообще может работать здесь будут другие числа здесь нужно будет использовать другие хэш-функции более крупные так далее вас все равно будет больше памяти люблю нужно сохранять но это будет работать и это единственное что будет работать ясно хорошо спасибо я думаю все спасибо большое"
}