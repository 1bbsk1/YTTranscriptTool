{
  "video_id": "1AMYLbWQ29M",
  "channel": "HighLoadChannel",
  "title": "Худеем вместе: отбрасываем лишнее железо с прода  / Иван Давыдов (Яндекс.Деньги)",
  "views": 881,
  "duration": 2493,
  "published": "2020-04-27T12:13:02-07:00",
  "text": "итак эм привет меня зовут ван я работал к там я занимаюсь исследованием производительности да это время много у нас поменялось допустим когда я пришел нас было всего лишь трое задачи в основном состояли в исследовании фичерили знак сборок на тестовом стенде анапа и моего ходили не так часто со временем на стало большим как и задач мы развили новые направления поставили то автоматизировали многие процессы она бой выходим теперь намного чаще по несколько раз даже бывает на неделе и к краске моя история будет об одном из таких направлениях которые мы развили расскажу я сегодня об исследованиях в рамках capacity management моя история будет о тяжелых бессонных ночах когда мы сидели и обстреляли сервера про да да также с котейкой с кофе в зубах с помощью яндекс танка мы провели более сотни street проанализировали 1000 срок логов проанализировали кучу графиков копались настройках приложений многое другое а собственно зачем необходимо было навести порядок на проводим разгаре силе гостям которая бы работала неэффективно котором было сложно управлять также провести оценку приложений чтобы понимать как они работают и выявить и устранить узкие места которые нас ограничивают вкратце немножечко яндекс деньгах и нашей команде наш канал наша компания занимается электронными платежами и переводами а наша команда делают все чтобы повысить производительность в основном мы исследуем бизнес-процессы то есть те операции которые делают пользователя у нас каждый день будь то там перевода или платежи мы стараемся обеспечить некоторый запас чтобы моменты допустим какой нибудь распродажи наш сервис жил и мог обработать весь трафик который на него нахлынет проблема в том что он наших исследованиях мы исполним и исследуем целую цепочку запросов которые проходят через систему и мы решили как раз таки пойти дальше исследовать как те самые узлы системе а именно приложение и запросы которые проходят через них давным давно когда начну с предыстории вообще связи с чем у нас вообще данное направление появилась давным давно когда на параде когда еще не было исследования производительности напротив у нас раз пугался железный сервер самый настоящий бормотал который содержал себе монолитное приложение в данного сервера были клоны в размере м штук которые каждый себе также содержал монолитное приложение тем самым они образовывали кластер все бы хорошо но со временем компания приходит микро сервисной архитектуре и начинается великий процесс деления монолитного приложения на микро сервиса а куда пихать новые микро сервиса правильно туда же обратно в кластер в итоге получилось так что у нас на вроде было более 15 серверов каждый каждая но до которого содержит себя более 20 капель в кого прилла до приложения микро сервисов из того как то все сложно это умещать а управлять длину инфраструктуру и подавно тогда мы что хочет сказать по этому поводу такой подход в использовании железо крайне плохо во первых у нас просто-напросто не было точного управления приложениями мы могли управлять только всем сервисам всем серверам также у нас были проблемы с масштабируемости потому что они решались путем выкидывания железа в кластер и полным копированием уже существующих not то есть то отсюда у нас происходил перерасход ресурсов также было тяжело планировать закупки потому что непонятно было сколько надо выделить ресурсов мощно с ней под новое приложение ну а про утилизацию ресурсов думаю тут вообще ужас какой то был потому что у нас была одна общая миска откуда все ели и по идеям и когда начали изучать производительность в таком кластер выяснили то что у нас работа всего сервера идет со скоростью самого медленного приложения один сервер ой одно приложение у нас вообще может положить свой сервер и 1 мы решили прибегнем контейнеризации в принципе разделяя власть во всей красе что делаем берем упаковываем приложение в контейнер и разделяем под гипервизором лимитирует по ресурсам тем самым получаем гибкое управление изолированность вроде всё круто но остается вопрос а сколько нам вообще надо таких вот контейнеров поэтому для начала надо определить какой запрос по производительности имеет один instance приложение далее надо определить сколько нужно таких вот приложений для текущей потребности prada может быть нам не стоит кормить целую армию стоит сократить штат но и в конце надо определить что нас тормозит потому что преследование производительности важно не только нагрузить сервисы получить цифры а также еще понять причину что нас ограничивает хочет отдельное внимание уделите вопросы почему же именно на вроде да кстати извените за картинку это мой первый опыт в обрезании так вот тестовый стенд это не прот и как бы он не приближался продал никогда им не будет по крайней мере не в нашем случае да и представьте как это содержать 2 можно сказать про тем более только для нужд нагрузки от слишком расточительно не по capacity следователь и вторая часть это исследования на вроде они максимально похоже на реальную обработку трафиком потому что мы работаем с тем же окружением что я наши пользователи далее стоит определиться с методикой по которой мы работали она состоит из 3 частей эта подготовка следовательно дальше это сами проведение эксперимент само проведение экспериментов и про проанализировать результат остается первое это составить надо профиль который бы соответствовал реальному пользовательскому распределением а второй это провести серию стрельб и в конце остается собрать сливки это анализ результатов и заключения начнем с профилем профиль состоит из двух частей это сценарий подаваемая нагрузка сценарий может быть простой у нас есть сервис и одна ручка ну элементу но у жизни все гораздо сложнее и одного сервиса может быть два метода три или даже целая сотня чтобы померить и кучу емкость 1 инстанции можно нагрузить каждую ручку по отдельности но такой подход не даст общей картины потому что они ручки вызываются больше другие меньше при чаще или меньше или реже одни обрабатываются дольше другие реже и тогда возникает вопрос а какие же ручки стоят включать в сценарии для этого необходимо определить как часто вызываются методы и как долго они обрабатываются перемножаем два значения и получаем дал часто но если серьезно то получаем просто процессор на и время в итоге у нас есть красивый график с упорядочим списка вызываемых методов из графика видно что основной поток составляет первые 20 ручек сразу же можно из 50 8 отбросить больше половины потому что они составляют меньше пяти процентов всего трафика далее рассмотрим более приближенных график и посмотрим на суммарное процессор на и время здесь можно увидеть что 95 процентов составляет 1 13 ручек именно их и стоит включать в наш сценарий также если возникает сложность реализации обстрела конкретного метода то его можно в принципе исключите сценария но так чтобы у нас с esii суммарное процессор на и время было не менее 75 процентов то есть можно сказать что у нас здесь живое представление принципа парето когда 20 процентов нам составляет 80 процентов результата далее берем этот список и отсортируем по чистоте вызовов тем самым мы получаем те самые пропорции с которыми мы должны нагружать нас сервис причем здесь можно заметить что допустим ручки get статус рефанд и properties должна вызываться один раз в то время как инфа должна вызываться два раза при этом но есть один момент сценарий может состоять как из обособленных методов так и транзакций причем обособленно методы вызываются напрямую от транзакция мы должны как-то учитывать то есть они могут состоять также еще допустим из циклов то есть у нас внутри транзакции в цикле мы ждем получение какого-то результата это тоже надо учитывать поэтому у нас есть три критерия cценарий во-первых критерий сценария у нас может состоять как из обособленных методов так и транзакций и каждая ручка у нас должна вызываться с той же частотой что и напротив а транзакции должны вызываться чистотой инициирующего запросам количество повторов цикла надо ограничивать иначе какая-нибудь и назад убившая транзакция может быть просто-напросто заблокировать целые потока мы будем честно ожидать получения результата со сценарием вроде разобрались далее переходим к интенсивности у нас есть два стула ой у нас есть два две модели это открытая модель и закрыта в открытой модели интенсивность подается запросов в секунду в рпс и она не учитывает время ответа от сервера от сервисом и если используется генератор нагрузки реализующая открытую модель то в принципе реализация нашего сценария не представляют особой сложности мы просто наращиваем рпс и ловим точку в которой пробьем который сервис пробьет усилий и тем самым мы получаем предел работоспособности системы но мы стреляем на проводе поэтому эту сказать надо это делать аккуратно и поэтому и у нас сокращается значительно время на реакцию чтобы отследить когда у нас система упала поэтому лучше использовать закрытую модель закрытая модель подразумевают что нас есть время обратная связь это время ответа и то есть каждый поток у нас ждет ответ перед генерацией следующего потока и здесь мы уже управляем именно потоками с помощью такой модели можно оценить пропускную способность и поведение системы на максимальных нагрузках и такая модель лучше всего подходит для наших исследований а в качестве инструмента реализующего закрытую модель мы используем g метр дальше так как мы определили что мы будем использовать закрытую модель нам надо определить а сколько нам потоков выставить и как их наращивать поэтому это можно сделать выяснить эмпирически но это долго либо посчитать математически но это грубая будет и не точная оценка потому что мы не знаем как поведет себя системам и при максимуме поэтому можно сделать это это для начала делаем пристрелку то есть мы стреляем в прод пять-десять минут при низкой интенсивности в нашем случае это один поток но также если при таком тестирование мы абсолютно точно не навредим пользователям и не новым и не завалим прот если производительность даже при одном потоке будет высокая это можно спрогнозировать по времени ответа то тогда стоит сократить время подаваемой нагрузки примеры до 1 минуту ну или 30 секунд кому как удобнее возможные варианты у нас пропорции могут сохраняться по итогам пристрелки а могут не сохраняться если у нас пропорции не сохраняется то это можно отрегулировать путём необходим то есть увеличение конкретные увеличение интенсивности у конкретной группы либо выставлением пауз далее необходимо провести некоторые расчет вот по той формуле которая представлена в идеале мы хотим получить картину которая представлена на складе то есть сначала мы будем наращивать поток и получаем линейную линейный рост производительности до точки и когда у нас будет нормальная производительность потом у нас начинает расти время ответа и узкое место начинает ограничивать наш дальнейший рост далее мы достигаем точки б это максимальная производительность здесь происходит некоторое насыщение то есть дальнейшая производительность у нас перестает расти дальнейшем производительность у нас перестает расти потому что время ответа линейно увеличивается с ростом потоков поэтому потолок можно в принципе определить только после первой стрельбы форму а годится только вода точки а если у вас конечно же есть тестовый стенд на который можно сделать отладку примеру посчитать примерно сколько будет но посмотреть сколько будет потоков то обще здорово но зачастую говорю приходится из результатов пристрелки брать резуль количество потоков потолок и так дальше по поводу подачи потоков они могут подаваться хаотично но где количество потоков зависит от времени ответа динамически изменяется но нам это не подходит есть постоянный но он тоже естественно не подходит потому что нам надо не stability test провести а тест на выяснение максимум есть линейный рост то есть мы линейно увеличиваем поток и смотрим но такой ну у нас тогда сложнее определить нам точку в котором можно отловить насыщением и также есть ступенчатый раз который лучше всего подходит то есть он сочетает себе плавную подачу нагрузки с небольшими полками для отслеживания поведения системы в конкретный момент то есть так принципе легче определять также еще интересной задачей является определение размера ступеньки а именно длина эта высота длина это время а высота это количество потоков чем она длине и тем дольше будем достигать максимума тратить часы на эксперимент как расточительно а высота это в принципе насколько мы потоков увеличиваем тут если дадим много то следовательно мы можем пропустить некоторые моменты лучше подавать небольшими так сказать потоками длину ставим мы в основном она зависит в основном от времени ответа сервиса то есть чем быстрее сервис отвечать то есть ручка отвечает на запрос тем можно делать меньше ступеньку по длине зачастую мы придерживаемся правила это одна секунда на одну ступеньку то есть на один поток если у нас все хорошо то мы получим примерно такую картину то есть сначала у нас был линейный рост потом мы достигли некоторого насыщения и в этот момент уже в принципе можно останавливать тест потому что мы добились нашего максимуму полю наши пределы системы также в конце можно увидеть фон ошибок это говорит о том что уже сервис не справляется и надо уже останавливать поток но бывает и такое когда мы линейно растем и бах разлад к то есть получилось так что сервис просто в один момент перестал отвечать в таких случаях надо более кропотливо проводить настройку потоков исследовать проблемное место его снять чем причина разгадке потому что такое поведение не есть норма это надо устранять итак пришло время полноценно пострелять при обстреле очень важно иметь мониторим всех компонентов системы чтобы понимать как она работает и какое влияние на нее оказывает подаваемая нагрузка поэтому обкладывать всевозможными датчиками триггерами чтобы оперативно отслеживать состояние наблюдать необходимо не только за целевым приложением но и также затем что стоит за ним это может быть база данных сетевые балансировщик и сборщик метрик или другие вообще приложения и даже надо следить за теми приложениями которые обращаются в наше стилевое приложение в какие же метрики стоит смотреть первую очередь ну конечно же это первым делом время и количество вызовов которые мы делаем если у нас растут неуспешный вызов значит что-то идет не так надо с этим разбираться рост входящего просто времени входящих запросов говорит о том что не справляются целевое приложение а рост из ко времени исходящих запроса говорит что не справляется сервис уже за ним также еще стоит отслеживать утилизацию полов соединений допустим вот коллега вчера рассказывал про бдс пока балансиром потом я еще позже расскажу интересный случай на который мы наткнулись также еще про обработку и размеры череде и работу тут поконкретнее потому что был у нас один случай когда при одном исследовании мы на генерировали кучу записи которые легли в очередь а очередь делает то что отправляет уведомление партнером в нашем случае это было заглушка и заглушка не справлялась потоком она просто напросто тормозило и очередь накопилось и время обработки мне было дико и на несколько часов и когда пришли реальное уведомление легли в эту очередь они просто не могли обработать в итоге было кому-то не очень хорошо ну про физические ресурсы я и так думаю что всем понятно так стоит смотреть итак проводим первую стрельбу мы не знаем как поведет себя систему при достижении максимума поэтому стоит быть начеку иногда после первой стрельбы из требуется корректировка потоков как я уже сказал если все ок то фиксируем производительность точки насыщения стреляем по каждому инцес у приложения и так ходи так обстрела получаем некоторые распределение производительности point солнцем на графике можно заметить нехилый разброс особенно три последних сервера которая отличается по производительности более чем в пять раз хоть и анализа мы выяснили что причиной тому это разное железа и у нас на в кластере приобрела да и у нас кластере четыре типа серверов разного поколения и к краске те самые три они являются самым старшим поколением которые тянет всю производительность ну то есть общая производительность вниз итак первые результаты теперь мы знаем сколько жмет каждая надо приложением кто из них выжимает больше а кто меньше а какие сирожа вообще пора бы списать в следующей части исследования мы проверяли масштабируемость тут есть парочка моментов нашем случае так как мы используем g метр а это java качество обстрела напрямую зависит от костра танка с которого мы стреляем то есть если у нас потоков много то там попросту начинает тормозить потому что java вы жира и слишком много памяти и начинает плохо работать поэтому чтобы повысить качество мы запускаем стрельбу с разных танков параллельно где каждый профиль стреляет в определенный инцес приложения для данных манипуляций у нас есть кластер танков где каждый танк это докер-контейнер а все танки у нас одинаково по ресурсам и сервисом внутри итак масштабируемость для себя мы поставили условие запаса по производительности в 10 раз то есть если пользователь к интенсивность порядка 2000 дпс то для обеспечения запаса нам необходимо за счет масштабируемости получить 20 тысяч тогда получается что в нашем примере для обеспечения запаса необходимо всего лишь 4 не cisa приложения не те 15 которую мы использовали и того по данной схемы мы провели исследование для на масштабируемость для каждого приложения и выявили закономерность что для обеспечения запаса нам необходимо как правило от двух до четырёх нот тем самым мы выявили избыточность нашего кластер а в идеале производительность должна у нас расти линейно с увеличением инстансов с увеличением инстансов зачастую это так но не всегда причиной тому пытал ники тут есть разные типы это внешние ограничения и внутренние внешнему ограничением относятся допустим настройки соединения базы данных или там допустим настройки вина connect of на принимающей стороне разные другие ситуации сетевые балансировщика а в случае самого приложение это допустим настройки плов обработчиков или собственные подсказать входящие пулы соединений а также java машина расписание очереди или просто напросто физические ресурсы тут можно на самом деле часами разговаривать о всевозможных бутылках которая существует но вот расскажу парочку интересных историй приданым при одном из исследований мы съели все connect и со стороны базы что привело к очень неприятным последствиям получилось это следствие того что на приложение пул соединений базе данных был заметно выше нежели чем настроена по г балансире и получилось так что мы попросту блоки ли транс своей стрельбу облачили транзакции реальных пользователей база не могла за со всем этим трафиком справиться в итоге не перенастроили приложение и учить учли это на будущее теперь для всех приложений оставим connect пол соединения намного ниже чем может принять базы данных а также теперь мы их мониторим чтобы больше не получать таких ситуаций следующей частью это сетевой балансировщик который у нас утилизирован огромное количество циpкa получилось так что к прокси которые мы используем в роли локального балансировщика был не особо хорошо настроен и приезд стрельбе он выжидал огромное количество циpкa тем самым мешая просто-напросто приложение которое работает с ним плюсом еще создавал нехилый фон ошибок которые было очень сложно отследить ну и последний моя самая любимая это кот когда какой-нибудь разработчик перед новый сервис в котором есть запрос базе данных он ставит не знаю там select звездочка на небольшую табличку а со временем эта табличка начинает пухнуть и там появляется не знаю там 150 миллионов записей и в итоге потом ты делаешь нагрузку и смотрит блин какого ну в общем как и на картинке кстати да типу также как я говорю вот с историей про к прокси когда она мы выявили этого подлеца было очень сложно отловить причина тому что он даже просто первым не отлавливал всем было непонятно то вот живет столько циpкa впоследствии мы узнали и увидели что при изменении числа ядерно контейнерах складка краски те самые растет фон ошибок от локального балансировщика и тем самым мы можем можно сказать нашли его также мы еще подбирали лимиты под контейнеры это было одно из наших исследований мы шли от меньшего большему то есть наращивали количество ядер и смотрели на производительность производительность у нас должно быть не меньше чем на баром эту которая была с циpкa о из оперативка принципе так было все понятно посмотрели на размер сколько приложения жрет наборы монет а также посмотрели на стройке принципе докинули еще сверху и нам хватает а вот с ядрами они у нас дорогие и так что по итогу мы вывели из эксплуатации слабое железо маломощная сервера которые попросту на стянули вниз во вторых из оставшихся железом мы стали перетаскивать приложение в контейнер и освободившиеся мощности мы перераспределили под новые гипервизор и исследование по подбору лимитов дало нам понимание сколько я der необходимо под каждый контейнер а также за счет этого мы уплотнилась по ресурсам мы помимо того что мы нашли того подвеска прокси а также в конце мы определили узкие мидэ местам устранив которые смогли оптимизировать работу приложений поднять общую производительность также хочу поделиться некоторыми ссылочками вы можете ознакомиться с долгом яндекс денег где мы периодически выкладываем наши статьи да и разные другие материала а также добавляйтесь наш чатик где можно задать свой вопрос и поделиться опытом да и просто приятно поболтать всем спасибо ложимся вопросы друзья если есть давать растут одержал стати спасибо за доклад такой вопрос более подробно до ездить более подробно про техники работы с заглушками внешними зависимостями и как бы вот их реалистичность чтобы вас создалось при стрельбах какой-то называете не могли бы рассказать но тут есть у нас разные способы во-первых некоторые заглушки мы пишем сами то есть открываем код идею и пишем то есть реализуем какой-то сервис который отвечает также выставляем некоторые задержки чтобы они примерно ну чтобы сервис у нас отвечал примерно также как допустим там реальные контрагента если более сложные какие-то протоколы ну да тоже приходится реализовывать иногда дамы напрямую стреляем в контрагентах то есть мы согласовываем с ними время проведения стрельб подаваемую нагрузку возможные риски и тому подобное то есть вы заранее предупреждаете внешне ходе конечно могут поразить на какая то инфа накопиться данной но не это естественно естественно они готовы они даже сами за то чтобы мы проводили такие исследования потому что можно сказать для них это бесплатная стресс-тестирование они понимают свои пределы тоже то мы отвечаем за свой сервис они понимают на что они способны то есть тем самым общий профит получаем и они мы спасибо отлично у кого вот пожалуйста до 20 подскажите пожалуйста в начале то показывали графики ближе микрофона ты нам не надо процессор на и время на метод да как как вы это определяли как я и сказал то что мы перемножили количество вызовов на время их обработки тем самым получаем процессорное время дата и время их обрубок время их обработки чем да да да время их обработки всех вызовов и непонятно есть есть а фишки определенный набор каким образом вы определили сколько конкретно я фишка процессорного времени потребляли может быть конечно я некорректно выразился там про процессор на и время но мы к краске смотрели на общее количество вызовов и и на то сколько не обрабатываются и тем самым мы получали время которое тратится то есть нагрузку на систему которую осуществляется от эти вот вызовы и смотрели именно на пропорции это надо было для осуществления пропорций подбора пропорции так сказать и я не понял я положу подойдут на хорошо вот ровно через пять минут ловите здесь вот его на под сцены иван большое спасибо вам за доклад я хотел спросить вот смотрите у вас на продажу только вот так так в тестировании проводите на продакшен у вас есть на бок сервисов которые чисто плодов ски и получается для тестирования вы создаете копию каких-то либо же вот эти данные обрабатывайте которые вас приходят именно с танков как-то по-своему чтобы они не портили вам картину у нас есть сейчас разделю у нас есть кластер танков это наши сервисы которые мы используем именно для обстрела то есть это машин из которых мы генерируем трафик естественно сам трафик который мы генерируем и его обрабатывают прот то есть именно сервера prada а те же самые или вы делаете копию что мы получается atompub выходит трафиком на нем не нагрузка то есть им не мне мне мы генерируем трафик в провод который идет а про ту же его обрабатывает там и никаких копий естественно правда не делаем а я просто подумал там нагрузками на приходит настоящая какие-то транзакции там нет ну у нас используется сценарный язык то есть отжиме трон реализуется nor на язык и тем самым мы генерируем водка краски транзакции до которые как я и говорил либо обособленные методы то есть мы стреляем напрямую в ручку говорю то что с танков мы генерируем именно трафик который уже идет в прод то есть получается что никакой копию я понял нас нету ну понятно спасибо вот слева там есть еще и вам спасибо за доклад хотел бы уточнить вот только вопрос а раз вы говорили что вы обстреливали прот то получается вы какими-то но грубо говоря каким-то фейковыми данными обстреливали т.е. специально подготовленный был пол какие-то да ну грубо говоря платежи наверно да там или что то такое то есть вас получается приложение уже то есть умеет прямо работать с таким тестовые они как-то отфильтровывается отдельно когда получается на самом деле туда очень хороший вопрос у нас есть даже целая статья по этому поводу как мы имитируем пользователи реальных как мы имитируем данные даже как мы можно сказать имитируем деньги ну вот и как мы определим даже те же самые реальные банки то есть да мы подготавливаем какие-то данные которые необходимы для обстрела иногда да они получаются фейковый то есть не знаю сгенерировать какую-то рандомную строчку или там нет то есть как бы у нас есть как и фейковые данные так и некоторые которые мы за данные которые мы заготавливаем заранее для нашего тестирования вопрос больше был в том чтобы как их именно разделить в том плане что то есть понятно можно закидывать там фейковые строчки но просто чтобы это не повлияло на бизнес данную дату я платежи ре ля мистраля пользователя для да это на самом деле то же мы скором времени хотим как раз киноблог выложить такую вот статью и рассказать о том как вообще мы отделяем наши нагрузочные данные и реальные на самом деле тут в принципе все просто у нас есть некоторые метки по которым мы можно сказать отсеиваем наши боевые стрельбы и бизнес просто напросто их но особо то и не видит но эта информация естественно попадает допустим для каких-нибудь ну какую-нибудь выгрузку там один цели еще куда-нибудь чтобы они понимали какой трафик вообще может быть ну то есть как бы там в принципе разделение есть если вкратце от исправим пока понимаю что то есть вы эти данные грубо говоря еще ну то есть как бизнеса и там показывать этой чтобы быть это зазор какой может выдержать систему да естественно бизнес это можно сказать наш главный заказчик то есть мы им говорим сколько можно сказать сколько денег вы можете получить допустим любой естественно распродажи люди когда хотят понимать а выдержим и это трафик или нет а мы говорим да и еще такой вопрос что в одном из графиков было что показано что с какого-то момента начинались именно отказы сервиса это не доела на работу получаться обычных пользователей то есть как бы не более как то там жалоб со стороны негативного круто вот этого детектор сейчас тоже хороший вопрос как я уже сказал мы в основном стреляем ночью а естественно пользовательский трафик ночью намного ниже нежели чем днем тем самым у нас во первых повышается чистота эксперимента во вторых у нас меньше рисков навредить и реальным пользователям конечно иногда бывают такие случаи что как я вот приводил с примерно с очередями или теми же самыми когда мы цифры забиваем но так или иначе этот процент минимален и мы стараемся просчитывать все риски и перед своими стрельбами поэтому да то есть мы стараемся тщательно подходить к нашим стрельбам тем более на проводе понятно спасибо круто спасибо за доклад а вот такой вопрос вы сказали что вы меряете чтобы все укладывалось оставляете какой-то резерв или а вот этот резерв в смысле sbd память и процессор на во времени он какой в числах в числах но как я говорил допустим запас по производительности мы оставляем 10 раз потому что это по процессору и это именно производительность то есть мы смотрим на общего производителем сервис нов дпс то есть получается транзакции процента то есть если у нас говорю что один сервер выдерживал пользовательская нагрузку него две тысячи tps то нам необходимо для запаса иметь двадцать тысяч то есть чтобы когда у нас будет допустим не знаю какая нет распродажа у нас сервис не лег в 10 раз интегрально и производить до а именно уже по детальной настройки какой там запас пополам и так далее это уже более такая сказать детальная настройка индивидуально для каждого приложения ребятам на вашем отряде отлично соседу иван спасибо за доклад вопрос следующий если предположим вы прокидывайте запросы прям на продакшен там сначала маленьким предположим потом большой и вы читаете какую-то зависимость до что если при больших запросов там поменяется предположим функций распределения и там не линейная зависимость экспоненциально еще что то как нам на реальной нагрузки отследить такие моменты сталкивались ли вы с подобным есть уважение же скорее все-таки мощностей чтобы сымитировать ну многопользовательскую как бы нагрузку и поэтому вы только как бы выстроить все линейную зависимость я так понял что если я понял ваш в принципе вопрос как я уже говорил и показывал графика у нас идет сначала линейный рост то есть мы наращиваем потоки тем самым нас наращивается производительность потом когда мы достигаем нормальной производительности у нас узкое место начинает ограничивать дальнейший рост производительности потому что у нас начинает расти время ответа но это в основном и поэтому у нас уже начинается нелинейный роста к краске вот параболический вот потом достигая максимума и там уже все производитель дальше не растет у нас только линейно начинает расти время ответа часть вот как было на график и то что я представлял и в основном мы мониторим да и то по графикам допустим из граф она и так же у нас есть луна-парк который ну или overload который нам показывает динамику изменения производительности времени ответа да и в принципе других метрик можно настроить вот все что хочешь я ответил на вопрос принципе или как ну если что подходите а потом может быть конкретном вещь значит начало отвечает вопрос выстроить что вы восстановите ночью правильно потому что у вас люди пукают ночью и социальной команда которое выходит что это же довольно большие улицы проекте что-то нужно изобрести чтобы не монету или смотрели чтобы ничего не упало так лист да у нас во первых есть вот команда исследователей производительность нас сейчас вот 6 человек в принципе мы делимся в основном по парам либо иногда бывает одни выходим то есть если мы знаем что сценарий уже который мы гоняли несколько раз это у нас регулярные стрельбы то там и один человек справиться и в принципе мы то посменно меняемся иногда по двое выходим иногда по одному вот иногда когда у нас есть целая серия стрелять-то вообще бывает на целую ночь друг друга сменяем через несколько часов было пару раз такое в плане мониторинга ну во-первых да мы сами мониторим некоторые метрики то есть мы настраиваем для себя специальные душ барды в графа не то есть за какими метриками следить за какими систему мне следить также мы оповещаем мониторинг который у нас дежурит 24 на 7 естественно они тоже следят вообще за показательные в целом то есть допустим я не знаю мы проводим нагрузку в день что-нибудь отвалилась естественном говорят все ребята хватит гасите вот так же у нас есть жирные обмен и которые естественно дежурит 24 на 7 нот у них рядом телефон если что мы звоним как вы помните был вчера доклад на вот с метриками и стоит стесняться сразу же званий говорит ребята все там упала поднимает кстати я вот когда представлял тесла это с разгадкой когда у нас линейно растет производительность и и получается потом разлад к в конце таких случаях ну допустим в эту нас было связано с тем что у нас контейнер пау амма упал не хватило память естественно надо сразу же звоните администратору потому что дежурному потому что естественно сами-то мы не понять не можем естественном просыпается поднимает контейнер материт нас общего такие вот дела спасибо и видимо последний и самый красивый но хорошо предпоследнее да поехали добрый день и спасибо за доклад мне интерес интересна та часть когда вы говорили про то что на против приходится тестировать потому что очень сложно тестовый в среду воспроизвести да значит но другой страны понятны все минусы и так далее а вы пробовали нe тестовую в среду или просто было принято решение что ну она очень сложно и не будем делать просто очевидно что ну скажем так большинство ошибок просты и имел ли смысл стрелять пользователей там я огребать всех проблем действительно если сложных прошу ошибок мало вот не менее очевидно да тут есть разделение у нас есть тестовый стенд наш нагрузочным тестовый стенд железной который мощностям естественно во много раз меньше чем на шпрот и у него также отсутствует часть функционала потому что тяжело содержать тестовый стенд и можно сказать 2 прот админом тяжело они плачут также нам тяжело за всем уследить в общем мы в основном там проверяем да какие то мелкие делаем исследования то есть смотрим относительную зависимость между нашим стендам то есть мы получаем какие-то результаты пытаемся воспроизвести какие-то ошибки или попытаемся допустим отследить какую-то деградацию между двумя там релизами а вот такие вот исследования где нам надо именно получить производительность именно про да естественно можно оценить только на проди и тут никакой из тестовый стенд естественно не поможет потому что стреляешь тестовый стенд и получаешь данной тестового стенда а когда стреляешь про ты получаешь данной prada а нам как раз и нужны были данные про да да спасибо мне просто было интересно из какие конкретные кейсы вот в действительности не удается воспроизвести на тестом стенде то есть абстрактным и все понимаем что правда про то это вообще говоря ну скажем так гипотеза а вот понятно что есть какие-то конкретные кейсы что вот воспроизвести вот этот сложный баг который мы поймали это на проводе вот потребовалось бы месяц а вот есть какой-то пример так ну давайте наверно попробую вспомнить подумать какие потому что так сходу мне наверное не скажу лака иван думает что у вас такой голос вам надо книжки озвучивать для инженеров не серьезно не бросайте это дело вам не просто так хорошо это не просто добро пожаловать в мир манга тебе их средства придумал читаете или в кулуарах потом до лучших лорак тогда последний вопрос вот от стажёра из контакте да привет мне так гринов кохлеарный вопрос а почему не давали нагрузку индекс танком вашим же по сути я же говорил про яндекс танк у нас существует целый кластер танков но это было джим интернет да но джан dex тоже имеет нет g метр это генератор нагрузки а яндекс танк это фреймворк поверх генератора на блестки и яндекс там как раз таки нам предоставляет некоторую опишу ну допустим там та же самая отправку метрик вавилот или луна-парк которая нас используется вот это дима не осознал все спасибо друзья кому книжку подарим за лучшего год человек который задал интересный вопрос человека зависит а он здесь уже да он разговаривает а про что был вопрос спрашивал у нас повторить я не помню этого аплодисменты спасибо большое"
}