{
  "video_id": "qjrOnwzY9zo",
  "channel": "HighLoadChannel",
  "title": "Китайские товары: просто, дешево, надежно / Александр Тарасов (Одноклассники)",
  "views": 850,
  "duration": 3062,
  "published": "2020-04-27T12:09:49-07:00",
  "text": "меня зовут тарасов александр я не год с небольшим я занимался как раз таки разработкой сервиса по продаже китайских товаров внутри социальной сети меня можно найти в одноклассниках в твиттере на ките нагнетать на гитхабе и периодически я пишу статьи на хабрахабр их дизайн вот как видно в именах у меня полная consistent ность во всех социальных сетях про ним мы немножко сегодня тоже с вами поговорим и посмотрим насколько на самом деле нужно да и как раз таки и тема доклада на поэтому ему вести вас нам речь о том можно ли вообще на достаточно большой социальной сети сделать зрительно простой надежный сервис и при этом ещё использует достаточно экономичные при этом решении музыки и чем из этого можно например заплатить или пожертвовать всегда перед каждым докладом я вставляю этот слайд в котором говорю о том что за все ваши действия за все ваши решения за все ваши выводы в конце концов ответственность несете только вы сами вот ну давайте начнем и вместо предисловия несколько цифр значит вообще расскажем о чем пойдет речь ну во-первых одноклассники это большая социальная сеть нас посещают примерно 71 миллион человек в месяц пользуются китайскими товарами их покупать заходят покупают их у нас порядка пяти с половиной миллионов человек в месяц и мы делаем примерно 2000 раз в секунду мы принимаем решение относительно того какие товары вам показать вот блин за все примерно так то есть есть стандартная страничка в социальной сети и на нее можно зайти посмотреть это промо акции выбрать товар и посмотреть купить и так далее еще отличительной особенностью является то что у нас есть это будет важно в течение нашего доклада в том что у нас есть портрет в ленте то есть тогда когда вы зашли просто в ленту вы можете увидеть тоже портрет с товарами он здесь справа соответственно изображен и из этого портрета эти же самые товары тоже можно купить то есть это все нативно работает внутри приложений как мобильных такие выпуски а началось все с того что у mail.ru появилась подразделение которое занимается логистикой и вообще платформой продажа товаров договариваться с поставщиками проводится заказы их процессе тому подобная вещь мы будем называть эти как мол платформ соответственно в этой платформы есть ряд фронтов например там вряд может вы знаете там панда ну скажем они тоже одна из витрин скажем так этой платформы и мы по сути дела это тоже витрина и это важно по той поскольку наша задача на самом деле просто напросто показать товары пользователю и замотивировать его на покупку мы тем самым получить свою комиссию за это вот то есть получается обобщая что китайские товары это в принципе сервис внутри сервиса это отдельный раздел с точки зрения пользователя и это отдельный сервер с точки зрения архитектуры некоторой особенностью является то что мы довольно широко используем сторонний pink потому что мы на них завязан на самом деле для социальной сети это скорее исключение чем правило поскольку обычно на тем чем я работал до этого всего дела обычно ты сам все всегда контролируешь ты сам пишешь там данные источники данных сервис их обрабатываешь как тебе нужны и так далее а здесь именно большая еще работа была проделана с тем чтобы вот эта сторонний 5 использует удачно эффективно и вот как раз таки принципы которые ему руководствовались они из названия доклада то что мы захотели сделать решение простым надежными и при этом экономичным на потому что как сидят доход это разница между собственно играть тем что вы зарабатываете тем что вы тратить поэтому где-то ближе к концу докладом и вы сами сможете ответить для себя на вопрос смогли ли мы достичь всех трех показателей сразу вот и первый вообще с чего все это и началось то есть тогда когда представьте себе вы разрабатываете какой-то функциональность к вам приходит говоришь ну вот теперь вот появилась такая штука как товара надо бы их продавать и этом есть какой-то и пиа и с помощью которого это все можно делать и вот здесь первое то что вы начинаете думать о том что общей scythe стороннему платформой пьянь вы уже не master system по хранению этих данных поскольку master system это не вы это те кто их бук процессе и конечно у нас возникает вопрос а как это все дело контролировать как с этим всем работать если попробуют решить задачу тупо в лоб можно ее решить вот так ну вот этот сейчас поджарил у mvp значит давайте сделаем просто клиент будет прям туда ходить за данными как-то все that рисует начать нам нечего больше делать особо не нужна о чем и не стали так делать по одной по нескольким причинам во первых не совсем понятно что делать если платформа недоступна или тормозит и в этом случае мы не можем не только пользователь дать возможность купить товары мы не можем даже показать на самом деле это вообще как бы не очень во-вторых мы сами влияем на платформу когда мы запускали любимый принципе не совсем знали сколько пользы для будет работать с этими как быстро мы сможем больше будет покупать какая будет реальная нагрузка от это все дело во вторых у нас много пользователей и они тоже создают запросы за к этому стороннему и 5 тем самым влияет на него получается такая как бы обратная связь мы зависим от них и мы от них же влиять на ниже влияем собственно и помимо этого еще время ответа что мы полностью и целиком теперь будем зависеть от мощности платформы мы себя в какой-то смысле ограничиваем возможностях поскольку мы теперь заточены на это 35 и мы решили сразу так не делать где-то здесь может проскользнуть такое решение что давайте мы просто поставим некоторый битвой и часть проблем решим ну например поставим героев которые будут проектировать наши запрос соответственно мы там что-то попытаемся значит поставить это рейд лимиты что там попробуем пока жировать какие-то из них данные так далее но все это выглядит примерно вот так то есть это попытка нагибин собственно говоря сделать различного рода костыли велосипеды для того чтобы закрыть на самом деле часть проблем но фундаментально все эти проблемы так или иначе не решаются мы все еще зависим достаточно сильно и мы все еще влияем при этом в этом мы решили пойти другим путем и мы стали думать а вообще надо ли нам проектировать все запрос то есть наверняка есть какие-то разные типы запросов и наверняка какие-то запросы мы можем вообще в принципе не прокси ровать подавать данные сразу от себя то сделать вот так то есть было сто процентов запросов которые мы отправляли в начальном решение давайте делом так что мы бы отправлять только один процент запросов и в итоге мы сделали следующее вы действительно вели такой медиатор такой прокси однако он не просто проектирует запросы а кто на самом деле мы встали хранить данные по товарам для того чтобы их отдавать для того чтобы не ходить в собственно говоря в мол платформ при этом мы продолжаем проектировать часть запросов которые мы не можем проектировать например мы не можем перестать проектировать запрос на покупку товара да потому что обработка покупки товара происходит на стороне внешней системы самим и и провести не можем но таких запросов по факту из общего buncha и достаточно мало поэтому мы таким образом резко сократили вообще количество запросов с видя влияния по крайней мере нашу на платформу как минимум и получив контроль над данными их отображения если посмотреть принципиальную архитектуру то вот здесь можно видеть есть скажем так синие в прямоугольнички серые прямоугольники очищение прямоугольнички это наша части системы а серый преподу прямоугольнички этой части системы которые мы не контролируем здесь получилось зато здесь есть например наш как раз таки gate вы и есть еще scheduler на нем чуть попозже поговорим есть очереди его здесь видно что есть сама платформа есть процессинг платежей которая тоже делаем не мы в данном случае что здесь хорошего в такой схеме какой бонус мы получили на самом деле если например у нас вылетает платежи и мы не можем платить скажем мы не можем какой-то причине мы все равно большую часть функциональности пользователей предоставляем есть можно увидеть товар можно их открыть мы продолжаем их нормально процессить пользователю в ленту вы можете посмотреть свои заказы потому понять что же вы купили понять надо ли например идти на почту и так далее какие то даже нотификации при этом тоже будут работать даже если вылить и вся платформа и перестанет отвечать на любые наши запросы мы все равно в принципе для нас это то же самое что будто бы мы просто не можем оплатить товар какой-либо и в даже в наших частях которые мы сами сделали у нас есть scheduler и очередь которые тоже могут вылететь например работать в этом случае вы даже сможете купить товар то есть полнофункциональный с при этом тоже сохраняется в итоге получается так что реально таких компонентов которые действительно важны и за которым слой слить их немного и более того это не внешне компоненты тасс теперь мы получаем обратно назад к себе контроль в этому если подводя промежуточные итоги можно сказать что мы минимизировали зависимость от внешнего и пьяным теперь мы можем сами контролировать влияние на платформу можем более того получили бонусом возможность реализовывать свои какие-то уникальные фичи при этом обогащает как-то данные доме хранить как-то по-другому и выдавать делают а другую выдачу соответственно и при этом мы еще и повысить отказоустойчивость нашего сервиса однако вместе с этим мы получили ряд вопросов которые теперь нам нужно решить то есть до этого если бы мы этого не делали нам более 5 вопрос решать была бы не нужна а теперь решать их надо ну например а вообще какие данные хранить соли храним данные руки какие данные хранить во вторых как эти данные хранить собственно и вообще как эти данные получить и как их потом синхронизировать такие общие вопросы которые встали перед нами окей давайте поговорим сначала пара хранения данных очевидно что данные на самом деле разные и к разным данным обычно предъявляются разные требования вот на боря данные для отображения витрины чтобы показать список товаров мы их по сути на по большому счету можем потерять даже если мы и потеряли их всех это ведет лишь только к не которому простой сервис она примерно пару часов пока мы эти данные к себе обратно не загрузим вот поэтому к ним соответствующие требования есть а вот на преданный заказом по пользователи мы уже в астане так просто не можем поэтому терять мы их тоже не можем ну есть некоторый набор статистических данных которые наша цель просто взять собрать и отправить в хранилище для того чтобы потом по ним строить лучшие рекомендации ну соответственно такие данные как скажем то что посмотрел какой товар нажали ли вы кнопочку на то чтобы пар показать вот на купили ли этот товар вы или не купили и так далее поэтому мы постарались хранить данные наиболее эффективно скажем так для данных заказов пользователем из-за использовали те решения которые уже в общем в одноклассниках общеприняты например не узко и значит есть статья на хабре можно почитать что-то такое но кратко это кассандра который позволяет вам проводить в том числе например транзак в каком ты виде но завеса зачистки и данным это же кладем в нашу платформу там даже нет какого то есть доклады у ребят там тоже нет какого то есть это про для нас сервис скажем так вот данные для отображения витрины это несколько более интересно поскольку на этой части это большая часть запросов как раз таки идет к ней и соответственно этим пришлось сделать большую часть работы мы взяли себе метод кассандру без транзакции соответственно для того чтобы максимально облегчить хранение данных вообще для того чтобы понять наши потребности можно посмотреть типичную витрину то есть типичная витрина представляет из себя там набор товаров которые вы можете скролить там бесконечны и в какой-то момент если товар вам понравился вам нужно вы кликаете и нам нужно вам отрисовать карточку этого товара поэтому нам нужно хранить все эти данные то есть например название товара его описание варианты цены на него рейтинге отзывы вся эта информация нам доступно и мы ее можем хранить и мы решили хранить ее в кассандре а еще про требует с точки зрения требований как я уже сказал мы можем эти данные потерять но не хотим и то вы на самом деле делать во вторых у нас есть некоторые временные ограничения например нам нужно отдать набор товаров меньше чем за 50 миллисекунд поскольку как я уже говорю у нас из портрет в ленте если мы отдадим эти данные за более длительный промежуток времени нам засчитают тайм-аут и мы эти товары вообще никому не покажем и поскольку мы не можем позволить себе того чтобы лента строилась там слишком долгое количество времени и до 98 всех процентов вообще запросов в сервисе это именно запросы к витрине у нас соответственно мы взяли себе им видят кассандру почему собственно граммы и взяли поскольку это по сути дела кассандра которая крутится внутри джавс кого процесса и это позволяет делает некоторые интересные штуки например такие как скажем не ходить по сети заданный меня получать их прямо с диска прочитать локальным базируется на на нашем собственном for this там с некоторыми до пилами о которых я тоже немножко сегодня коснусь moiseeva трин солнца то есть это минимальный размер кассандры который только может быть кластерам с ритешем фактором равным 3 соответственно мы храним на каждом месяце полную копию данных в идеале как бы эти данные не консистентные одни и те же на всех трех инсов с точки зрения хранения схемы хранения данных на упреки и вылью поскольку основная часть запросов на самом деле к нам это просто запрос типа достань по набору идентификаторов нам пожалуйста товар чеки или их вариант соответственно и нам не нужны обратные индексы то есть просто-напросто не было таких задач когда бы он был бы нужен поэтому взяли наиболее простое и быстрое решение который у нас есть значит сейчас у нас в нашей базе более 100 миллионов записей различного рода и если представить что кассандра толикой наш пирсе снятый кэш товаров то его время жизни равно рыб времени жизни кластера то есть два смерти кластера по какой-либо причине если такая причина произойдет если посмотреть на схему то есть схематично у нас есть трин солнца до как раз таки и они расположены в трех разных дата-центрах это не такие это все развернута в облаке соответственно ресурсам употребляемые так много нам 8 виртуальных циpкa 12 гигабайт оперативной памяти 512 гигабайт жесткого диска чтобы все это дело хранить вот здесь никакой мистики нет а так как у нас реплики чем фактор одним trium запись у нас используются кворум на и для того чтобы данные были консистентные соответственно читать как раз таки мы можем локально и более того читать мы можем с диска прямо соответственно получается достаточно эффективна при этом понятное дело что если нас вылетает с точки зрения надежности если назвали тает 1 но дату мы можем оперировать в штатном режиме чаще всего такое случается при апдейтах в понятное дело вот с тучей выход из строя 2 mouth мы все равно самом деле живем да мы не сможем какое-то время апдейтить записи например вот эти данные товаров изменить изменение их количество какое-нибудь например или то что он больше не доступен для покупки но при этом основная функциональность почтение у нас продолжает работать ну и боль то она не требуют даже сегодня какой никаких походов по сети при этом и самое главное что мы здесь получаем еще это просто менеджмента в такой схеме то есть реально у нас так как на каждом intense находится своя копия данных то мы всегда можем при каких-то выходах из строя мы можем просто скопировать эти данные куда нам нужно и перезапустить incense выглядит даже можем сделать следующим образом что взять и создать новый кластеры сделать это за очень быстро то есть мы берем просто напросто был сара кластера переместили на новую моду соответственно все данные и переключили пользователь на новый класс стр а старые вывели из строя вот такую процедуру мы проделывали как минимум один раз и это в принципе не так уж дорого и больно вот делали мы для того чтобы было проще потрошить ресурсы наших инстансов соответственно просто так было проще чтобы не аптеке там что-то за реплей сеть получается в принципе что у нас простая схема хранение данных у нас простое управление этими данными то есть там нет никаких таких каких-то сложных на воротах и эта вся система работает надежно у нас за все время был на самом деле один достаточно серьезный инцидент за год вот все остальное так больше даже внешне каких-то факторов однако здесь возникает такой вопрос а хватит ли нам вот мы запускаемся так вот хватит ли нам 1 кассандры для всего этого дел для оперированы вот давайте посмотрим значит по факту на самом деле те две тысячи запросов секунду t2000 принять решение которое у нас должны быть они требуют достаточно большое количество записей там может быть до 1000 записей соответственно это все равно походу на диск это чтение это мер значит записи и там пути мутации этих записей и мы все это должны на самом деле уметь делать на одном intense ну потому что два у нас мог быть неоперабельный поэтому надеяться можно только на один стресс на в этом случае наш latency она увеличивается и это в принципе нормально для витрин и вот когда вы заходите в раздел товаров это нормально однако это совсем не нормально для лент и потому что влиять ему уже перестаем скажем мы попадаем уже на тайм-аут и и не отображаем данные пользователя несколько раз мы пользовались этим трюком когда у нас были большие нагрузки и проблемы на кластере мы пользовались тем что просто отключали портрету ленте и в принципе сами товары нормально себе оперировали там даже с большими проблемами на самом кластере которых я расскажу чуть попозже какие здесь есть варианты вообще самый простой вариант как всегда это говорят ну давайте из горизонтальную масштабироваться будем вот и просто добкиным еще железочка ну в нашем случае добавим еще not добавим еще инстансов однако здесь у нас и получается такой трейдов что с одной стороны мы либо должны изменить как-то схему хранения данных тоже вести какой-то sharding как эти данные начать размазывать уже между инстансами и тем самым мы теряем в простоте менеджмента другой стороны мы можем просто увеличить реплики шим фактор то есть оставив ту же самую схему но в этом случае у нас получается так что теперь кворум будет больше но соответственно уже бы записать большее количество мест дождаться большее количество одета ответов от реплик поэтому мы решили так не делать а сделать несколько по-другому пойти в общем-то в вертикальное масштабирование но при этом достаточно умным вертикальное масштабирование просто накинув ресурсов а накинув их правильным образом мы понятно как говорится в нагрузка когда начинается то у вас спасает несколько вещей одна из них кэш мы посмотрели что на самом деле большинство запросов они идут не за всем набором там не зато не до десятью миллионами там 20 million товаров а по факту за некоторым ограниченным sux этом вот и мы назвали эти товары горячим ну почему он ограничен потому что в ленте же вы хотите показывать популярные товары те товары которые продаются в масло не могут быть все товары популярными вот и достаточно ограничены какое-то количество поэтому на самом деле 99 процентов всех запросов они за одними и теми же товар они идут поэтому кажется что это хорошая точка для того чтобы поставить кэш вот и более того мы получаем такой бонус что в этом случае мы можем не увеличить количестве состав и вообще ничего больше не менять не в схеме хранения данных ни в менеджменте нигде ok значит мы получается и в итоге у нас получилось вот такая схема если рассмотреть кассандру как некоторые так кэш в котором хранятся все товары пусть он будет нас адвокат соответственно у нас теперь есть кэш который мы добавили он и memory соответственно и это будет у нас кэш первого уровня в котором поменьше но в котором лежат вот эти самые товары которые довольно часто хотят получить клиенты надеть броню и nemo recache базируется он на of happy эта структура лежит и имплементация в проекте у а не о его можно найти на гитхабе одноклассников это open source ный проект вот поэтому каждый из вас может взять купишь имиджа и взять и по использовать и он позволяет как раз таки хоронить десятки гигабайт информации без ущерба для кучи java то есть понятно что нас есть наш процесс у него свой хит и хранить в этом себе там информацию по товарам мы решили так не делать вот и решили храните вне hippo ну просто много гигабайт но важно даже больше нет а то что мы за использовали еще и механизм shared memory для того чтобы у нас был быстрый старт сервиса без потери данных но соответственно если вдруг там приобрети мы конечно не теряем то есть у нас получается кассандра который живет очень долгое количество времени и есть кэш который живет долгое количество времени то есть меньше но тоже достаточно долго при этом вот при этом его там объем который мы храним данным он составляет примерно 10 процентов от всей базы как то так в среднем ну и время жизни записи там обычно 24 часа это обусловлено тем что как правило 1 сутки идет достаточно глобальный апдейт информации по всем товарам вот и информация начинает становиться более менее не актуальна уже поэтому смысла храме больше двадцати четырех часов товар в кэше особо нет вот если рассмотреть схемку то тут нет никакого в принципе не тоже никому никакой магии у нас есть ja вский процесс внутреннего кольца доу а это да уходит в кэш не получает если данные то она идет на диск с диска их читает соответственно в кэш кладет и при этом есть еще отдельный трек который как раз таки гайд проверяет отель записи и смотрит capacity значит выполняет различную рутинную работу для того чтобы наш кэш оставался горячим однако есть одна проблема с кашами как говорится есть 2 проблемы в разного рода распределенных системах значит одна из них это когда же нам кашу него лидировать дело в том что апдейт может прийти совсем то есть а как у нас кэш он крутится внутри jalas кого процесса то понятное дело что апдейт может прийти совсем не на этот процесс не на этот ин сон сама может прийти на соответственно какой-то другой с нашего сервиса и соответственно тогда в него в кассандру данные попадут безусловно и кассандра ти данные за реплицируют однако в кашу этих данных не будет вот значит что с этим ты в этом случае мы получаем так что нас конечно но ценник ассистентом значит решили это мы все с помощью того что есть в нашем форте кассандры так называемому то ционный listener любое изменение данных кассандре называется мутации соответственно listener делать следующий он слушает все изменения которые происходят в кассандре и в случае если эти изменения касаются данных которые лежат в кэше он этот кэш облетит таким образом у нас получается такая схема с венчальная консистентной стью когда любые данные с кассандры они автоматически точнее изменения данных в кассандры автоматически приходит изменению данных в кэше если соответственно данная запись в каше есть ли здесь еще труп который мы применили в том что в этом случае мы то тель записи не сбрасывай это важный момент поскольку технический апдейт подобного рода информация не говорит о том что товар на самом деле нужен чтобы запрашивать он просто может быть в кэше поэтому случае tt не сбрасывается в отличие от случаев когда вы запрашиваете информацию в кашу окей мы посчитали наш хитрый то он там порядка девяносто девяти процентов составляет и надо больше есть промежутки времени там например 20 30 минут когда hit rate может быть сто процентов мы это обязательно все делала героем для того чтобы в принципе понимать насколько хорошо работает нашем случае возвращаясь к нашей схеме можно увидеть то что мы вертикально по масштабировались их заплатили за быстроту до которая нам была нужна мы заплатили 30 двумя гигами шарит небе то есть оперативная память на каждым sms бесплатно ничего не бывает очки можно подвести промежуточные итоги во-первых мы посмотрели то как у нас общее приложение работает проанализировали запросы и поняли какие технические решения нас спасут лучше чем другие технические решения при этом у нас получилось так что двойное кэширование который мы вели это работает быстро и надежно и мы оставили за собой простоту хранения информации и соответственно простоту и ей управление ею что я считаю тоже достаточно немало важно окей про хранение данных мы в принципе поговорили и теперь у нас остался вопрос про то вообще как эти данные к нам попадают и как на миг синхронизировать окей проговорим про синхронизацию данных вообще у нас есть 3 вида синхронизация данных и каждый из них нам нужен во первых у нас есть большой им так называем большой импорт большой он потому что по сути дела мы получаем весь snapshot состояния внешней системе виде некоторого набора джейсон файлов и при этом мы обновляем там более 16 миллионов запись должны обновить у нас есть апдейты который мы читаем с исков к соответственно в нее платформа тоже шлет до события о том что данные например по товару изменились скажем изменилось вот цена или зимина сего количество и соответственно тоже эти чтение этих событиях и считаем это же тем самым синхронизируем даны и у нас еще есть так называемый real-time апдейты этот момент когда пользователь совершает покупку мы тоже еще раз сверяем данные но уже посредством похода выйти и вот наиболее интересен для нас этой большой импорт данных ну с точки зрения именно доклада по той простой причине что количество апдейта внутри который нужно произвести в рамках него она превышает так примерно раз в 10-15 чем все апдейты которые мы делаем за день с помощью ивентов или real time здесь есть несколько особенностей у этого большого импорта во-первых мы его можем провести ночью то есть лучше его провести ночь чем днем скажем лучше провести во ночью по той простой причине что он практически всегда провоцирует необходимо сделать compaq шин данных причем менеджер compaq шин то такое compact если как вы знаете как не знать как в кассандры встроена запись соответственно то есть мутации и для каждой записи у вас может быть несколько версий по сути дела и когда эти версии может накапливается очень много поэтому для того чтобы понять как бы какая какие данные именно конкретные точнее какие данные актуальны и консистентные нужно взять несколько чтений с диска соответственно компактным позволяет это все он компактен количество чтения диска уменьшить это если очень кратко вот а понятно что компактный не бесплатный во-вторых мы данные перетираем полностью то есть мы пробовали у нас был эксперимент мы пытались как данный читать как-то их там проверять надо ли апдейтить там пытаться значит придумать это эвристики для сокращения количества записей однако это все себя не оправдала поскольку получалось так что в общем записи значительно дешевле чтение поэтому гораздо проще просто перетереть все данные все тем более что как бы мы считаем что они актуальны мы им доверяем непонятно чему еще при этом доверять и соответственно мы всегда данный только добавляем на то есть у нас есть удаление но оно происходит через флаг достаточно стандартная механика и есть при этом у нас постоянно копится какие-то ненужные данные нам не нужно ни свой с точки зрения что за этими товарами скорее всего больше никто не придет потому что скажем они распроданы это привело к нам интересной проблеме что вот вы такие приходите не вам кто то говорит что вот таких пошли графики на самом деле то есть графики танкер с ошибками какими теле графики с увеличение времени таймаутов и на самом деле это график которой вы бы наир они хотели бы видеть ночью здорово если он у вас есть ну что если у нас есть и вы получаете уже информация о том что у вас какие-то проблемы от пользователей значит соответственно что-то не так я если посмотреть на бизнес датой с точки зрения бизнеса это примерно выглядит вот так то есть это график там продаж и видно что здесь где-то есть провал в продажах те деньги которые мы недополучили бизнес графики тоже очень важны для нас поскольку они позволяют понять насколько проблема приоритетно вот и обычно все начинает отталкиваться от того что у нас есть мониторинг и он говорит на брауна сплаве продажа надо бы разобраться в чем дело и вот как-то один раз значит нам приходит этот у нас падают продажи их мало мы посмотрели а проблема в том что портрет не успевает рисовать отрисовывать данные то самом витрины вроде бы работает а портрет не успевает пошли смотреть и разбираю начали разбираться оказалось что у нас не успевает компактный как растут самые они сбивают перелопачивать все записи которые нас есть это все приводит к тому что у нас увеличивается количество со 100 блица них начинает заводиться дескрипторы в java хиппи и вы в какой-то момент получаете out of memory вываливайте сименса в никуда разумеется это не очень клёво потому что то несет в себе от следующем не очень приятные последствия например резко увеличиваться latency соответственно тайм-аута тоже резко возрастают это все видно на графиках вам приходят мониторинга лифтинг и так далее перестают работать snapshot и потому что они тоже у не успевают за snap шутить всю нужную вам информацию incense и начинают стартовая долго то есть мы ловили на том что там при перезапуске nissens он поднимается там скажем 20 30 минут вместо двух трех и еще проблема вся в том что на самом деле сначала увидите последствия потом понимаете конечно уже причину немножко обратно здесь процесс понимания проблем а че мы посмотрели поняли что у нас проблемы с compaq шинами стали тюнить под это все дело во первых мы увеличили потоки для обработки то есть нас было изначально стоял один поток мы сделали 4 это позволило проходить процессу компактно быстрее во вторых вы поняли что эти диски мы не справляется мы поставили себе в этом случае так у нас облака мы просто перед переехали в новый кластер ssd дисками как раз таки и достаточно просто а мы убрали все snapshot и потому что решили что у нас и так все данные хранятся трижды моих еще и можем к тому же восстановить поэтому зачем они нам нужны на этом можно просто сэкономить место и мы размазали данные по шпинделем то есть у нас был один диск соответственно которые хранились все данные мы разбили на 2 диска и соответственно это тоже увеличивает скорость процессинга compaq шины и так далее ну плюс мы еще потеряли некоторые параметры которые у нас есть в том плане что сделали так что не увеличили размер ним таблицы и тем самым немножко пожертвовали оперативной памяти для того чтобы создавать меньше промежуточных файлов на диске то есть из кого я ем сократили там до то есть резко сократили количество походов к диску значит чем мы за это заплатили ну то есть понятно что все они бесплатно опять таки то есть нам пришлось на кинуть немножко циpкa пришлось на кинуть немножко оперативной памяти и поменять тип дисков примерно такой конфигурации мы существуем сейчас вот на текущий момент здесь такой вопрос есть конечно там что вообще у нас ассистент ностью что мы делаем и вообще зачем на мужа консистентной вот если вы как бы говорите о нам нужно консистентных хранилище нам нужно вообще can системность просто так то наверно то не очень правильно потому что в 1 ч надо понимать какой бизнес смысл для нас она несет они сюда на для на следующий бизнес смысл что если наши данные будут не консистентной то пользователь будет получать чаще такие окна ну например что изменилась цена или скажем о том что товар закончился или еще какой-то ships ты не смог купить товар соответственно чем больше таких окон тем ниже конверсия тем хуже пользовательский опыт записи с этим надо как-то бороться и у нас есть некоторые меры по восстановлению консистентной как раз-таки тот самый ночной импорт который позволяет полностью перелопатить почти всю базу и восстановить всю потерянную информацию за день если мы каким-то причинам потеряли апдейты исков кито же служит этой же цели но они приближают нас к не которому состоянии консистентной сью идеальному скажем так состоянии консистентной sti a real-time апдейта нам нужен для того чтобы делать некоторые хитрые трюки скажем когда цена товара изменилось в пределах рубля мы можем так как пользу видится только в рублях без копеек соответственно там мы можем правильно пересчитать правильную цену соответственно и не показать пользователю окно об изменении этой самой цены вот скажем так поэтому на самом деле в нашей ситуации даже им нечего льна консистенция это на самом деле предел мечтаний потому что i do you нас тоже скорее всего практически никогда нет поскольку есть некоторый внешний источник данных и любая информация у нас она всегда к нам приходится некоторой задержкой и важную роль здесь играет лишь только в том как нам минимизировать эту задержку и все пусть мы можем ее сделать меньше но не можем ее исключить более того кроме внешнего источника данных у нас как ни странно есть пользователь об этом тоже частенько забывает поскольку пользователь то видеть цены видит товары вы ему это их отрисовали даже отрисовали карточку а потом он думает нажатием у кнопку купить не нажать ему кнопку купить перебирают нам варианты смотрит цены в этот момент прежде итоге когда он принимает решение уже прошло некоторое время и время то есть лак между собственно говоря тем когда вы данные показали принятием решения когда вы сделали запрос и в этот момент времени тоже все может поменяться поэтому нужно такие ситуации обрабатывать вот те самые окна это как раз таки попытка обрабатывать не консистентной если пытаться как вы бороться с этим всем делом вот мун так реально бороться то во-первых борьба выглядит бессмысленно во вторых без нее получаются проще и дешевле потому что по факту мы опять-таки обложили с графиками и поняли что на самом деле у нас менее одного процента тому зависимости от фазы луны количеству апдейтов по информации по товарам и dom 2 недели и так далее она меняется но в целом это меньше одного процента вот пользователь когда видеть соответствующие окна мы считаем достаточно неплохим результатам не с точки зрения бизнеса это окей последние как бы о чём поговорим это синхронизации заказов ну вот кажется что за казаться тоже должна быть как эта консистентной сети вы зашли посмотрели а ваш заказ какой то не такой то наверное это не очень хорошо однако тут есть несколько видов консистентной sti из одной стороны ткань системность данных ну что например вы купили за 100 рублей на чем и соответственно должны показать потом чтобы купили товар на за 100 рублей а есть консистенции за другой а вот если посмотреть как у нас организована то опять таки понятно что заказы мы должны хранить у себя но при этом эти же самые заказы имеют некоторые отображения вы внешней системе под миша там-то они тоже должен быть заведены между ними есть какое-то соответствием там по идентификатору по которому можно смочить в конечном счете и эти заказы они реагируют на разного рода события например пользователь там отменил заказ или оплата не прошла или поставщик горит у меня этого товара на самом деле больше нет хотят пользователи его купил но заказ всего нужно отменить а потом еще вернуть деньги за этот заказ ну и так далее и кажется что здесь как бы ну напрашивается с первого взгляда что может быть здесь можно это очереди как будто там значит обработку событие и так далее мы на самом деле сделали достаточно просто у нас есть стоит машина есть набор scheduler of которые просто тупо берут и обрабатывают заказы а там есть один небольшой фокус но по сути дела все достаточно просто мы взяли соответственно все наши заказы сделали сплит по ним по доменам это некоторых ешь функция от идентификатора заказа каждый scheduler было брата только свой собственный домен и таких доменов может быть dota в 250 до 256 сейчас но реально в жизни мы используем только 8 метров вполне хватает для того чтобы успевать в процессе работы то следующим образом то есть у нас есть база в ней структурка в ней есть статус заказа соответственно есть домен к по которым можем вычислить этот надо ли этому scheduler убрать этот заказ и есть некоторые поля которую мы назвали индекс активы шантом значит мы смотрим она далее заказ обрабатывать сейчас в терминальном статусе это поле имеет значение null значит что заказ нужно перестать обрабатывать и соответственно каждый shot и лера он берет информацию по своему домену обрабатывает ее ходит сам в мол платформ запрошу туда данные и производит скажем так и данных синхронизацию то есть он соответственно переводит нужен статус коррективы какие-то поля и тому подобные вещи какие здесь есть и вот даже есть у нас график такой скажем что мы себе можем позволить некоторую задержку в обработке заказов то есть кажется так что действие там у нас мы задержим обработку заказов у нас могут графики как-то поплыть можем не получить денег и так далее на самом деле нет все это по той простой причине что ну да например мы там пять минут заказ они переводили из нужного статуса в другой но по факту вы скорее всего это всего не заметить вот потому что ну будет вас через пять минут скажем заказ переведется в другой скат asus как отправлю вольтовой сети статуса например как отправлен там пересек границу то миле от отправлен поставщиком и так далее там нету скажем так необходимости минута в минуту в секунду в секунду переводить эти статусы поэтому можем позволить себе некоторую асинхронную обработку и даже если вы это синхронно обработка нас будут какие-то за тупые на реальных графиках мы увидим что у нас все хорошо и до тех пор пока на наших реальных графиках по бизнесу все хорошо все эти за тупые на синхронно обработки не иметь никакого значения ну кроме того что надо посмотреть почему не происходит где-нибудь фоне разобраться и сделать их поменьше какие здесь есть преимущества и недостатки ну из плюсов у нас за счет разделения по доменам у нас минимизируется конкурентность обработки заказов ну то есть от веса нет борьбы scheduler of за обработку от этого заказы нам достаточно простой транзакцией получается так далее в 3 у нас лет постоянно сверка с мастер системой мы не можем потерять сообщение поскольку мы не теряем сообщение поскольку нас нет сообщений никаких соответственно это позволяет заказы показывать в правильных статусах пользователем и у нас есть независимость от внешней системы в том плане что наша обработка заказов идет параллельно обработки заказа во внешней системе на самом деле из минусов которые есть это ограничено парализация поскольку scheduler сделаны в этот день incense и их нужно менеджере если мы хотим увеличить количество доменов нам нужно увеличивать либо кризиса шендеров либо на каждом шагу лили теперь запускать уже два потока и делать кого-то ручное при распределении этих самых доменов чтобы знать где и кто что должен обработать краткий значит выводы из всего этого мы постарались сохранить решения простым надежным и экономичным и мы сразу же минимизировали влияние внешнего ip я вот это позволило как раз таки и нам наворачивать какие-то свои фичи самим следить за данными соответственно самим к все контролировать во-вторых мы начали достаточно простых и экономичных решений потом их соответственно наращивали его ли исходя из наших потребностей то есть мы повышали надежность уже таким образом чтобы не усложнять начальное решение в этом вопрос о том получилось ли у нас сохранить все три собственные качества как просто надежность и экономичность наверное решать в общем то вам мое мнение по этому поводу что до на этом спасибо и вашей вопрос спасибо отличный доклад у нас небольшой подарок за выступления на первом петербургском хайло де и очень понравилась вот эта часть в самом начале когда мне не докладчика может не совпадать всего точки зрения да ей с другими людьми вопросы перед тем как задавать вопрос пожалуйста представляетесь готов этот wargaming до такой вопрос а ты рассказал именно то что вы делали до с данными а что вы делали со статикой когда вы там картинки выгружали хотя ну это смотря есть с картинками смотрелись ли конечно тут очень упрощенная схема понятное дело потому что если я бы привел бы еще дополнительной части виде рекомендательные системы картина которая собственно грамма у себя проектируем кожуры на это бы не хватило бы и наверное полутора часов или двух часов доклада поэтому конечно какую-то часть пришлось вырезать но если как бы вопрос по статике то средством платформы есть свои url и за картинками которые они предоставляют ну то есть это те картинки который заливает продавцы мы со своей стороны тоже решили их не использовать у себя в решения за запилили собственно гра свой настроили свой прокси то есть то как мы делаем например с картинками из интернета то есть когда пользователь например закачивает картинку из интернета там ее у себя и и сохраняем потом мы там делаем тоже есть двойное каширование всех этих картинок для того чтобы отдавать и быстро там сидена и тому подобные вещи и те картинки которые вот в товарах и видны а это собственно говоря как раз эти картинки которые раздаём уже мы сами с помощью сервиса фотографий но мы по сути дела в своем решении просто за использовали один из сервисов одноклассников которые уже есть то есть там никаких ну рокет сайенс там нет ну да но тоже решили использовать свою систему раздачи статики и контент брэт добрый день меня зовут павел спасибо за доклад я слева от вас и мой вопрос как раз был про рекомендательную систему можно буквально в двух словах там я не знаю как не рассеять теги как он у вас работает уж очень интересно спасибо смотри я не большой спец гек дети и рекомендательных системах хотя я ее трогал в какой-то момент поскольку пришлось дан разрабатывал я у нас есть соответственно своя как раз таки платформу вот обработки данных соответственно которое позволяет выдавать в некоторые набор рекомендации в соответствии с тем как ты настроил но если очень кратко той используя следующие то есть я не зря рассказал про статистику хотя вскользь это было соответственно мы собираем различного рода стату например о том что мы товар показали о том что товар купили о том что вы кликнули на товар о том что вообще вы увидели эту карточку товара потому что на самом деле вы могли и не увидеть поскольку мы ее от рендере или где-то там внизу а вы до нее просто напросто не дошли и закажу такое действие мы начисляем некоторое количество баллов ну если как бы очень просто и соответственно затем товары просто ранжируются по набранным количеством баллов и там есть бандитское замешивании которое как раз таки позволяет привносить некоторую до низа цию выдачу вот наверное если кратко то так спасибо за доклад а вот вопрос про удаление данных ты сказал что выданы не удаляете и можешь пояснить зачем такая избыточность нужно просто была идея в том что ну пока во первых при удалении данных образуются могилки которые потом тоже надо за компакте правильные так далее вот вторых ну у нас есть старые заказы и в этих старых заказах эти данные могут понадобиться а для того чтобы понятия были ли эти заказы все в принципе как то сделано нужно еще систему сверки то что нас есть заказах какие товар вообще принципе покупались для того чтобы эти товары не выпихнуть из основного хранилища данных вот однако у нас был один раз как раз таки мы переезжали с потерей данных и в этом случае мы просто показывает нам заглушки некоторые в заказах пришлось 11 такое сделать в принципе можно придумать скрипт который пройдется по всей базе и собственные горя там эти товары вычистить либо разделить на два кластера то есть оставить старые там с каким-то накопленными данными но не очень быстрый и поставить новые котором данные будут уже новые и в который будут входить запросы именно на получение скажем витрины но это как бы задача для будущего потому что пока мы вполне вписываемся в наши как бы ну в наши текущие потребности то есть класс стр удовлетворяет типом а как вы на самом деле справляетесь с моментом а вот если характеристики товара изменить будто кого-то как его заказали а после того как его заказали но вообще такая ситуация я честно не помню может быть бизнесу быть такая ситуация или нет насколько я знаю не может быть если оно скорее всего будет то заказы должны за реджи птица но по факту это уже как бы не совсем наша как бы мы на это не можем особо повлиять поскольку этим занимается уже платформу логистическая то есть если инф данные по заказу изменить скажем там кардинально как то очень давно жил по заказу данные по товару в заказе как-то кардинально изменились то все равно платформа каким-то образом должна этот товар про процессе или что вернуть вам деньги или я немножко другой хорошего у вас заказ сделан ну после это ваш карточка товара может поменяться завтра после завтра может прийти и как вы с заказом показываете карточку товара если товар уже изменен а такое вообще таких случаев у нас такой случае у нас один раз по моему был мы решили это тем что стали сверять просто то есть там невозможно такой случай что пара-продукт вариант оно поменяется как-то кардинально ну кроме там изменить каких-то небольших правок и внесение изменений поскольку данный товар на платформе будет скорее закрыт и сделан новый вместо старого вот но однако его там были коллизии в идентификаторах вариантов товаров и мы боролись тем что просто-напросто перепроверяли что иди продукта равен именно целевому иди продукта они просто так не просто пойди вариант вот такие да то есть такие коллизии мы обрабатывали а в принципе товар он как бы не может кардинально измениться кроме цены но цену мы не храним товаре заказа мы показываем ту цену которую вы купили тут понятно и еще вопрос вот вы говорили что вы статус не торопитесь не торопитесь на взять статус или газа потому что как бы но многие из них не срочной с теми которые есть у вас те которые как срочные которые вы старайтесь обновлять быстро или нет есть один на статус там есть один статус который мы пытаемся быть более менее срочно но это тоже срочно здесь вопрос как бы нам в пределах часа скажем так это вроде оплата прошла ну да то есть получение call back as сервер о том чтобы если callback не клиентский то есть клетка мы получаем в реал тайме апдейт а если серверный callback тогда мы получим через некоторое время но нам это уже нормальную поскольку мы уже знаем о том что пользователь пытался платите теперь просто через какое-то время там скажем течение часа мы либо увидим что заказ оплачен и и покажем уже правильные уже правильный статус в карточке заказа его мы увидим что заказ затекла не то есть если на наш день спасибо за такое вот мне вопрос там на слайде со способов со способами синхронизации был вариант по событию до обновление это я так понимаю какой то крауля входит постоянно по магазину и какие-то изменения считывает это извини где была на слайде со способом синхронизации попробую допилить тут вроде недалеко было надеюсь давай данных вот иван дай сюда по которой event или real-time нет event-а event это просто кафка это на самом деле просто консьюмер кафки который вычитывает лента который к нам присылает мол ну мол платформа мол платформ присылает ивенты тогда когда например идет покупки товаров даже на других фронтах скажем тогда изменилось скажем количество товара то такой ивент к нам придет ему себя то количество обновим вот для такого больше и эта информация не хватает точно нужен дополнительно полноценный порт во-первых могут быть да это дело в том что эта информация не то чтобы не хватает она во-первых полноценный импорт нужен для того чтобы за инициализировать кластер ему не взять заинтересовать через кафку во вторых но мало ли что там случится где там какое сообщение потеряется вот и не всегда все элементы к нам могут приходить то есть мы не можем положиться на то что мы сможет нам придут все элементы и мы их корректно обработать поэтому есть ситуации например может быть проблема у нас real-time апдейтами что мы обновим в реал тайме обновим неправильно у нас были такие кейсы и потом начну не импорт он как раз таки позволяет защитить подобного рода проблемы а вариант ходить постоянно на протяжении там суток обновлять пачками небольшими вы не рассматривали начну импорта сделать пачками у нас было вас нет ну как бы зачем вот потому что за ночь в принципе ты проходишь полностью и тогда мы на самом деле его и днем как-то запускаю нас был эксперимент в общем тяжело кластеру просто и все при этом спасибо у нас регламент все остальные можно найти будет вне аудиториям вот до какой вопрос бы лучше это всегда достаточно сложно собственно я и хотел бы назвать и вот этому человеку за вопрос про собственный график резаться даже за два вопроса но при затем извините спасибо большое"
}