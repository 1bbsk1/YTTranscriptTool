{
  "video_id": "h1vn44Di6tQ",
  "channel": "HighLoadChannel",
  "title": "Как мы качаем 60 миллионов страниц в день из Веба / Александр Сибиряков (Scrapinghub)",
  "views": 3762,
  "duration": 2994,
  "published": "2019-05-15T02:41:29-07:00",
  "text": "всем привет ещё раз меня зовут сибиряков александр это доклад как мы качаем 60 миллионов документов из баба эволюция архитектуры наши ошибки пару слов о компании где работают скопенко полностью распределенная компания мы занимаемся скри пингом веб сайтов на заказ у нас есть свой прокси сервис есть свой клауд вот я для них разработал собственную качалку промышленных масштабах я сам постоянно живу в чехии вот на россии очень люблю поэтому очень часто делают доклады да я думаю сейчас сейчас сделают погромче значит давайте несколько слов о постановке проблемы это все делается для поискового движка значит пришел заказчик сказал что я делаю поисков движок и мне нужно кончать система наша задача доставить только контент их поисковый движок работает по компаниям с окей и используется для поддержки продаж цель этого движка найти компании а выдача соответственно себя представляет список компаний запросы могут быть примерно следующее там тепловой насос производства кобелей пищевая сода то есть это все около такое около бизнеса значит наш контент уже чтобы именно поддержать поисков полнотекстовый поиск мы доставляем главной странице странице компании продуктовые новостные контактные метаданные мы не доставляем но у них тоже использую для того чтобы обогатить фильтры сделать пользовательский экспириенс лучше значит доход численность сотрудников регион и другие бизнесы есть например такой представьте себе что вы делаете печи для булочек соответственно вас интересует компании торгующей хлебопекарными изделиями скажем дикий печь у вас бюджетные поэтому вас будут интересовать средние компании с доходом не выше значит какого-то лимита и численностью не более ста человек таких компаний будет много соответственно в крутим фильтр например по таким событиям недавно получили инвестиции или были приобретены за такие фильтры у них отвечают так называемые событийные триггеры это алгоритм который срабатывает на навесной страницы этой компании просто выставляется галочка значит в цифрах требования такие 14 миллионов сайтов месяц 100 страниц сайта примерно полтора миллиарда в месяц контента значит страниц если это пересчитать на дни и часы и минуты то выходят какие-то 47 миллионов день два миллиона в 1:35 уве 700 секунду если немножечко подумать чего нет нас хотят на то получается что нам нужно будет 1 литу обнаруживать ссылки не должно быть затыков в разных частях системы мы должны быть вежливыми потому что мы качаем от их имени мы во-первых не должны грубо говоря себя вести как-то грубо иначе они потеряют репутацию до или же все закончится тем что они нас просто забанят и следующий раз мы уже у них ничего скачать не сможем опере обход там и делать бы хотели значит с нашей стороны идет требование минимум железо соответственно измени вам железо требуют следует загрузка канала ну и если вы знаете в европе есть джуди pr да где как бы есть некие предписания о том что некоторые обязательства связанные с хранением личных данных но когда мы качаем web не можем сказать там есть личные данные или нет такая тоже не простая инженерная задача также мы понимаем из переговоров заказчиков что рендере тем пока не нужно то есть у них не так уж и много сайтов от которые сингл пять апликэйшен или java скрипте не был тут этого возможно потом захотят мы ресурсы им тоже не нужны потому что пока что у них там в общем в целом обработка текста давайте подумаем как но подойти к снаряду вообще значит самый начнем с того что просто подумаем какой функционал вообще должен быть как минимум должен быть сетевой уровень разрешение dns установка соединения отправка запроса получения ответа потом нужно форсировать да то есть нужно разбирать html извлекать ссылки должен быть фильтра дубликатов то есть есть некоторое хранилище значит ссылок которые мы уже скачали и мы то что скачали не хотим перекачивать на хотим так много должна быть очередь то есть что то что мы сохранили для дальнейшей прокачке вот эти четыре вещи дальше будем использовать термин я на них сейчас остановлюсь чтобы дальше было понятно вот там мы называем всего робота целом всю систему до есть некоторый конфуз в отрасли что такое вот что такое spider вот и spider am мы называем вот процесс содержащий сетевой слой он только скачивает ну сетевой позирующая spider fit это некий поток запросов на прокачку налог того что вот будет сейчас прокачана spider лог этой результаты прокачки документы ссылки aside это url с которого начинается обход если совсем примитивно то мы можем задачу решать так у нас есть ну положим все в один бетон процесс будет один поток будет очередь в памяти простого счетчики до чтобы организовать лимит на 100 домино стоит страницы домена тоже в памяти фильтр тоже в памяти и здесь нам поможет элементарно скрепим но как бы обход долгий процесс может упасть все придется начинать сначала один процесс это максимум 20 fps а нам надо 700 ссылке или очередь они могут не влезть в память соответственно все это нужно масштабировать и отказывал стоить вам делать давайте подумаем а что если бы у нас могло быть скажем несколько скрепки процессов да и они бы например общались через редис это был бы уже распределенный робот дуб фильтр мы могли бы реализовать средствами редиса множествами очередь могли бы организовать сортированные множествами по ключу все эти примитивы в одессе есть но в итоге что получим single point of her если мы потеряли редис все остановилось проблемы с операциями нарядись и потому что он же он однопоточный грубо говоря он не может например писать на диск и одновременно значит что-то делать с данными до принимать обслуживать запросы да если мы значит его просим например что то писать на диск то он сразу блокируется и все на этом все встает ну и как это все дальше масштабировать не очень понятно значит ты такое решение есть вот она доступна по ссылке для пеленок задача может подойти и вот сейчас мы немножко лучше понимаем чего же мы все-таки хотим да все части робота должны масштабироваться хранилище будь то значит tube фильтр или ссылочная база очередь spider процессы процесс обнаружения планирования также мы хотим чтобы все было отказа устойчивым хранилище было с репликами были минимальные потери данных при падении какой-либо части системы спайдера обнаружение планирования а также чтобы процессы были независимы ну то есть условно говоря если мы остановились спайдеры у нас планирование продолжается или если мы остановили скажем планирование the spider продолжает качать и тут мы понимаем что вот для наших нужд как шина данных apache кафка очень удобно потому что она гибко шраги руется гибко реплицируется позволяет нам удобно разбивать данные по partition там есть такая примитива кастомного портишь энинга которая вставляется в в продюсер но его дальше вот так мы будем обозначать топик кафки на рисунках наша первая версия выглядело вот так мы spiders обнаружения сделали в в одном процессе вот этот процесс писал результаты spider ног ссылки для обхода дальнейшее ли планированием писал spider fit при этом spider fit читался на повторно значит спамерами проблемы в том что спайдеры очень часто простаивали да из-за неэффективного планирования локальный дуб фильтр приводил к тому что было некоторое не консистентной steam и повторно перекочевали контент об этом я чуть дальше расскажу в деталях значит несколько слов 2 неэффективную кладку ссылок в логиков к вот всякий раз когда мы находим начинаем какой-то хвост обходить мы находим какую-то страницу да и вот очень часто получается вот представьте себе главную страницу gazeta.ru мы пришли туда сразу мах 1000 ссылок новых да это как бы такой типовой пример и на деле получается что мы один хост сразу скопом вот так если мы во влог положим лог же он же последовательный да то есть у нас будет хвост лежать вот такими вот пачками да собственно рисунке это и показано до spider он оперирует некий некоторым некоторые пачкой именно в блоге да то есть он идет вот так вот одну пачку вторую третью четвертую и соответственно в его контексте оказываются вот именно очень часто хасты ссылки с 1 х 100 поскольку помним что нам качать нужно вежливо да мы делаем задержку между запросами получается что мы де-факто большую часть времени качаем один хост это вот предполагает загрузку меньше чем один рпс где-то начать 60 запросов в минуту один-два процента цыпа довольно неэффективно на то есть пока мы холста не прокачаем хвост бы не будем качать виду того что так они уложены утра в топике эффективная укладка она могла бы выглядеть вот так остыл нас перемешаны нам при этом мы значит пока ждем паузу для хоста можем был закачать запланировать пост бойцы да и вот это правильно так работают лучшие качалки в мире и значит приводит к максимальному использованию доступных ядер и вот с нашей желанной производительности значит первая версия то патче квк only и мы понимаем что чтобы устроить нам эффективную кладку нам нужно вот в этом месте сделать приоритетную очередь сделаем хранилище добавим еще один топик который назовем скоринг лог там будут лежать ссылки которые нужно запланировать сделаем два маркера которые один будет in giving писать в хранилище 2 будет извлекать из хранилища при этом извлечение будет именно создавать порции с множеством постов на приблизительная производительность где-то 500 запрос на 30 секунд естественно оба worker работу в бесконечном цикле вот что получилось на вот мы видим что от спайдера впадают ссылки вскоре нг лук и скорым клок читается chiar кирам пишется базу или в хранилище значит потом читается оттуда дик юинг маркером пишется в spider fit вроде все хорошо но снова неэффективная укладка почему дело в том что производительность вот этих процессов между spidey раме и хранилищем да и хранилищем вот и spider фидом она существенна разница да мы намного медленнее качаем чем можем записать из хранилища в spider fit соответственно запросы покидают очередь слишком быстро мы не успеваем танцев набрать достаточное разнообразие восток до для того чтобы она у нас была перемешиваем в топике соответственно нужна некоторая согласованность по скорости третья версия согласованная приоритетная очередь значит это вот даже картинка только слегка перерисованы на то есть у нас spiders пишут скоринг лог скоринг лог читай оценки арки рамы с приоритетной очереди идет в дикий worker который пишет spider fit и spider сочетаются с по драме но добавляется слежение за смещениями spider of то есть у нас дикий worker постоянно проверяет смещение spider ах да и сравнивает его с позиции последнего сообщения влоги вычисляется вот такой вот лак по каждой партиции да и как только мы видим что spider подошел ближе чем на тысячу запросов ну словно до концa logo мы генерируем новую пачку а пока этого не произошло мы накапливаем разнообразие в хранилище и вот это как бы дало результат у нас получилось что большая часть очереди лежит хранилище они в топике кафки это нам гарантирует приоритизации разнообразие хостов потому что сами понимаете как только она оказалась в топике мы с ним уже ничего не можем сделать не сдвинуть не вперед не назад а пока она лежит хранилище мы там можно устраивать мы можем добавить например более приоритетный запрос и он соответственно у него будет шанс попасть в spider раньше чем умение приоритетных заброс теперь у нас имеется ответственно наша желанная нагрузка spider потребляют все доступные ему ресурсы окей очередь масштабируется что дальше у нас есть еще проблемы внутри спайдера вот здесь я нарисовал примерно то что происходит внутри spider а значит у нас есть запроса на входе есть качающий слой и есть марширующей есть извлечения ссылок и в конце концов есть планирование дав планируй не прежде чем что то планировать мы должны проверить нет ли там дубликатов да не видели ли мы эту ссылку раньше да и вот напомню мы сделали дуб фильтр в памяти в первых трёх версиях до средстве на он как минимум не переживет перезапуск процесса тут еще есть другая особенность dos обнаружением ссылок вот 123 the instance и spider of на а по оси абсцисс отложены время соответственно а 0 это вот первая ссылка с каста а значит сейчас у нас сид да этот первый этап на втором этапе мы его прокачали прежнему знаем только а0 сделали портирование обнаружения обнаружили что в каста а 0 пришли еще две ссылки а1 и а2 значит эти ссылки проходят нашу систему скорем клок хранилище вот это все и попадают в spiders 2 и 3 spider 2 и 3 происходит их прокачка и поскольку фильтры у нас локальные не синхронизируемые the spiders 2 и 3 ссылка а 0 будет считаться новый да и в итоге у нас все перемешается и мы мире качаем все известные сайты нам ровно столько и количеством раз сколько у нас spider понимаете как бы ну чтобы оценили масштаб проблемы весьма неэффективно да то есть у нас падает процессах несколько раз ты между процессами перемешиваются часть дубликатов все равно проходит фильтр нам нужно это сделать надежно как первое что на ум приходит а давайте бахнем сейчас централизованное хранилище но мы же помним что у нас во первых как бы есть spiders да они подключены к кафке сейчас еще они под куда-то ходить что-то там проверять увеличивается связанность да не хочется потом спайдеры должны качать у них и так там значит есть некоторые объем сетевого взаимодействия при этом мы будем еще будем всю эту прокачку тормозить на синхронных вызовах как бы не хочется соответственно мы приняли решение все убрать в отдельный компонент назвали его worker дубликатов этот маркер читает spider лак значит и пищит скоринг log поэтому и хранить состоянии ссылок во внешнем хранилище то есть как бы развязали эти два момента тут еще нужно понимать под аспекты вежливости да значит чтобы быть вежливым нужно делать задержку по отношению к стал чтобы делать задержку нужно помнить когда и какой запрос когда мы послали запрос к конкретному хасту на если ну и как-то понимать когда мы можем снова послать к нему запрос когда у нас много качающих процессов и много хостов тут возникает идея например да чтобы если бы было если мы качали один хост нескольких процессов получается снова опять же нужно где-то централизовано хранить состояние или же в состоянии синхронизировать между качающимися c сами и все это выглядит с точки зрения архитектуры супер сложно потому что опять же задержки на синхронизацию черт знает как все это отлаживать можно нормально так конкретно влипнуть если оставишь круто действительно не очевидную ошибку в итоге понимаем что лучше бы не надо все это делать решение в том чтобы на самом деле качать каждый хост не более чем с одного процесса ввиду того что мы хотим быть вежливыми естественно у нас вряд ли будет такой хост который превысит наши 20 fps на процесс до соответственно если мы качаем скажем каждый хост с одного spider а то у нас и состоянии шарить по нему никуда не надо на очень удобно вот соответственно чтобы это осуществить нужно назначить cast in a spider процессы жестко вот здесь a b c назначено на 1 spider drf на 2 g.na и на 3 и соответственно там вот происходит прокачка разных ссылок проблема в этого решение в том что мы не сможем нормально масштабируем митрий балансировки да но все остальное реально сильно сложнее потому вот этот white угол тут вот нужно пару слов сказать распределение хостов между spider am его рокерами дубликатов дело в том что у worker а дубликатов намного меньше работы чему спайдеры вычислительный да и мы с точки зрения экономии ресурсов просто-напросто решили что дубликат ных фокеров нужно намного меньше да естественно каждый worker дубликатов обслуживают намного больше хостов чем spider на изначально мы конечно думали почему бы не сделать один к одному но и целесообразно с точки зрения ресурсов мы всю эту систему сделали с точки зрения ну вот хотим расширить функционал до маркера и вот всю эту систему мы делали с прицелом на open source вообще скрепим краб он возник на форме на фреймворке скрипи да и у нас как будет культура open source она приветствуется внутри компании естественно мне сразу представлял себе что эта система будет использоваться разными людьми разными компаниями в разных условиях и они будут хотите разным образом кастомизировать да по этой причине как бы понятие и вообще смысл дуб баркера я решил расширить до сделать из него некий компонент который будет не только проверять дубликата иметь возможность планировать что угодно и когда угодно соответственно появились состояние у каждой ссылки новая ссылка обойдена и ссылка запланированная и завершившийся с ошибкой появилась такая вещь как по доменные метаданные то есть там прямо можно сразу счётчик хранить сколько с домена ты уже запланировал или скачал ну естественно вот это вот фича до планировать когда угодно и что угодно и появился вот такой вот грубо говоря компонент стратегии обхода да и вот мы наверное единственный робот куда который сейчас доступен в open source и который позволяет взять остановить стратегию подправить и и заменить и снова запустить при этом весь обход не начиная с самого начала на то есть прямо на лету значит финальная версия архитектуры выглядит он примерно вот так у нас есть спайдеры они пишут spider лак spider лог читают стратегий маркерами стр а также worker и проверяют ссылки в ссылочном хранилище пишут скоринг лак и вскоре нга logo ссылки на планирование идут ваньке варки рынке уркер их добавляясь приоритетную очередь значит из очереди они читаются дикие worker of пишутся в spider fit при этом дикий worker исследовать смещение spider of вот несколько слов о том как мы выбрали хранилище да значит понимаем да все все процессы у нас однопоточный общих данных нет соответственно требования к и сид у нас довольно низкие в то же время у нас много данных на мы сразу рассчитывали что будем качать объем и сравним ее с объемными с объемами яндекса и гугла вот и снова нас миллиарде ссылок миллионы your love терабайт на и очередь и мы понимаем что нужно эту штуку конечно сортировать значит xs паттерны у нас пример такие то есть хочется взять get по ключу и хочется иметь возможность сделать git и пупочки значит что касается очереди в очереди шпиц малый там все за оптимизирована там именно пачка до при этом так получается что качаем и сильно медленнее чем можем что-то сделать данными и данных то немного вот состояниями ссылок другой вопрос ссылок очень много orbeez высокий вот но имеется большое пересечении достоинстве на большой простор для каша и мы хотим это хранилище вот именно гибкую настройку каша тут большое пресечение связано с тем что когда мы качаем ссылки с 1 х 100 мы очень часто натыкаемся на одни и те же ссылки потому что про линков к внутри сайта она как бы вот с точки зрения юзабилити всем очень удобно на то есть вы мастера часто это делают ну и нужно понимать что в один момент времени мы работаем с небольшой частью данных помните вот эти 14 миллионов сайтов dos которых все начиналось мы все разом не будем качать на то есть мы будем скажем работать с 1 2 двумя тысячами постепенно вот таким вот смещающиеся окном проходить с 14 миллионов хотим чтобы хорошо масштабировались я то есть наш объем данных он растет по мере обхода над начинаем с нуля закончим не знаю это байтом вот и хочется чтобы при этом хранилище как бы росло до занималась все больше объем и на дисках на машина да и при этом как бы не было каких-то затыков остановок down time of всего остальную также понимаю что стоимость потери данных велика если мы представьте все месяц скачали потом у нас все накрылось начинать сначала очень не хочется естественно хотелось бы чтобы все реплицировать вот постепенно приходим к тому что вот ведь bass это было хорошим здравым решением для того чтобы сэкономить усилия на диплом использовали колдера менеджер перейдем тестированию в мою первым делом как только все началось мы начать задавать или dns amazon а значит начали обхода соответственно мы посылаем очень много запросов им потому что все эти состоянию нас dns по ним не закодирован да и не нам просто начинают пасовать тайм-аута на все запросы да наш обход ты факт останавливается значит вылечил увеличили тем что установили свой рекурсивный dns с кэшем значит это может быть бинт или on bound on bound лучше потому что позволяет манипуляции с кашей вы можете прямо ему сказать с дампе весь кэш а потом при его перезапускать загрузились кэш даст с нами все свои 14 миллионов можете положить и больше как бы ты нас не разрешать значит следующая проблема это замедление очереди через два-три часа после откачки прокачки после начала обхода значит выглядит так очередь работает соответственно у нас часто добавляются строки в таблицу и удаляются до при этом как бы там образуются так называемые мертвые места до мертвые строки в таблице соответственно скан когда делает запрос типа возьмите nudie подай мне все что есть таблицы да он вынужден эти мертвые строки пропускать так вот примерно через два-три часа накапливается критический объем этих мертвых строк да естественно мы не можем наши запросы да на получение данных из таблиц в очереди начинают занимать в секунды там да там может на минуту доходить вот естественно понимаем что нужно чаще регенерировать таблицу да потому что эти весь к сожалению и такой умный как вот современные базы данных да и он не умеет переиспользовать вот это место которое было удалено да у него это называется мэйджор комп action мы выставили в 30 минут и это в общем помогло следующая проблема это вот высокий и abs на регион серверах связано это с проверкой состоянии ссылок открыто джокера мы используем такое хранилище как одна ссылка это одна строчка в таблице вич bass вот проверяем их пачками естественно представьте себе качаем клиентуру значит первой странице нам сразу пришла тысячи ссылок и такие расправляем тысячи ссылок вич боясь на проверку эти bass сразу же по всему кластеру делает трендом секи и у нас время ответа там не знаю секунды соответственно медленно и решили проблему тем что просто-напросто ну как данные быть вы с организованы в блоках до при этом все ссылки хранятся сортированы по ключам мы просто-напросто дизайн ключа поменяли так чтобы ссылки с одного хоста они лежали рядом друг другу нас при этом размер блока настроили так чтобы он примерно вмещал средний хост и кэширование выть быть она работает на уровне в блоках до соответственно подкрутили вот эта вот вещь ведь без для того чтобы у нас техас ты с которой мы работаем сейчас они все лежали в памяти он не ходил за ними на диск стало лучше вот но все равно очень высокие циpкa доходило до 200 тысяч рпс ко всему кластеру значит в итоге закончили тем что сделали локальность скажешь ссылок в памяти у нас там на стороне стр и тот же маркера мы же помним что у нас там значит хасты не пересекаются согласны дизайну соответственно мы можем в каждом процессе тот же worker зафиксировать ссылки с которыми сейчас работаем и просто напросто кэш ходит выжми из только в том случае если ссылки не только ше организован по принципу ле street and laces помещает себя три миллиона нет заработала финальным аккордом был бы китайский гемблинг значит с этим мы столкнулись когда начали обходить дот ком значит на деле получается так что китайский game link китая запрещен вот они соответственно с этим из своих зону шли в дот ком вот этот кроме они массово скупают домены которые сгенерированы автоматически вот видите там вверху дпм джей джей джей dji.com то есть как бы это все генерируется автоматом вот на этих сайтах они генерируют сайты при этом эти сайты ссылаются на другие автоматически сгенерированные домены используют под домены 2 3 уровня и выше все это жестко про ленкова на короче наш 3 3g worker до при проверке вот этих ссылок при проверке по хостовых лимитов просто потрясал по швам и де-факто ссылок было очень много все лимиты наши новостными поломались выглядел это так что мы заходим на этот сайт чуть-чуть работаем встречаем домен второго третьего уровня у него соответственно новый лимит на хвост и мы короче бесконечно застреваем на этих сайтах и качаем и качаем и качаем их на то есть на каждый новый has no и он основы лимит в итоге проверка состояния снова перегружена долго тогда разбирались искали проблемы выйти без проблем это на самом деле вот вот в этих вот товарищах с китая значит решение было следующим везде для проверки используем домен второго уровня значит чтобы это сделать используя библиотеку public суффикс она содержит в себе правило разрешения доменных имен но если у вас в зоне 2 токен надо соответственно разрешать нужно в зависимости от того что это за токены случае доткой окей это считается как один остатком как бы разрешается вот так поэтому не можете просто использовать количество точек вы меня да ну и не ходим выйти bass если превышен лимит по доменам от к сожалению ходили вот и естественно тоже его грузили этим вот public суффикс сделать mozilla foundation так что он доступен берите пользуйтесь самый большой наш дипломант в суммарно это 250 лидер значит в низостью работают процессы стратегии worker a spider of всего остального а вот hbs он выглядел вот так там 7 машину по 12 ядер вот новиков кто же там вот здесь дисках на машине система работает продакшна уже полтора года мы ее падали в бою и я вам скажу честно что вместе с мы тратим не знаю может быть суммарно часов 8 на ее обслуживание то есть те что неплохо основные находки из доклада дефективные прокачки нужно значит хорошо перемешивать ссылки и дозировать их от разных состав не допускать обработку 1х стас нескольких машин дубликаты если вы это делаете будут дубликаты и просто напросто биспинг бесконтрольно рпс к серверу да сразу продумывать назначение хостов на этапе планирование системы да то есть как именно группы хостов у вас будет распределяться по всем компонентам вашего робота при проверки ссылок очень полезен и эффективен кэш страниц с одного сайта имеют похожие ссылки как ключ удобно использовать домен второго уровня использовать public суффикс то с очередью постоянно перегенерировать или же использовать такую базу которая эффективно перри использует эти мертвые строки значит на питание концы можно построить эффективного робота для обхода да вот именно масштаба веба масштаба яндекса и гугла и вот он доступен по ссылке спасибо пожалуйста вопрос дорасти я хотел спросить вот у вас были проблемы с производительностью быть бисовы на кассандру смотрели они чем-то похожи и words сценарии использования чем-то похож так скажем у нас дома постоянно использовался x-bass поэтому кассандру глубоко не смотрели но со стороны понимали что там уже как минимум будет overhead на sql да и он нам данном случае совершенно не нужен утро стену есть пример совсем другой архитектуры на порядки больше нагрузки а вот вопрос у вас вот то что касается самого робота сколько там машин вот занимается нагрузкой 600 страниц секунду у нас один качающий процесс выдает этот 20 fps мы понимаем что питона и он не стоит много хотите употребляет одно ядро соответственно мы можем получить 20 рб с ядра поэтому одна машина нам может выдать условно говоря где-то вина значит 200 240р ps вот так но поподробнее про нашу архитектуру если в 12 часов метод будет комната 13 а еще вопрос а вы когда караулить ой то есть анализируйте и результаты вы что эмиль парсить или у вас какие-то регулярке или как вы там именно r7 и конечно мы должны ссылки обнаружить земли штатными средствами и политиками alex m l он в общем то же вы - да спасибо у вас там такую юзается видимо очень должно конечно спасибо все это нагружена здрасте у меня вопрос вот вы говорили что у вас к spider am excel и живот кирам жестко привязаны конкретных а стыда вопрос а что происходит когда умирает и даже уокер не знаю ну просто сервис сломался что делать то есть это как раз воешь родирование перезапускается вся система с новым шарнира вы им или как они не они не как быстро этот же worker он сбрасывает свое состояние в space да то есть ссылки все эти ссылочные состояния они у нее хотя за кашированные данных постоянно сбрасывает соответственно по домены и счетчики точно также большая часть данных который идет ему на вход она хранится в кафе китов кафки до соответственно если ты упал то просто напросто ты какую-то часть данных там небольшую потеряешь да но для обхода в целом скорее всего это будет незначительно а то есть в это вариант допустим данных остыла просто недоступны для обхода не то чтобы они не планируются да то есть если они лежат в очереди и там большой объем то есть на деле скорее всего это будет там часы да за это время успел обнаружить проблему и пофиксите то есть как бы в нем не паули смотреть в сторону не знаю что-то типа консистентные хэширования когда если вылетает сто и одна машина до больше часть данных не меняется это план и равномерно более менее рассасывается по соседним короче это это будет супер круто для все это серфить iqtour и нам получается придется или балансировать все эти стратегии worker и на лету сильно усложнит ну там примерно типа один запрос пересчитать нам точки на окружности и все ну вот представьте себе у вас есть хвост у него там не знаю 100 тысяч ссылок и внезапно эти 100 тысяч ссылок и у вас не один такой хост вас их миллионы и этот холст внезапно переезжает куда-то но он он он рассасывается более-менее равномерно по остальным внук spider am или старшего ну короче вот мы так подумали подумали пока что это то есть пока вы это такая по воле места и твой пост а ждать ну да то есть как бы можно так здесь запилить конечно будет все круто и красиво но вот экономически не имеет смысла сейчас понят пасибо здравствуйте спасибо за доклад у меня вопрос больше не про архитектура про метрики какие метрики вы снимаете с этих всех процессов хороший хороший вопрос тут вот смотрите нам платят за доставленный объем соответственно мы должны качать день и ночь вот если мы хреново качаем было не получим вот и главная метрика это короче сколько мы сейчас доставляем до как только мы доставляем 0 сразу включается alarm что что делать да вот соответственно очень важно смотреть сколько сейчас пропускная способность на spider ах да вот сколько качается именно сейчас на спайдера очень важно смотреть к пропускная способность дальше по системе то есть смотрим сколько запросов скаченного проходит strata g worker ах сколько проходит значит вот wing hing дикий квартирах примерно оно должно быть на 1 уровне да но к концу обхода так так получается что strategy маркеры почти что простаивают потому что все что нужно уже лежит в очереди вот в то же время на начало обхода там как правило очень интенсивный рост до этого пиковая производительность начала обхода а к концу обхода мы будем ходить уже по сути самые медленные хасты значит проверяем кэш состояние в стрые тот же worker и да какой там hit rate над тот скорее такой хавчик что она вообще живое там также смотрим какое то время ответа вич bass да папа разным типам запрос да если она там значит начинает сваливаться в секунду на отчет делать вот такие вещи спасибо скажите пожалуйста есть ли у вас какая-то динамическая подборка времени задержек между запросами потому что некоторые сайты из практики знаю кто то позволяет быстро и запрашивать кто-то не любит быстрые запросы и у вас какой то старались у вас какое-то стандартное время задержки между запросами к одному хасту или есть какие-то способы динамической подборки и но в продолжении вот этого вопроса некоторые сайты не любят когда их качают и соответственно проще чтобы использовали пулу прокси-серверов у вас это как-то используется прокси не используем наоборот пытаемся быть максимально открытыми у нас есть значит use рейтинг юзеры gentoo нас прямо вот теперь уже есть ссылка на наш бот где мы пишем зачем мы все это делаем значит что касается задержки она у нас адаптивная как вы сказали если нам отвечают медленно там и соответственно чаще выходим пореже при этом как бы она у нас с адаптивным определенных пределах потому что ну мы понимаем что в этой адаптивности есть некоторая погрешность она например связано с работой сети да то есть и не стоит ей верить до конца то есть в идеале бы конечно хорошо бы вот в ответе во всем light in se видеть например только время работы сервера до но к сожалению там еще и сеть может очень сильно сбоить да и по этой причине мы там сделали скажем максимальную задержку три секунды спасибо можно вопрос здесь скажите у вас поисковая система изначально проектировалась правильно ну для заказчиков на на его стороне и когда он выводит информацию после поискового запроса то у вас сниппеты выводится или только ссылки если выводится то как вы выбираете блок для сниппет а это вопрос номер один второй вопрос как определить и кодировку в таком случае ну короче это делается там на стороне заказчика я я могу ответить как бы я это сделал да но я не пассивным только ссылки отдаете и к этому мы даем контент но как он закодируем байтами просто да да дальше он снова сниппет выбирать но сами понимаете что для того чтобы нормально определить кодировку нужные заголовки конечно в заголовке берете берем отправляем а вот если допустим вам на посылке доступен не hdmi или допустим pdf-файл вы проверяете там вот эти все остальные вещи которые нам из зависеть ваш робот мы смотрим мими как бы от нас на самом деле не волнует то есть если нам не шел pdf там и из него не пытаемся извлекать ссылки отправляем как есть и все буфер короче байта да это блок значит уже может понижать эффективность системы в целом что допустим вы попали на сайт который хранилища pdf файлов и они все огромные вы будете просто их очень долго качать не рассечек каких-то каким-то признакам есть количество вот на этот запрос у нас максимальный максимальный размер испанца он ограничен по этой охраняла сложить вот и все больше ничего другого нет спасибо и вот еще вопрос и добрый день спасибо за доклад вопрос такой вот вы обходите все эти ссылки все это замечательно хорошо а как вы принимаете решение когда повторно пройти когда собственно говоря ну на сайте могло измениться какое-то содержимое в общем-то то есть как вы принимаете вот где вы храните эту информацию как вы ее получаете ну короче или вы не обходите вообще мне не переходим обязательно как я говорил уже что нам платят за доставленный объем да и когда в первый раз мы с этим столкнулись и такое горе ребята как бы вот смотрите мы сейчас все скачали и надо как бы по идее что то что то еще качать потому что у нас железу наше все это занимает деньги ты то ли они горят и ксюшей ничуть не изменилась еще я говорю о делать в итоге короче мы как-то как цикл закончили сразу начинают снова и все то есть цена с цикл примерно две недели у вас ограниченный какой то пол ur love научите раз миллионов сайтов от которые были на входе вот мы по ним перри обходим спасибо хотелось бы имеет там 100 миллионов до тогда был как бы мы были бы более адекватными об этом в плане перри обходов ну теперь чуваки получают там super eight двухнедельную периодичность вот сша есть и вот добрый день я тут можно вопрос тут оттуда ok мне интересно почему вы выбрали именно питон в этом деле потому что есть гол который намного эффективнее был есть и который еще более эффективный а у вас такой выбор питоном производительность вами о как бы оси согласен абсолютно с вами вообще как бы по хорошим бы наверное не джанибек написать вот но как бы так получилось что скрепим капан традиционно бетонная компания да и мы как бы своего рода не располагаем ресурсами пастельным про языком до естественно вот в этой причине в основном понятно пасибо вот еще есть вопросы спасибо за доклад мне очень наивный вопрос я после когда не сталкивался что такое robots.txt это первый вопрос и второй вопрос каким образом именно вы распределяете spider am и хосты то есть если это все в кафки то получается что spider должен брать только определенные сообщения но это вряд ли на что то есть сейчас скажу ругаться текст содержит правила для обхода конкретного сайта каждый сайт размещают у себя robots.txt и там есть соответственно некоторые указания какие url и обходить какие не обходить и какому роботу это очень важно на самом деле потому что мы себя обозначали да и у вебмастера есть возможность не через , и босс нас банить а просто-напросто включить запись robots.txt да и мы уже не будем обходить соответственно что касается распределения хостов мы используем ключ кафки да и используем при при чтении мы назначаем одну партицию кафки на один spider соответственно мы точно уверены что вот именно в эту партицию при продюсер положит только хасты которые вот в этом бине от сша и все так вот спасибо можно спасибо спасибо за доклад я хотел уточнить а как вы работаете с конечными вот ваших spider ах вы закрываете открываете одежде постоянными или если они рвутся как вы подключаетесь можете рассказать поподробнее да у нас значит скрипи переподключается ну прямо скажем качалка используются скрипи доу скрипи построен на три стадии да то есть это получается что синхронный фреймворк работающий в один thread да использующий и пол select вызовы во всем вовсю глубь как бы ширину значит при этом как именно мы себя ведем с connection ами если нам оборвали connection при прокачке то скорее всего мы уже не вернёмся а если мы значит понимаем что это какая-то сетевая ошибка да тогда мы будем перри повторять там несколько раз там до пяти раз вот и но есть фатальные ошибки да например это разрешение dns тогда мы соответственно больше не будем пытаться туда туда ходить вот поэтому походе и конечно открыт на все время жизни worker атак на самом деле нет там есть некие некий пул да и этот пул такого размера что он несколько превышает количество хвостов которые мы качаем в параллель да там тоже мы примерно так проектируем очередь что она будет качать 100 х 100 вт где-то в параллель да естественно мы держим где-то 300 со едет соединение может быть тысячу да и постепенно те которые не используются отпадают спасибо чего нос со временем кстати я помню свои тайминги просто у меня было сорок минут и способо хороший доклад я понимаю почему вы избрали uses а ночью уай are you using проявите сын таких героинь это от проема тифе черты до cuz you are using перу или me that it's mostly about a fling дифферент холст и агент belkin continue on my feet зелья а какая это сделано для перемешивания хостов дело в том что spider работает эффективно только тогда когда мы можем качать одновременно с нескольких постов а кстати прокси не используем это говорил всего вот тут еще есть рука спасибо очень интересно вы вначале сказали что не использовать и ringing из по приложения то есть проход air помимо вас а в перспективе планируйте в эту сторону смотреть ее как бы на что смой конечно конечно на самом деле думаю щас все кто качают веб должны в этом направлении смотреть на что смотрим на что смотрим на то чтобы раскачать какой-нибудь chromium или веб kit вот у нас есть своя разработка вот но к сожалению splash называется в open source и но она значит массируется на неким фёрт парте форк рыбки то вот мы сейчас вот пытаемся думать в сторону того чтобы как-то ее значит улучшить и сделать более эффективно с точки зрения ресурсов потому что реально один рпс это как-то с одного ядра это как-то не современно phantom ps то есть такие вещи мы работаем быстрее фантома спасибо"
}