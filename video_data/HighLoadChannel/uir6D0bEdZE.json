{
  "video_id": "uir6D0bEdZE",
  "channel": "HighLoadChannel",
  "title": "Возможности Spark Streaming для аналитики данных в потоковом режиме / Артем Гогин",
  "views": 2767,
  "duration": 2997,
  "published": "2022-03-21T12:55:00-07:00",
  "text": "всем привет меня зовут годен артем я дат инженер x и сегодня я расскажу о презентации на экране сейчас должна появиться и сегодня я расскажу про возможности spark streaming для аналитики данных в потоковом режиме поскольку я дат инженер то буду касаться довольно дат инженерных задач и поскольку that инженеры в основном работают в хранилищах данных или по-крайней мере где-то рядом с хранилищами данных то и мои примеры будут в общем-то относится больше к хранилищем данных хотя это эта архитектура и это описание может также быть применено не только в хранилище данных давайте для начала посмотрим на название возможности spark streaming для аналитики данных по таком режиме и здесь я хочу выглядеть три части первое что мы разбираем аналитику данных что я признаю под аналитикой данных это то что мы должны откуда-то взять наши данные уже сгнили он ее до этого загрузить их и обработать сделать какое-то решение на и тип данных то есть мы должны записать результат обработки данных или мы должны передать когда-то эти данные или по-крайней мере просто сохранить далее здесь говорится что мы будем это разбирать потоком режиме что такое что имею виду под потоковым режимом поток в режим можно перепутать с режимом реальном времени и в принципе это близкие понятия и я хочу сразу провести черту что такое режим реального времени что такое потока в режим в принципе одно и то же приложение может на время работать и в потоковом режиме и в режиме реального времени однако поток вырежем говорит именно про то что к нам данные постоянно поступают и мы их постоянно обрабатываем не обязательно в реальном времени то есть поток вырежем может быть с задержкой минута десять минут час или даже четыре часа и это все равно потоковый режим главное что наше предложение постоянно включен а именно про это сегодня мы и поговорим и последняя часть названия это spark streaming а значит что мы посмотрим архитектуру именно на примере spark стриминга и в конце разберем возможности spark стриминга конкретно что он умеет делать какой у него функциональность и где функциональность ограничено для каких задач он подходит для каких нет идеи которые я надеюсь будут полезны по итогам этого доклада это нужен ли вообще streaming для каждой конкретной задачи и чего это будет стоить а именно какая необходима архитектура дополнительная для этой задачи для решения конкретных бизнес-задачи со стримингом необходимо ли довольной инфраструктура нужно ли имплементировать все очереди какие-то но новый стиль база данных и так далее или мы можем это сделать на имеющуюся архитектуре соответственно как определиться именно с это архитектура и стремянка какие зубы до выбрать и вообще понадобится дополнительно инфраструктура для задачи или нет далее из идеи это подходящее задача для spark стриминга какие задачи мы можем именно использовать реализовывать нас парк стриминга или нам необходимо будет искать другой инструмент и тестировать на же задачи на другом инструменте если же мы все таки выбрали spark streaming то мы разберем как именно за дизайнер каждое приложение для самых популярных задач которые встречаются в хранилище данных в принципе для аналитики данных теперь давайте посмотрим на план то есть там имена будем разбирать сначала мы образцом контекстом мы с тони вообще как в хранилищах обычно обрабатываются данные и начнем с краткого обзора пакетной обработки для того чтобы потом спроецировать это на streaming задачи на обработку данных потоковом режиме и мы будем следить что мы теряем когда мы переходим на streaming и что мы в общем то хотим сохранить и какие классные отличие от стриминга мы хотим получить не потеряв классные пищи от пакетной обработки далее мы посмотрим когда все таки нам необходимо переходить на streaming когда нам необходимо когда наступает тот момент когда нам бочонка недостаточно и когда в итоге нам хочешь не хочешь нужно делать 3 мин приложения мы посмотрим параметры стременкова стриминговых задач в зависимости от каждый конкретный бизнес-задачи мы разделим параметры на константы которые присущи каждому streaming приложению а также мы посмотрим переменные которые мы можем тюнить которые можем включать или выключать в ту или иную задачу и в принципе в зависимости от задачи мы просто должны будем тюнить в итоге эти наши параметры в целом логика может быть может оставаться одинаковой и в конце опять же разберем именно задача решения задач на примере spark стриминга и посмотрим какие именно параметры необходимы для популярных бизнес задач для начала для начала давайте der будем держать в голове такой простейший цикл данных который изображен на рисунке в начале у нас есть какие то источники данных их изобразил слева это сайт и база данных моем случае или серым то есть нас есть какие то источники данных которые генерируют настоящие данные которые происходят с пользователем опять же не нужно особо обращать внимание что тут нарезанный на база данных кроме база данных может быть любой веб сервис может быть даже ftp сервер в общем что угодно где мы просто получаем реальные данные и начинаем их анализировать как только для того чтобы эти данные получите загрузить их хранилища нам скорее всего нужна какая-то деталь приложение где может быть парк этим положение может быть spark как раз также здесь изображена стрелочка от витрин данных к сайту эликсир м то есть это означает что мы опять при нем что это значит что мы используем эти данные и мы хотим сделать на этих данных какие-то выводы и переиспользовать их вспомним пакетную обработку с пакетной обработки данные загружаются разведение и скорее всего у нас используется использую дневные партиции которые изображены на рисунке то есть здесь мы можем видеть две части опять же я это изобразил просто для того чтобы обрасти контекстом чтобы мы представили себе в голове пакетную обработку это не значит что любой вид эльба чего выглядит именно так однако давайте пока вникнем в эту задачу с утра это наверное будет не очень просто но потихоньку давайте образцом контекстом и будем держать в голове как данные обрабатываются что мы здесь видим данные для начала как показано в этом примере например загружается в два ночи ночью и попадают в дневную партицию от 3 января что мы видим самые нижние строчки эти данные мы считаем что в общем-то законченные что они содержат все данные которые произошли например clique clique на сайте которые произошли 3 января мы их погружаем 4 января то есть мы примерно может представлять что данные содержат всю полную информацию то есть нас нет live events так называемый хотя на самом деле слайд вентс также эта архитектура может работать мы тогда вы записывали данные в несколько портится общем те кто работают запоздалыми ивентами могут тоже представлять себя эту архитектуру разницы большой на не сделает итак у нас используется дневные партиции данные загружаются разве не и потом из этих данных сырых строится аналитика витрина в нашем случае где мы можем читать какие-то угодно сумм какие угодно суммы и предоставлять эти данные заказчику бизнес-пользователям рассудке к этому в общем все привыкли всем это нравится и у этого есть достаточно много плюсов а именно это хорошая консистентной то есть мы можем доверять нашим данным мы можем делать проверки на наши данные мы у нас есть куча времени на то чтобы сделать любые проверки мы можем проверить качество данных мы можем проверить сумму и сходится свой мы не сходится и вообще то что мы хотим получить или нет а мы можем хорошо управлять наши нашей джаббой который запускается раза в день мы можем ее перезапустить за прошлые периоды если мы сделали ошибку или она упала мы просто можем опять же перезапустить имею с нужной даты если мы хотим исправить нашу логику задним числом то опять же мы просто подъем параметр входной или запустить за прошлое число и сделай витрину за прошлое число и в итоге мы все данные имеем консистентные управляемый мы можем всегда их восстановить и здесь мы можем быть спокойны что у нас данные не пропадут не потеряются из ошибки всегда могут быть исправлены и это хорошо так же у нас есть группировку нас есть предел мы читаем все данные только в режим в пределах одного дня а значит у нас есть большая возможность для маневра в виде группировок сортировок потому что у нас есть пределы этих данных мы можем всегда их сгруппировать отсортировать с джой нет сколько нам нужно раз у нас есть на это много времени в общем-то нас в принципе аналитика наше ничем не ограничена единственны минус большая задержка то что мы можем ждать с момент реально событие может пройти 30 и более часов если открыть событие произошло ночью например то пока мы его увидим подсчете в битве не пройдет уже 30 больше часов и скорее всего это подойдет нам не для всех задач что же делать если нам необходимы данные получать чаще чем раз в день чаще чем 30 часов тут есть два выхода 1 и такой самый самый стабильный опять же иметь и требующие меньше усилий это запускать не мной джаббу несколько раз в день а второй вариант это делать новый устремим джагу и тут уже придется поработать придется что-то анализировать что-то менять и не хотелось бы сразу опуск пускаться в тримминг джаббу для начала хотелось бы попробовать запускать нему джебу несколько раз в день у этого есть плюсы так как логика проверено она работает а не нужно особо нечего думать этим опять же легко управлять и вносить изменения то есть мы можем опять же запускать данные запускать живу за какой-то период и она должна запуститься сделать нужно логику и закончится понятно она понятна и доступна стабильно и понятно назад завершилась успешно или нет и опять же легко управлять но появляется несколько определенных условий для того чтобы это можно было реализовать первое нам необходимо более мелкий партиций на вход если раньше мы использовали партиции часовые теперь необходимо чтобы нам были по крайне мере если раньше мне если раньше мы использовали дневные теперь необходимо хотя бы часовые а может быть и более мелкие патриций если мы хотим читать данные чаще также проблема начинается с женами потому что теперь мы если запускаем одну и ту же джаббу несколько раз в день тает несколько раз мы должны выполнять тяжелые join и мы ограничили входные данные для нашего единственно вот это сайта но если есть джони to join a будут пересчитываться несколько раз целиком особенно если это большие join и это будет может занять кучу времени мы можем даже в час на уложиться а также если мы считаем какие-то суммы то суммы скорее всего нам также необходимо показывать дневные как они были раньше то есть сумма не за час а за весь день и раньше у нас была сумма раньше у нас были входные данные за весь день и выходные данной завезли мы просто делали сумму и все в итоге легко работала но теперь у нас входные данные ограничены входные данные содержат всего лишь час данных о суммы возможно нужно показать человек за весь день поэтому необходимо будет дочитывать либо какие-то опять выходные данные и приплюсовывается суммы вместо того чтобы их перри затирать либо считать дополнительной длинные партиции на рисунке показано опять же как это может выглядеть к нам приходят мелкие клики которые мы разбираем по каждому часу и мы определяем из такого часа этот клик и дописываем дописываем более крупные более крупные дневные партиции из которых опять же делаем витрину ту же самую витрину которая содержит дневные данные но теперь она может запускаться 1 час например но что же делать если нас это не устраивает если допустим у нас есть много джайнов у нас в общем наша логика нам не очень подходят на наш логика более сложное чем можно решить с помощью запуска дневной джаббы несколько разбить нам необходимо переходить на streaming и для начала давайте посмотрим константы стриминга общем то что отличает streaming от баччан г это эти эти фичи стриминга должны быть актуальны не только для spark стриминга а в принципе для любого streaming приложения первое это то что это приложение нужно запускаться один раз и работать пока не упадет а как только упадет на должны вы сразу перезапустите в общем то оно должно работать постоянно то есть нам нужно постоянно слушать новые данные и как только получает новые данные должно обработать их и записать какой-то результат после падения мы продолжаем там же где закончили то есть скорее всего у streaming джаббы есть какая-то начальная точка но нет конечной точке то есть она на она закон она начала свою работу и работает до бесконечности все время все время сохраняя свой прогресс в конце входных данных то есть она не может откатиться назад и перезаписать что-то для этого нам опять же необходимо логика почивать жабы и необходимо сохранить бочче выживу если мы хотим исправить ошибки в прошлых периодах теперь нам нужен особый из точный источник данных или скорее всего источник может быть тот же если это были у нас файлы как пространство теперь нам необходимо просто чтобы эти файлы всегда open делись нам не нужно нам нельзя допускать чтобы эти файлы перри записывались потому что тогда мы их обработаем повторно и нас получится дубли нам необходим источник как в которой идут только append и который мы которые наше приложение уже сможет разбирать скорее всего это будет кафка или какая-нибудь другая очередь казка про страну популярная очередь но это необязательно быть к вк или очередь также могут приходить новые файлы на файлах тримминг работает так же хорошо и теперь у нас появляется проблемы с группировкой потому что если мы хотим почитать дневные сумму или просто суммы по клиентам то в общем то непонятно как ограничивать данные потому что данные могут приходить бесконечно у нас никогда нет гарантии что нам через 3 дня не придет событие которое там случилось три дня назад она просто где-то затерялось в сети она долго очень шло и в итоге пришло к нам когда уже слишком поздно когда мы уже не хотим собрать сумму за 3 дня назад в итоге мы должны быть готовы к что в принципе мы не можем теперь так просто ограничить наши данные давайте посмотрим давайте посмотрим как это выглядит в картинке то есть у нас опять же есть какой то источник данных слева это не обязательно должна быть база данных это может быть гибок сервис это может быть хотя бы ftp server server либо просто опять же база данных главное чтобы она выгружала нам данные о фендом и довольно регулярно то что изображу синеньким append и кликов это либо файловая система либо очередь были просто появляются новые события в итоге происходит что-то и мы в итоге опять должны получить наши нашу витрину наши выходные данные которое разделено по дням но возможны и не под ним возможно нам нужно и большим более мелкая детализация возможно нам вообще не нужны выходные партиции это уже сильно зависит от задачи но по крайней мере мы просто пока думаем что мы каким-то образом хотим складывать данные также как мы это делали раньше давайте теперь подумаем и про говорим о параметрах которые мы можем все таки тюнить и которые мы можем менять в зависимости от нашей задачи если раньше у нас были константы которые присущи всем streaming задачам то теперь есть параметры которые нам которыми мы как раз можем управлять в зависимости от разных входных данных от разных входных требований 1 эта система входных и выходных данных для начала мы сказали что это может быть кафка и gdfs или просто любая файловая система распределенной системы ли не распределенное даже на выход у нас есть на выход у нас есть разнообразие и escape основой стрельба с а также есть файловой системы но о них мы поговорим чуть позже чуть позже подробнее следующий параметр это как мы храним результаты потока то есть как мы следим за прогрессом тех данных которым вы прочитали и как мы следим за суммами которые у нас получились в процессе третий параметр это как часто мы читаем входные данные то есть несмотря на то что это потоковый режим мы можем читать данные опять же раз в час и обрабатывает данные 1 часа можем раз в секунду четвертый параметр это latency это не то что мы прям можем настраивать однако это опция даже опция параметр про за которым нужно следить и нужно оценивать насколько мы даем задержку для обработки наших данных то есть сколько в принципе потребуется времени для того чтобы event от входа дошел до выхода нашего приложения и 5 самое важное это логика обработки данных и логика бывает стоит ли состоит full мы ее разберем подробнее позже но это будет самой важной часть потому что именно это именно от этого пункта в основном зависит какое оно должно быть приложение как мы должны какую инфраструктуру мы должны использовать необходимо ли нам добавлять дополнительные базы данных в нашу обработку или мы можем это сделать на имеющихся мощностях теперь давайте перейдём к более подробно x парк стриминга и я сделаю оговорку что мы будем варить им на прострочен streaming в парке есть два варианта стриминга это spork-и streams или просто спортивных и spark страха streaming и если кто-то с этим не знаком то я скажу что spark streaming или д streams это старая версия который работает на рдд и в принципе сейчас малопопулярна я бы не советую вам ее использовать для своих будущих проектов а спаркс ракши тримминг это streaming новая версия стриминга которая работает нашим санок dataframe of или data set of it самое современное и именно поэтому самая современная функциональность именно про это функциональность парк строчит фермент мы будем говорить итак первый параметр про которого говорили это виды входных и выходных данных и в спаркс трав чик streaming доступные в основу со два самых популярных источника этапе либо кафка либо файлы а также можно подключить любую базу данных если мы только поймем как мы будем управлять комментами в этой базе данных потому что нам необходимо постоянно слушать новые данные и как только мы сделали какой-то как только мы получили какую-то порцию данных нам необходимо отслеживать прогресс в каске отработает а в сэтами в файлах это работает по названиям файлов то есть spark леди таких файлом прочитал и какие нет в кастом коннектором нам необходимо думать об этом сумму на выход доступны все коннекторы из обычного sparco кафка файлы джессика sandra n bass и другие комьюнити коннекторы которых там огромное множество которые могут работать как бочче общем то в общем то любые коннекторы из бача нам помогут поэтому следующий параметр хранения результатов потока мы можем хранить промежуточные суммы кометы то есть прогресс точечных данных а также промежуточные агрегаты агрегаты это суммы которые мы получили в процессе работы и здесь есть две опции это либо мы храним вазу в оперативной памяти ли мы храним файловой системе если мы их продадим в пир в оперативной памяти то они каждый раз сбрасываются при падении джаббы и наш прогресс начинается сначала как будто бы вы ни чем мы ничего не делали поэтому скорее всего нам всегда необходимо хранить прогресс файловой системе тогда это называется чекпоинты спарка с этим все хорошо мы просто указываем папочку в которой мы храним чекпоинты и забываем об этом далее частота хранения данных стандартно sparkling оперирует микро патчами хотя на самом деле есть функциональность когда он обрабатывает прям каждую 1 прим каждую запись пришедшую отдельно однако это функционально с не очень непопулярная и поэтому не очень я бы ей доверял она довольно редко используется я такого не встречал поэтому стандарт на миг streaming оперирует микро патчами и минимальный размер микро бача это одна секунда также можно легко увеличить до часа и это также будет работать стабильно хорошо только еще более это только увеличит пропускную способность если мы увеличиваем размер нашего микро бача главное что как мы должны определять какой миг работе использовать это уступаем ему выполнить логику за время микро бочча и какую этом зимой в итоге даем на финальное событие если мы не успеваем выполнить наш микро батч нашу логику в течение краба chateau spork-ов образует очередь и просто начинает начинает копить сообщение что-то дробот этих позже также возможно ограничить количество сообщений mic рабочий если вдруг нам пришел пришло миллион сообщений в одну секунду и мы не хотим работать сразу совсем миллионам мы можем ограничить для того чтобы опять же sparkly стянул это все очередь супер говорим про latency latency мы говорим не именно в пределах spark стриминга в принципе просто как как признак на который мы должны смотреть и следить во время нашего во время нашей стримами жабы именно сколько времени проходит между тем как событие произошло у клиента то есть в реальном мире и сколько времени проходит между тем как она отразилась витрине я как мы получили какой-то результат по этому событию как видим здесь например показано что пока события дойдет до нашей очереди до доедает дойдет нашего open до криков проходит уже 4 минут и потом мы это обрабатываем и на это тратим еще действий но в итоге только вы нашла танцы получается 14 минут в данном случае самый главный пункт логика обработки данных и многих к обработке данных делиться на 2 вида с title as i stay full для начала что такое стоит на stay close эта логика которая не требует каких-то группировок и не требует который хорошо про ли лица и не требует собирать данные в одном месте или шафф лиц в терминах sparco это логика которая может выполнится на одной записи о блоге которые одинаково выполняется на одной записи или на 1000 записей к таким видам операции относится фильтр map или любая другая логика которую можно применить именно на одну строчку эта логика отлично про ли лица не создает никаких трудностей для sparco и здесь нам доступна вся функциональность парка а также нам доступна вся функциональность в принципе языка программирования которые мы используем а именно java скала питон эскель все что мы можем ли заливать на этих языках потянуть любой библиотеку и если это если эта библиотека работает на одной строчки там и всю эту логику можем распараллелить так чтобы она выполнялась на экзекьютора одновременно на многих серверах и не скапливалась на драйвере это стоит вас логика она хорошая и она стабильная и у спарка с ней все хорошо также к сайту логике относятся join и и join а также хорошо выполняются для каждой строки если мы можем эти join и хорошо выполнять для каждой строки здесь картинка немного забивается забегать вперед потому что здесь уже показаны до или винду на картинке но на них пока можно не обращать внимание а о них про них я расскажу чуть позднее на следующем слайде что я хочу рассказать на этом слайде это то что джейн и могут быть с оперативной памятью с файлами с какой-то из келий база данных или иной скальный база данных и вот здесь как раз уже сильно зависит от того с чем а именно хотим j нить и насколько большие данные мы хотим подтянуть для каждой нашей новой строки если мы хотим если мы хотим обогатить наши данные с большим дата этом то скорее всего нам необходимо использовать новый сквер базу данных для того чтобы мы могли быстро выбирать из этой базы данных строчки по ключу и ключ нас должен быть такой который мы используем для джона для join a из-за нашего streaming приложения если же наш data set маленькие то мы его можем поместить в оперативку и просто он будет во время нашей во время нашей джавы всегда хранится в оперативке даже если этот сет иногда меняется например раз в день то мы можем его просто перечитывать и раз в день заново помещаю оперативку и это будет тоже классно работать если мы не хотим с этим париться между цвет какого-то среднего размера или если у нас большой период микро бочонка то мы можем читать и файлы и эскель базу данных и если у нас допустим есть час на микро бочанг то и можем потратить 15 минут на jojen и в этом тоже ничего страшного не будет просто будем каждый час перечитывать наши файлы заново для обогащения данных для джо и нам мы будем перечитывать базу данных для join a и если у нас на есть на это время это тоже может работать хорошо и теперь самое важное а стриминге самый сложный случай про который нужно очень много думать это стоит в обработка данных а именно то что требует группировки сун сортировок и так далее возможно где дуплицирования это все относится к стоит в доработке и это требует каких-то группировок как мы видим на рисунке у нас есть событие от трех юзеров один два три и два три повторяются и предположим что мы хотим считать сумму для этих событий суммы по клиентам для этих событий и тогда мы видим что сначала мы считаем суммы хорошо когда у нас есть одно событие для каждого юзера но что делать с предыдущими событиями когда у нас пришли и следующий юзеры мы увеличиваем каунт а как же следить за тем что было с предыдущими комнатами для этого в spark стриминге есть пара опций есть опции окна и опции вотермарк а что больше похоже на ритмичными а та же самая логика применяется если мы с джоном 2 стрим от его есть мы jonim мы выполняем джонни со статическим патос этом джойнер 2 стрима на что вообще является достаточно редкой и неприятной задачей и я бы вам никогда не советовал женить 2 стрима в одном приложении лучше для этого использовать дополнительных базу данных но давайте посмотрим что мы делаем если нам необходимо посчитать суммы на стриминге для этого нам скорее всего потребуется обязательно ограничить наш интервал интервал данных пределе которого мы хотим собирать суммы в данном случае это будет окно дневное то есть мы хотим сыграть сумму в пределах только одного дня это необходимо нам для того чтобы приложение знала когда избавиться от старых данных и для этого нам необходимо опять же watermark и в данной конфигурации мы можем указать например что мы хотим собирать самой причине и одного дня и хотя танич памяти всего лишь два дня то есть мы храним суммы только которые пришли не не позднее 2 дней и каждый раз при при приходе нового события мы будем пересчитывать все наши до или винду и обновлять суммы для нашего ателье window результате мы можем перезаписывать нашу исходящую витрину перезаписывает опять же все del window в нашей витрине кликов либо можем просто обновлять 1 запись опять же в зависимости от того как наша витрина кликов работает итак мы рассмотрели мы рассмотрели основные функционер основной функциональный spark стриминга основные параметры какие мы можем тюнить и давайте теперь посмотрим как мы эти параметры можем применять в реальных бизнес задачах и на какой на какой конфигурации нам необходимо остановиться для каждой задачи для начала давайте рассмотрим одно из популярных задач хранилище данных это просто загрузка данных загрузка данных в режиме реального времени в качестве требований тут скорее всего будет что наш выход и должен быть попортится им и в паркете версии в любом сжатом формате колоночка в общем главное не строчному то есть он должен быть как-то сгруппированы и процесс должен должен быть организован таким образом мы читаем какую--то ношу очередь сообщение либо к скале в файлы мы трансформируем данные фильтруем можем менять джейсон и на паркет и и так далее и делаем частая пенда финальные партиции я покажу сейчас рисунок приходит много событий мы выполняем какие то стоит ли с операции то есть мы можем обрабатывать каждую запись по одной или пределах микро батчат там по пять записей по 10 за записи все за секунду за пять минут и просто закидывать много мелких файлов в нашу выходную систему это может быть хранилище данных или таблицах но скорее всего это будет хранилище данных в данном примере и файловая система поскольку нас паркет или другая кромочная как колоночный система хранения как только у нас образовалось много мелких файлов мы можем подождать определенное время и пройтись по этим файлам еще одной джаббой которая сделает компактным то есть объединит много мелких файлика впадин и в итоге вы прошедшее там по прошествии трех дней у нас останется один красивый паркет файл который обновляется в процессе и после трех дней и выглядит хорошо с активированным следующий популярная задача это обогащение данных и для обогащение данных мы опять же можем использовать либо большой либо маленький datasette а также мы можем обогащать динамическим и статическим to those этом если datasette большой то мы можем у помещать поместить в памяти и особенно о нем не думать если datasette извиняюсь перепутал если взять маленький моего помещение в память если 20 большой то скорее всего мы его помещаем в новый сквер для того чтобы эффективно обращаться к нему по ключу и читать не весь dataset для join a а читать только те записи которые нам пришли в нашей стремян джабе читать только обогащение для наших новых записей если же у нас datasette стриминговый они статические то скорее что опять же для достижения лучшей консистентной стенам необходимо вторую streaming 2 стриминговых datasette сначала опустить в новый стиль и 1 streamingassets который мы используем перед joy нам просто опять же джон нить на новый стиль то есть в этом случае новый склеили нам спасает практически в обоих случаях вот так это может выглядеть опять же мы делаем те же самые операции только теперь на каждую строчку мы можем еще добавлять нашу нашу таблицу для обогащения и насколько и насколько быстро мы читаем и таблицу зависит то где-то таблица хранить сохранится на в новой сфере базе данных файловой системе или другой базе данных 3 полярная задача это мониторим безопасности или в принципе можно заменить просто на мониторинг мониторинг чего угодно выглядит это с примерно таким образом мы можем искать подозрительной операция или просто любые операции которые нас интересуют храним в памяти так но по которому хотим делать вывод это последние сутки или последний час и фильтруем самые подозрительные события или просто на самые интересующие нас события например события у которых превышает там 5 5 одинаковых событий для одного и того же юзера в данном случае мы видим что у юзера 2 например есть как попыток входа и мы можем сразу собрать эту сумму за последний час и подсветить и отправить это сообщение эту маленькую сумму только поэтому юзеру в какую-то определенную выходную систему например в карту 4 популярная задача это онлайн отчетность то есть опять же собирать сбор петрин в реальном времени и здесь есть два варианта реализации первый вариант это если мы не хотим использовать если мы хотим оставаться в файловой системе или мы не хотим использовать новый скин базу данных то скорее всего нам необходимо будет каждый раз переписывать выходную витрину переписывают определенную партиции выходной витрине в данном случае показано что мы переписываем час каждый раз когда к нам приходят новые события мы делаем какой-то сдвиг по окну например спускаемся там раз в час наш период микро бача равен час и каждый час мы перри записываемся выходную партицию не очень удобно но это позволяет вам избавиться от новый сквер база данных в таком случае у нас приложение работает в стоит full режиме нам необходимо следить за окнами доплат добавлять дополнительно логику довольно труд трудоемко второй вариант и той же самой задачи это оставит вас приложение если для нашей выходной витрины используется новый скейт база данных в таком случае для каждого нового для каждой новой строчке мы можем получить предыдущую сумму предыдущего задания по счету you сумму и просто сделать плюс 1 к этой сумме и записать эту стыд и записать эту строку в выходную база данных в таком случае приложение будет работать намного стабильнее она будет stateless и мы избавимся от этих партиций в выходной витрине и будем хранить только всегда итоговую сумму и 5 полярная задача это применение миллер опять же в потоковом режиме состоит из следующих шагов первый шаг мы должны прочитать нашу модель мы должны прочитать и модель которую мы хотим применять если она у нас регулярно обновляется то мы и должны перечитывать раз в частном раз в какой-то промежуток времени далее мы примерно так же как и для витрин и собираем обогащаем сырью трансформируем наши данные для того чтобы собрать datasette который необходим для применения модели машинного обучения собственно применяем модель и результат записываем файлы кафку или любой другой приемник который доступен также для всех остальных задача выглядит это примерно следующим образом у нас есть какой-то append кликов у нас есть скорее всего несколько in rich ментов несколько обогащение мы можем потянуть историю по клиенту мы можем подтянуть просто какие-то дополнительные параметры по клиенту мы можем трансформировать наши строчки в какой-то определенной to set мы можем развернуть эти строчки сгруппировать отсортировать в пределах нашего микро бача и как только мы подготовили данные в то в том виде который необходимо модели мы можем эта модель у принять применить и получить на выход какие-то выводы как с которыми опять же вольны делать что угодно мы можем их отправить в кафку мы можем их отправить в какой-то новый стиль базу данных mysql база данных и так далее итак мы с вами разобрали чем отличается streaming от бача какие есть константы у бача что необходимо для того чтобы в принципе начать streaming и потом мы разобрали параметры spark стриминга мы разобрали константные параметры которые присущи всем streaming задачам и именно параметры параметры которым мы можем тюните в зависимости от задачи на примере spark стриминга в конце мы разобрали хорошие параметры которые можно использовать для уже готовых задач которые дают хороший результат консистентной и которые вполне вполне целесообразно поддерживать выводы паспорт streaming много плюсов немножко минусов и многие плюсы больше относится не только к спортингу но и в целом к спарку если spark если эти если эти плюс из парка для вас актуальны то и spark streaming для вас проблемой не станет если только вы не упретесь несколько минусов из плюсов это богатый и я и sparco и на иск вели питоне скаля джаве который в принципе часто использовал довольно популярный и если у вас он уже есть на проекте то проблемой это точно не станет следующей плюс это распределенный для многих операций и то что spark хороша греют операции на экзекутор этой все будет выполняться параллельно и распределена следующий плюс это разнообразные коннекторы которые есть во всем инструментарии sparco следующее это консистентные чтение управления кометами мы не должны думать о нашем прогрессе мы не должны думать о нашем отслеживание того что мы уже прочитали каких сообщений мы прочтем мы прочитали не прочитали spark управляет этим всем за нас следующий плюс от эффективные планы обработки данных spark здесь также применяет свою планировщик запросов он строит эффективные планы он нам подсвечивать какие операции мы можем выполнять какие не можем выполнять и здесь мы тоже можем нас парк положиться и довериться ему следующее плюс эта поддержка m-elle что является довольно редким для streaming приложений и последнее это экзо клевым гарантии с одним патентным приемником что на самом деле является довольно тоже часто фичей для многих streaming мешков вот с этой оговорочка эдем патентные приемник в общем-то все хорошо пока у нас приемник и дым патентный что значит патент на всякий случай скажу что это значит а если мы упадем и сделаем потом операцию несколько раз если мы запишем одну и ту же строку несколько раз то наша выходная система должна понять что это одна и та же строка и не дать нам ее за дублировать то есть основном это зависит наше выходное системы чем от нашей логике так что с маленькой оговорочкой что у нас есть этот патент на приемник данных мы можем обеспечить экзарх ливан гарантии стриминга теперь минусы у нас не очень богатый функционал управления окнами то есть если нам необходима сложная логика зависимости от времени события то есть мы там хотим следить например за сессиями мы хотим и ждать там 10 минут пока события прошло и потом через 10 минут после этого события применить принимать какое-то решение the spark не очень хорошо позволяет управлять нашими данными в зависимости от времени события практически что там можно сделать это только сделать вот эти окна на время события и окна можно сдвигать определенным интервалом но это не покрывает всех задач и есть rect некоторые случаи когда мы не можем воспользоваться каналов для того чтобы реализовать необходимую логику и последнее это зависит последний минус это стабильность нашей джаббы принц сильно зависеть от того на чем spark наш работает будет yarn обернитесь и какие именно сервера использовать для этих планировщиков ресурсов если у нас и так сильно нагруженный кластер где-то крутится много джебов то эти джордон могут мешать нашему стриминга и настя миг может регулярно падать и тогда нового и тогда нам его нужно будет в общем то перезапускать и он будет продолжать с того же места где он упал проблем с этим быть не должно однако если нам не хочет за этим следите нам хочется что чтобы мы запустили приложение но работал там 3-4 месяца чтобы мы на него даже не смотрели нам необходимо будет сильно подумать о нашей инфраструктуре и от тем на каких ресурсах количество наш spork-и если все эти ресурсы расчистить если предоставить свободное место для spark а то он в общем то может работать стабильно многие месяцы и не падать и сам следить за всеми своими за всеми своими статусами и поднимать упавшие джаббы перераспределить чтобы и так далее здесь мы тоже можем ему довериться если у нас стабильная инфраструктура все спасибо артём спасибо друзья ваши вопросы я пожалуй верну этот слайд если кто-то хочет этим поспорить потому что здесь все считают довольно большой горизонт оливарес спасибо большое за доклад смотрим мы по сути в рамках твоего доклада сказали то что у нас streaming но давайте не будем будем делать микро починка да да потому что возможно нам нужно просто уменьшить задержку во время привели для появления финальных данных и не обязательно нужен streaming в этом случае вы это знаете да стриминга у нас по сути остается только ситуация когда у нас стоит вас полные вообще как бы можешь time was примеры это тогда им range streaming это тогда когда у нас предложение работать постоянно и она не должна завершиться никогда и органа ну то есть ты так и еще когда есть есть один слайд предстать на прям пять пунктов именно когда это обязательно streaming то есть когда приложение не должна завершаться и когда мы постоянно треком новые данные в приложении в основном вот это главные пункты of states is painful в общем то просто трудности которые добавляются для streaming приложения по сути то говорит что как бы давайте просто называется именным все когда мы работаем ну относительно непрерывно где упали там продолжим если что-то да именно поэтому я называю это потоковой обработкой они обработка реальном времени все спасибо спасибо еще вопрос а вот у нас тут есть два проявляться спасибо за доклад внутри самое интересное the prostate full в чем он хранит свой state и что будет если этот стоит про теряется в чекпоинтах он хранится то есть он но опять же я говорил что вот можно использовать чекпоинта или не использовать чекпоинта если мы их не используем то есть мы храним прогресс оперативки то он теряется и такое практически никогда не нужно делать скорее всего мы должны использовать удачи плент и для того чтобы хранить кометы и вот стоит full как раз наш стоит вал суммы то есть весь 3-ндфл принцев чек-поинтов там же где и кометы поэтому если наша джабба упадет то когда на западе заново она продолжит имена с тех жестом которые были до этого а и возьмет она их из chip and a face помнить хранится файловой системе до файлов с этими просто а если диск выйдет из строя у нас файловая система распределенная поэтому раз придя на фарме 1 тема не должна выйти из строя оффе и которую spark предоставляет это что-то типа киева или будет чтобы утром опять которая для работы состоит а мы такие вылью а это в общем-то обычных до сет который который доступен this batch и выйду обед с это просто как выглядит табличка там колонки полной сквер там огром угрозе тонармы of rain да-да-да маленький то тот же самый the flame практически все функции доступны кроме там редких исключений это с ними тоже можно еще поиграться в общем против в полный функционал dataframe а доступно прикольно пример seba согласен прикольно доброе утро спасибо за доклад очень интересно подскажите пожалуйста вот не совсем понятен пункт с едим патентными приемником это на выходе из парка имеется в виду что spark может за дублировать какие-то свои расчеты ну или на входе к нему как раз и подразумевается что он может переварить одно и то же событие три несколько раз он не может переварить одно и то же событие несколько раз если мы используем чекпоинты то есть это он хорошо трахает доч имеется ввиду игроки ван гранте может нарушиться на выходе нашим то есть допустим нас есть нам два выходных источника допустим мы сначала пишем в dfs потом мы пишем в кассандру то есть нужно сделать две операции и допустим и записали в кадре фреска с андре мы упали получается мы перезапускаем джаббу он знает что нужно обработать опять же вот те же самые входные данные и он прогоняет их по полному кругу то есть опять их пишет фхд fs и в кассандру и вот в ходе fs он бы заданный за дублировал если бы хдс был ней темп антенный а вот как мы сделаем и дым патент нас в хадисе например зависит от нас мы можем делать убирать партиции какой-то или там убирает файликов с одним и тем же данными либо мы можем просто их делать об ентом если мы делаем append получается у нас нет м патентный выходной приемник получается мы дублируем данные получается не экзарх лимон с гарантии печать сет-лист vans а если мы пишем например кассандру то кассандра перетирает записи подключён об остальных обновляет получается кассандрой ты патент на и получать тут мы особо нет ничего не должны даже думать кассандра для нас всегда это подобное получается мы сколько бы раз не писали данные всегда получаем экзо кривонос да спасибо большое понятное объяснение у нас есть вопросы резон лайна здравствуйте спасибо за доклад меня интересует как реализована функция авто скиллинга меня просто интересует гарантированная обработка время обработки данных чтобы весь поток был я понял в парке есть несколько для этого опций и там есть как динамическое выделение ресурсов так и стандартные выделение ресурсов если на основе стандартно этому просто соответствовать для им просто учу ресурсов и их занимаем и наверное это не то что вас интересует сколько вас если вас интересует авто skinning мы хотим наверх начать с маленькими ресурсами и продолжить потом увеличивать насколько я ну то есть если мы хотим увеличить прям наш plaster то есть реально добавить какие-то облачные сервера чтобы не включились то тут наверное не только функциональность парка нужно еще необходимость связывать как-то вот со всей инфраструктурой но если у нас уже есть готовый кластер ён просто пустой и мы как бы сначала хотим занять чуть чуть кластера а потом занять больше кластер еще больше кастера то в спарке есть тоже опции динамическое выделение компьютеров и все что мы задаем это верхний лимит то есть если спарку необходимо мало памяти то он а где которые не используют а как только ему нужна память он раздувается до предела определенного а вопросов к вам сам взаимодействие надо планировать вот этот скиллинга заранее там не очень много параметров настройки там есть просто вот по параметры тот авторских илинга динамическое выделение ресурсов и верхний лимит и все то есть а все остальное внутри он сам планирует как он там займет как он освободит это уже дело спарка всего здравствуй спасибо за доклад и вопрос какой вот насколько вы вот эти джаббы они устойчивы к изменениям ну то есть если нужно job обновить новой версии будет поставить задачами понятно ты просто вот код изменил и все а тут видим это как торчик pointee то же самое на самом мы просто диплом новую версию и когда мы диплом понятно что предыдущей версии упадет и должна сразу запустится новое и когда запускает когда запускается новое оно ложно взять прогресс из чип плэнтах и тут могут быть и на проблемы если мы сильно поменяли обработку данных допустим этом изменили колонки там мы теперь обработаем другие колонки тонн то он может не разобрать spark может не разобрать прошлый чекпоинт и начать прогресс сначала и тут же опять же вопрос мы хотим сохранить нам наши промежуточные суммы или не сохранить также если мы запускаемся если обновляем версию мы можем указать новую начальную точку для нашего приложения то есть мы допустим хотим начать мне с середины дня он там не вот с 2 дня с 00 часов допустим что об этом сегодняшний день хотя бы охватить его можно задать первую начальную точку и сделать новый чит point то есть наши джеба как бы возьмет начальные какой тип n и будет работать снова и будет сказать про бесов новую директорию не использовал в прошлую то есть тут мы можем тоже подтюнить сохранить прогресс ней сохранять прогресс в зависимости от того можем ли мы переиспользовать прошлые суммы всем спасибо за вопросы артем тебе спасибо за доклад очень четко по полочкам разложил как как под правильно готовить spark streaming но тебе осталось выбрать самый лучший вопрос за который мы подарим книгу у нас если вопрос из онлайна отлично давайте вопрос так есть время на вопрос был вопрос я не услышал просто из онлайна должен был быть вопрос а или вы должны услышать онлайн человека куда она должна услышать звук крутова а вот они круто большое спасибо за доклад у меня есть несколько вопросов во первых какая власть инфраструктура второе что есть одни факты сыр и перед описываются и третье сколько одновременно обрабатывает один не ну это же как бы так я не понял второй вопрос поэтому могут ведь и пока на 1 3 1 значит инфраструктура я работал со стримингом со spark она стремится стринги и на ярни и нами за себя и на купер найтись а то есть принципе а и в качестве баз данных танцоры использовались и gdfs и кафка выходные были их от их pdf с кассандра hbs то что принципе инфраструктуры много разный и везде работ примерно одинаково зависит сложность возникало только с коннектором квтч пэйсу на самом деле это это тоже касается не только стриминга в принципе просто коннектора к базе данных так что с тремя крайность работает везде одинакова на любой инфраструктуре не он на это не как не влияет следующее по 2 3 чтобы я и увлажнение и возможно пример печь в какую-то дату и дописывает и чём именно файлы не папа понял скорее всего нам не нужно будет нам не нужно делать так чтобы мы записывали все в один джейсон файл мы нет нам необходимо это делать либо следующий файл а еще лучше следующий партиции опять же чтобы это было и дам патент на также чтобы также возможно чтобы сохранить эдем патента есть нам необходимо будет не просто делать append и пришедших данных также перечитывать те данные которые были уже до этого для того чтобы опять же переписывать каждый раз всю партицию то есть не просто одно сообщение которое пришло до этого переписывать еще то что было до этого опять же завести на от хотим ли мы сохранить экзо клеманс хотим ли мы сохранить на патент ность хотим ли мы использовать файлы в общем здесь могут возникнуть сложности и здесь как раз необходимо подумать инфраструктурой и необходимо ли нам вообще файлы они новый стиль какое-то решение для того чтобы опять избавиться от дублей и так далее здесь мы множество опций множество вариантов смешивания архитектуры логике перезаписывание агентов и так далее но spark предоставляет любую такую функциональность тут уже зависит от того как мы именно организуем эту дедупликации юрвик consistency нашего и последнего просто что делать если как мы как и начинаем несколько входных источников как мы и денег надо мне ваших одной точке и ней поработать о тех несколько проведен job до эту проблему хорошо решает кафка потому что кафка как раз зависит решает когда удалить данные не от того кто их прочитал от того как долго они там лежат тут то есть если мы хотим несколько контейнеров то нам идеально подходит кафка но даже если мы читаем файл это все равно мы тоже про должны решить когда нам удалить файлы а если мы не хотим удалить файл сразу просто после прочтения их оставляем опять же без проблем их читает просто другой источник друг друга я джабба все понятно спасибо спасибо спасибо очень классно что к нам смогли подключиться из онлайна но"
}