{
  "video_id": "EEi0eO8ztaY",
  "channel": "HighLoadChannel",
  "title": "Построение инфраструктуры LLM с нуля на основе опыта Х5 Tech / Мичил Егоров",
  "views": 4856,
  "duration": 2742,
  "published": "2025-02-06T02:25:11-08:00",
  "text": "Всем привет Меня зовут мичел Я сейчас Работаю руководителем команды разработки продукти визаир интеллекта в X5 Извините а И сегодня я вам расскажу как какой инфраструктуру мы построили какой архитектуры мы пришли а какие были сложности Расскажу чуть немного про выбор моделей как это делать и подведём в конце итоги А давайте начнём с традиционного знакомства Меня зовут как я сказал мичел до X5 я писал кнд и разворачивался в Яндексе до этого строил Биг дата инфраструктуру в старлайне и параллельно с этим писал научные статьи и метори в университете имо откуда я и выпустился КС ПМ я совсем недавно с середины прошлого года и как раз-таки в этот момент Мы создали команду irun за прошедший год мы сильно выросли У нас сейчас уже более 4 человек в направлении а сделали очень много пилотов связанные с лэм ками И не только А за это время успели обработать более 10 млн запросов на генерацию и протестировали очень и очень много ЛМ моделей а для каких кейсов мы применяем мки это очень много кейсов связанные в основном конечно с чат-бота и суфлёра для модераторов из интересного могу выделить сумарин г резюме и обработку отзывов клиентов Понятное дело что для поддержания всех этих кейсов параллельно ещё эксперименты эсеров нужна какая-то инфраструктура чтобы обрабатывать эти запросы Но прежде чем перейдём к инфраструктуре Давайте посмотрим какие к ней были требования какие у вас у вас будут и какие предпосылки были у нас для того чтобы наша инфраструктура была масштабируемой требования конечно же базовые требования - этон пропускная способность мы не хотим чтобы пользователь ждал по 2 минуты ответа какого-то чат-бота и хотим обрабатывать не запросы от одного клиента а от тысячи следующее требование - это конечно же безопасность мы не хотим сливать какие-то какую-то коммерческую тайную персональные данные третьим лицам и мы должны заранее подумать как мы будем с этим бороться это стабильное качество ответов Мы хотим чтобы с каждым обновлением модели или их сжатием и всему остальному у нас Оста высокое качество ответов для мки Это имеется в виду качество генерируемого текста возможно качество классификации возможность ну как моделька думает и так далее конечно же хочется чтобы это можно было удобно масштабировать и для этого мы наве это мониторингом и каким-то логирования подскажите пожалуйста кто из вас сечас строит рение на основе открыты модельки типа gpt кто подумал про безопасность про передачи персональных данных и так далее ну видим 50-40 про на самом деле это тоже путь к использование к но почему мы вообще решили думать про локальные модели про их разворачивание потому что это дорого и всё остальное какие есть проблемы использования открытых опи вопервых данны это самая большая проблема сейчас в бизнесе потому что мы не хотим передавать нашу коммерческую тайну и персональные данные клиентов третьим лицам в данном случае это как раз таки ваши провайдеры I это зависимость внешних систем на примере моделей от Open вду вро они ра обновляли к снижению качеству качества ответов но нигде они об этом Вообще нигде не писали и на самом деле этот случай супер сложно отловить потому что сначала упадут ваши продуктовые метрики потом Вы посмотрите почему это так произошло и окажется что вы ничего не меняли но ваши технические метрики тоже упали третья причина - это Линейная связь между количеством генераций то есть количеством ваших пользователей и стоимостью использования этой фишки то есть тем больше чем больше у вас пользователей тем больше будут затраты на использование сторонних сервисов но чтобы эту линейную не было этой линейной связи в какой-то момент вам придётся подумать про то чтобы разворачивать свои локальные модельки Ну и это конечно же ограничение на использование то есть Вас могут заблочить потому что вы не из Той страны или могут посчитать ваши запрос сказа ши запросы не соответствуют политике компании Вот и исходя из этих предпосылок Мы решили строить свою инфраструктуру какие были у нас вообще водные и какие есть вызовы к следующему двадцать Пятому году Давайте на частоту ни про какой лот у нас пока не идёт речи мы сейчас скорее Тестируем какую-то гипотезу сейчас у нас только два сервера это rtx и GX на нём генерируется 40 rps на пике это от дети пилотов и экспериментов эсеров Но сейчас есть огромный спрос от бизнеса мы должны масштабироваться не знаю на все каналы общения с клиентами внешними обрабатывать все не знаю видео встречи которые проходит внутри X5 и к двадцать Пятому году мы должны отрабатывать 10.000 ПС на модели на самом деле мы это предусмотрели поэтому сразу придумали архитектуру таким образом что его можно было легко и удобно масштабировать ну говорим про инфраструктуру какую-то Давайте наконец-то на него посмотрим а вот так выглядит архитектура А давайте представим что у нас есть какой-то проект сторонний который хохочет использовать новейшие модели ЛМ возможно наши локальные возможно какие-то внешние А первое куда они приходят - это некий гейтвей где происходит авторизация этого сервиса учёт токенов затем уже две пути то есть сервис может захотеть обращаться к нашим локальным моделям тогда они идут в наш LM backend и eding inference модельки хранятся в неком Mod registry в нашем случае это какой-то C ML эти сервисы чутко мониторят логи ется И за счёт этого можем предоставлять доступ к локальной моделям а но есть ещё другой сценарий использования они могут захотеть использовать внешние модели как я говорил напрямую мы доступ дать не можем потому что наш не позволяет это делать поэтому мы придумали некий обход через маскирование и де маскирование входящих и выходных данных Давайте посмотрим на узкие места это конечно же наши сервисы инфе моделей потому что они требуют какого-то GP ресурсов это сервис маскирования потому что там тоже Ну используются какие-то мные модели и как бы то ни было странно это внешняя модели а Объясню почему Потому что обычно сейчас провайдеры как правило предоставляют доступ через опиш А у одного II ключа есть ограничение на количество токенов в минуту Поэтому нужно придумывать А какой-то менеджер ключей чтобы можно было генерировать большой rps А мы пока к этому не пришли потому что не требуются но в целом нужно будет в дальнейшем про это подумать А как можете заметить что все сервисы носят какой-то технический характер кроме сервиса маскирования и демаскировка у него Ну скорее такой продуктовый смысл Ну давайте начнём с него и посмотрим все остальные сервисы как они устроены И зачем они нужны компоненты система Первое - это маскировки который предоставляет данные предоставляет запросы к внешней моделям В чём суть а допустим к нам приходит вот такой вот запрос в пятёрочках начали продавать слонов и крокодилов для уточнения информации нужно Позвонить некой Светлане А можете поэтому номер не звонить Светлане есть Пятёрочка это выдуманный номер а что нужно сделать чтобы отправить такой запрос нам нужно замаскировать данные Что это значит мы меняем а какую-то коммерческую тайну то есть про пятёрочку на некую семёрочка меняем мимо Светлана на другое и предоставляем совсем другой номер телефона и такую информацию Мы уже можем смело отправлять а внешним лмм а потому что мы не отправили ничего За что могут нас отругать ишни вот но Понятное дело что если мы отправим такой запрос мке она вернёт информацию не про пятёрочку и Светлану А про какую-то семёрочки и Марию и тут происходит обратный этап де маскирование То есть те данные МОК на которые мы замаскировали Нужно вернуть в исходные и это называется ки операци де маскирования Какие вообще трения к Маров Поня дело что во-первых это высокая точность распознавания чтобы мы не сливали никакие персональные данные коммерческую тайну потому что будет высокий риск узнать про кого конкретно идёт эта речь второе требование - Это согласованность имеется в виду что если мы заменим например номер телефона на какие-то Рандомные числа то мке будет сложно понять что это идт речь про конкретный номер телефона и изза этого Т съехать из этого может съехать качество генерации и это работает на самом деле и с семенами то есть женское имя лучше заменить на другое женское мужское на мужское А с адресами с номерами паспортов и так далее Возможно даже с Днём Рождения С годом рождения следующее - это консистентность То есть если в одном месте мы заменили Фёдора на Ивана то дальше по тексту Мы должны по-хорошему всех Фёдоров переименовывать в Ивана так и самое последнее очевидное это мы у внешних людей не должно быть способа вернуть исходные данные из замаскированных данных Тогда вся вся суть маскировка пропадает Так давайте дальше Зачем он нужен во-первых там происходит какая-то авторизация балансировка нагрузки между модельками возможно Но самое главное что она выполняет Это она унифицировать мы придумали свой как бы супер способ передачи данных на генерацию использовали его во всех проектах Но со временем начали выходить инструменты которые автоматизировали работу с МКА Ну тот же Lama ик и нам приходилось каждый раз писать свой провайдер к нашей инфраструктуре со временем нам это надоело это было сложно поддерживать потому что нужно было держать свой форк каких-то библиотек Но как оказалось все модельки поддерживают формат И для этого нужно просто задать четыре ручки это Get на получение списка моделей это пост на генера Ну продолжение текста этот comp на продолжения чата и для получения вставление текста мы поддержали все эти методы у себя в Gateway и теперь можем как бы с коробки ходить по S инструментов просто меняя базовый URL C Open на нашу ссылочку Давайте дальше это token Management Она работает по тарификации pays Goo То есть ты платишь только за те токены которые ты сгенерировал и подал на вход Таким образом получается супер прозрачная система биллинга а но главная суть не в этом а в разделени компьютера между клиентами то есть допустим у нас есть какой-то сервис который генерирует очень много запросов в секунду и есть такой риск что он может занять весь свободный компьют и другие клиенты не смогут постучаться к нашей инфраструктуре А с помощью явного ограничения по количеству токенов генерируемых например в минуту мы можем явным образом делить компьют между сервисами и избавиться от риска который я Обозначил а следующее это самое интересное Наверное это про Ирен моделей на самом деле Существует очень много инструментов для этой задачки Мы у себя используем vlm и Tex eding inference А почему они потому что во-первых vlm поддерживает некий распределённый иренс через механизм R про который Антон рассказывал на предыдущем докладе А и Например если у нас есть моделька весом 80 ГБ то нам не требуется пушка не знаю а соя на 80 Гб а потребуется четыре пушки по 24 ГБ по которому можем распределить этот иренс А можем пойти ещё дальше если у нас есть несколько серверов А мы можем тоже по ним распределить этот Ирен за счёт того что vlm с коробки поддерживает этот механизм и мы можем а суперум масштабировать а inference моделей использовать vlm супер просто вам понадобится просто Python А мы его устанавливаем через Pip Install и серм с помощью вот такой вот а простенько команды Ну это очень удобно для проведения разных экспериментов Но если вы хотите использовать это в продуктивен нер и разворачивать либо через comp либо в куб нетис а МКА тоже это поддерживает с коробки на самом деле мы сначала сидели на Lama cpp в самом начале потом решили использовать какие-то другие подходы и у нас был выбор в то время ме между TJ и VM мы повели их для своих для своего окружения и получилось выбить ускориться в 30-40 раз по количеству генерируемых токенов Но это удалось нам не с первого раза потому что во-первых у нас была старая версия драйверов из-за этого нам Пришлось сделать код и всех пилотов и экспериментов чтобы обновить эту куду оказалось что не поддерживает коню из-за старой версии именно видеокарты это вообще никак не починить оза пожи из работает чуть медленнее чем не кванти Зро модели и самое страшное - это падает изза кошена если переполняется GP память во время инса мы не знаем как с этим бороться кроме как перезагружать сам сервис но чтобы этого не было нужно явно ограничивать виртуальный память кото вы нужно мониторить и логировать про мониторинг Я много не расскажу это стандартная связка Прометея и фаны Прометей может доставать данные Ну конечно же сра и с и с мки То есть она предоставляет такие технические параметры как количество генерируемых токенов в секунду скорость генерации между токенами и так далее а про логирование есть что рассказать мы используем инструмент infuse - это некий распределённый иренс типа Яг а специализированный Именно под ный модели и который учитывает проблемы разработки сервисов которые используют мки а она ещё поддерживает некоторые версионирование промто можно при желании проводить между ними а тесты можно хранить датасеты и этот инструмент недавно защитил раунд в кобине он активно развивается Так что советую вам присмотреться к этому инструменту интерфейс выглядит примерно вот так справа можете видеть э трейс для чат бата то есть можем видеть там этапы перефразирования запроса а вытаскивания контекста предобработки этого контекста и непосредственно самой генерации в которой мы можем видеть посерёдке то есть у нас есть inp это запрос пользователя а контекст который мы нашли и то что мы выдали пользователю вот таким вот образом выглядит текущая архитектура а готовая К масштабированию И А который мониторит и логировать новая модель и непонятно как она будет решать наши задачки будет она лучше или хуже М давайте как раз-таки про это и поговорим вышла например какая-то новая модель на публичных бенчмарках она бьёт всех и вся Казалось бы отлично бери и обновляй но есть две проблемы во-первых лучше ли эта модель на самом деле и лучше ли она на наших Задачка У нас есть Прикольная история связанная с этим вышла в апреле д чего года какая-то модель мы посчитали что она супер классная протестировали ну как получится и выяснили что она ну реально лучше нашей текущей модели Мы сели вечером Обновили и стали ждать фидбек с утра нам позвонили сказали что моделька начала глючить не то отвечать вы что-то сделали не так ну мы А что полезли смотреть ги и заметили вот такую вот интересную деталь вот так модель отвечала до обновления вот так стало отвечать после Да вроде бы ответ хороший правильный но не в том контексте и не в том сервисе дальше мы расследовали это дело я казалось что моделька хоть и лучше генерирует русский текст но она хуже следует си и почти не смотрит на контекст который мы передали пришлось срочно откатываться к старой версии и думать как бы нам уже модельки обновлять и наконец-то пришли к своему какому-то внутреннему бенчмарк для бенчмарка нам требуется три вещи это запрос пользователя это контекст и эталонный ответ даери какой-то ответ с помощью запроса и контекста и сравниваем его с эталонным ответом есть несколько метрик это могут быть классический ж Blue и так далее А есть метод dpv который используют мки для оценки ответов других ллк и это работает зачастую даже чуть лучше и больше коррелирует с разметкой людей вот так вот может выглядеть пример на примере Как принимать алкоголь у нас есть некий запрос контекст - это текст статьи написанный в нашем каком-то инфоцентра таки сгенерировано ответ модели мы его сравниваем таким образом у нас получается свой внутренний бенчмарк на своих Задачка и можем увидеть что какая-то вот розовая модель она лучше по всем Метрика нашей предыдущей модели и мы можем смело обновлять нашу модель и не Ожидайте звонка в 8:00 утра что наша моделька полетела А за прошедший год мы успели протестировать очень много моделей А некоторые из них можете видеть вот тут Если у вас будет вопросы по какой-то из них то смело переходить в дискуссионную зону я вам расскажу про них Чутка подробнее Но конечно же это нужно автоматизировать для этого мы построили некоторый fure Store У нас есть запросы пользователей которые собираются во время пилотов и промышленной эксплуатации всё это мы складываем в некоторую витрину данных Откуда мы отправляем данные на разметку и сотрудники вот поддержки Как раз-таки помогают нам его размещать а размещенные данные обратно кладём в витрину и используем для своих целей а для этого мы используем оркестратор workflow Perfect мы достаём данные фю нашего а заводим его laing в Click House из кликхаус в labeling Service Для нас это жила м как только данные разметили мы кладём его обратно в Click House и у нас получается удобная витрин данных которой может пользоваться и DS команда и которой мы пользуемся из мля м запуская бенчмарк что в итоге в какой-то момент вам придётся задуматься про использование собственной инфраструктуры и нужно будет подумать об этом Чутка заранее Но если вы решили использовать какие-то внешние модельки то как-то их защищайте Используйте некоторый Маров самописный или можно у кого-то даже купить если будете думать про собственную инфраструктуру то Выбирайте Кэн для Ирен с умом А чтобы не было такого код Фриза как у нас м и как можно угодно заранее начинайте собирать пользовательский фидбек чтобы собирать некоторые бенчмарки и смело обновлять модельки и чтобы у вас не было такого казуса как у нас Всем спасибо за внимание готов ответить на ваши вопросы Спасибо л большое Сейчас переходим к секции вопросов напоминаю за лучший вопрос Вы можете получить подарок от онтика кастомную матрёшку А если вы будете задавать много классных вопросов то много матрёшек я живой свидетель у меня дома небольшая коллекция вот так что задавайте ваши вопросы прошу ну давайте так да вот у меня раз Про алкоголь заговорили У меня вопрос я не мог не спросить а в Ланг фьюз нету модели для мониторинга как раз токсичных вопросов и тем более на русском языке Вот что-то вы смотрите в эту сторону или ещё нет это не является проблемой вот вашего внедрения А да Привет Жень у нас есть свой внутренний вот как раз-таки во-первых бенчмарк на который мы смотрим и во-вторых мониторинг генерации конкретно текстов и мы сравниваем содержимое Ну и смотрим Как там хорошо получается вот про токсичность мы пока не думаем Вот но наверное придется подумать в ПМ году То есть это что-то что на радаре Да понял Спасибо большое а вот ещё короткий вопрос в можно режим просмотра логов ограничивать по правам потому что сами мты которые в модель уходит Это довольно чувствительные данные Да там пользовательская ну конфиденциальная информация есть все кто доступ к умеет всё видят или там есть настрой да Это хороший вопрос в есть Так называе прое в нашем случае какой-нибудь кес то есть один чатбот вот и эти проекты как раз таки разделяются по правам которые можем выдавать с помощью админки и например одни сотрудники поддержки могут видеть запросы к своему утту но не смогут видеть Запрос к другим и таким образом появляется явная правовая какая-то модель простой там есть Спасибо бытует мнение что вот в твоём кладе только один маленький кусочек услышал про пром ты бытует мнение что 90% как бы вопросов ответи и вот промт они очень сильно улучшают ответ А что вы делаете с этим Есть ли у вас как-то обучение пром тамм там сотрудников или вот вообще в целом вас сервис там как-то докт эти мты Да спасибо большое за вопрос с прота сейчас мы не предоставляем его сотрудникам поддержки скорее этим занимается некая ДС команда которая конкретно занимается продвижением этого бизнес-кейс вот а Есть такое мнение Да и оно было правильным наверное до вот середины этого года а потому что модельки стали сильно умнее они могут сами уже дорабатывать свой промт А и очень хорошо следуют инструкциям Так что А сейчас прямо сильно запаривать на промто уже а не имеет большого смысла а нужно использовать какие-то модельки посильнее Спасибо Здравствуйте чи Спасибо за доклад Я здесь Я здесь а вот хотел задать такой вопрос вот по поводу ваших бенчмарком то что можно у мки спросить оно практически никак не ограничивается То есть это очень большое множество вопросов но бенчмарки эталоны выкладываете ВС это бесконечно множество в эталоны И вот вопрос А как вы работаете с разницей между тем что вы сложили в эталоны и тем что может спросить пользователь ведь это множество намного больше Угу Да хороший вопрос а в бенчмарке мы стараемся покрыть ту базу знаний по которой моделька должна отвечать а и если Бывает такое что спрашивает не по базе знаний то есть две пути Первое - это ответить Извини я не знаю Вот либо сказать Извини я не знаю но можешь поискать похожие вопросы в таких в таком-то разделе А позвольте тогда уточнение А сколько у вас бенчмарком хотя бы ориентировочно Ну вот этих вот эталонов которые вы храните количество про количество данных я не скажу но в целом мы собираем бенчмарки по пяти проектам это как раз таки пять чат Батов А разве Тогда тогда нужна МКА разве это не просто Экспертная система Вы же можете приводить вопрос Сва это в экспертную систему без вообще да Это тоже вопрос для Лива Стоит ли так делать Вот в целом Существует множество способов эту задачку решить есть например способ решить это спомощью правил То есть если задают похожий вопрос то иди в эту ноду и так далее вот можно собрать базу вопросов ответов инид косинус растояние вопрос Вот Но эти системы они сложные для использования именно сотрудниками поддержки То есть если какая-то информация добавляется то они должны его дублировать в одном месте Куда Не знаю человек может зайти или это какая-то инструкция Вот и в базе это как раз-таки чат бата А мы работаем с единой базой данных единой базой знаний и они должны обновлять это только в одном месте и это им Ну намного удобнее вот плюс к этому мы можем добавлять некоторые механизмы например механизм уточнения про тот же отпуск Можно спросить 10000 вопросов и непонятно что человек имеет в виду и обычный человек когда он заходит в чат бата он не спрашивает как мне получить доступ к сервису с отпусками например он просто пишет отпуск Вот что он имел в виду уже нужно в дальнейшем до уточнять и как раз таки справляется Спасибо большое за ответ Спасибо за доклад Здесь вопрос про компью ресурсы и видеокарты как они делятся можно ли их делить между сервисами разными моделями и кто как бы за это отвечает И где это настраивается если это вообще можно делать Да это можно делать вме то есть мы явно указываем Какие видека он должен занять индексу А и Какой процент этой видеокарты предоставляется этому сервису Вот и таким образом мы можем явно ограничивать Какие видеокарты эта моделька использовать и Сколько памяти она будет занимать в общем пуле Добрый день Спасибо большое за доклад Алина Грибанова сбер а подскажите пожалуйста используете ли вы какую-то автоматизированную валидацию ответов модели и автофикс в том случае если она ошибается перед тем как выдать результат пользователю а то есть вы имеете в виду вот мы сгенерировать ответ и мы должны его как-то пере тировать Да Что для того чтобы не выдавать пользователю заведомо ложный ответ да на самом деле проблема галлюцинации это основное Что может быть при проекте при проектировании систем сло ланками м и мы боремся с этим с двумя путями во-первых мы используем прочен чтобы модельки меньше люци и выдавали более эталонный ответ А после этого м происходит этап проверки на галлюцинации и этот этап можно сделать как угодно мы используем Крос энкодеры для проверки Подходит ли ответ модельки к к тому контексту который мы подали То есть если мы видим что она ответила вообще не по контексту то мы заранее говорим а Извините мы не смогли ответить на ваш вопрос Извините раз раз да здравствуйте Спасибо за доклад Вопрос такой там собственно была компонента связаная с маскирования запросов тут два таких под вопроса один может ли дать какие-то подробности по поводу того что у вас тут из поис а второй под вопрос Есть ли тут какой-то е дополнительный контроль качества то есть Понятно понять смысл того зачем это нужно и соответственно вот есть ли какая-то дополнительная ещё проверка от того что эта функциональность действительно работает так как мы ожидаем то есть что например мы там вообще никакие персональные данные не сливаем что здесь ничего плохого не происходит да большой вопрос получится Сейчас отвечу по порядку во-первых мы используем сейчас решение от подрядчика Вот про какую могу рассказать в дискуссионной зоне но мы выбирали сразу между тремя подрядчиками и для этого Мы создали некий свой внутренний тоже бенчмарк по проверке качества этих систем мы его составляли с помощью мки той же то есть генерировать какие-то Моко вые данные затем мы их как-то маскировали в полурусским категориям как умеет этот маскировки решать То есть это какая-то организация это имя адреса и так далее и таким образом мы могли сравнивать эти три решения между собой затем для использования в продуктивен и де маскирование и мы смотрим Какие вообще логи уходят к внешним сетям И это всё собирается в от ишков Вот И там уже нно происходит просмотр того что несли мы случайно какие-то персональные данные то есть они просто в ручном режиме какой-то СМЛ данных посмотрит там с какой-то периодичностью да да Добрый день Спасибо большое за доклад Богдан подскажите пожалуйста вы сказали что у вас есть некие Инфоцентр которые генерируют базу знаний соответственно в больших базах знаний очень часты противоречия и необходима какая-то обратная связ МКА каким-то образом генерирует метрики для обратной связи к базе знаний чтобы её можно было поправить и разрешить противоречию Угу э как раз-таки это удобно L Fuse или предоставление лмо очень удобно тем что конечные пользователи А например админы этих сервисов могут увидеть непосредственно что эта моделька отвечает вот мы им раз в месяц предоставляем какой-то Дамп они его смотрят и если появляется какой-то проблемный запрос то они могут зайти посмотреть что реально тут есть какое-то противоречие и сами редактируют эти письма таким образом чтобы этого противоречия больше не было То есть там может быть либо устаревшие данные либо э неправильно написали и как раз-таки с помощью этого инструмента а администраторам этого инфоцентра очень легко исправлять А какие-то свои статьи которые они написали ну то есть это Не поставлено на автоматический поток это раз в месяц происходит там О ну оффлайн по сути Ну в целом Да спасибо большое Добрый день спасибо за доклад Максим а у меня вопрос возможно это не ваш конкретный кейс и у вас проблем таких нет но иногда после ответа Нужен пост процессинг ответу а соответственно на вашей схеме Я не понял Куда бы вы его засунули то есть cle или же это должен обрабатывать там специализированый сервис который вот под эту модельку которую организовали или ещё куда-то А вы имеете пост процессинг именно ответов перед выдачей пользователю или пост процессинг данных которые мы храним для бенчмарком пользователю то есть модель что-то выдала вот там же можно настраивать и формат вывода ещё что-то там всякое соответственно иногда нужен знаю что-то сделать например увидеть что А антифрод ещё что-нибудь и соответственно вот на вашей архитектуре куда бы вы его добавили если его сейчас нет Если есть то где Он был Если я правильно понял вопрос Ну расскажу на примере например для тех же чат-ботов очень важно смотреть на галлюцинации и может быть такое что мы Галю Ну конкретно в ссылке вот ин в этом случае заключается в том чтобы посмотреть на эту ссылку а понять есть ли она в контексте если его там нет то Это скорее всего какая-то галлюцинация и можем либо сразу отвалиться сказать сори не знаю либо попытаться понять Какая из этих ссылок правильная Вот и этот механизм как раз-таки должен выполнять непосредственно сервис который обращаются к нашим мные моделям вот задача инфраструктуры - это предоставить удобный интерфейс для генерации вот а как эту генерацию сделать уже решает сам непосредственный сервис То есть это тот зелёный квадратик который был про Здравствуйте спасибо за доклад вопрос про мультитекс из внешних источников Вот вы как-то это обошли стороной Вы как-то учитываете что не только текст может быть а наме картинку присли и если нужно в контекст добавить данные не только из внутренней базы данных ну базы знаний а там из Википедии к примеру Есть ли какие-то веса что вот из базы знаний с одним весом надо подмешать Ну обогатить контекст А из внешних источников с другими весами мм да а на самом деле этим тоже занимается как раз-таки внешний сервис А могу рассказать на примере чат ботов м а ту-ту-ту-ту-ту Да мы сразу даём инструкцию что мы работаем Сейчас только с текстом если какая-то часть инструкции содержится в картинке то Извините мы не сможем его распознать вот есть две пути либо полность игнорируем этой картинки либо как-то расписываем в инструкции что в этой картинке содержится Вот то есть пока сейчас мультимодальной так таковой нет Работаем только с текстом Вот про внешние источники по Бест практис в принципе их просто подмешивают в каком-то соотношение а прямо задать смотри на этот контекст С такой с такими-то Весами сейчас скорее нельзя чем можно вот и всё это подбирается с помощью экспериментов командами эсеров То есть у них есть Задачка им предоставляются какие-то входные данные То есть это вопрос пользователя и эталонный ответ и задача де эсера максимизировать э-э выдачу сгенерированных ответа таким образом чтобы он максимально был похожий на эталонный а не будет проблемы ограниченности эталонного ответа а такая проблема есть Наверное вы имейте в виду что не всё указали и моделька может более развёрнуто ответить А да такая проблема есть а у разных заказчиков по-разному то есть они могут сказать Нет только так и вообще никак по-другому тогда мы стараемся к этому приводить другим Ну скорее больше нравится развёрнутые ответы и мы тут исходим от потребностей бизнеса непосредственно спасибо спасибо за доклад Подскажите пожалуйста как часто приходят запросы от продуктовых команд на тюнинг И как вы их решаете или это остаётся на там на дв и вы просто платформа Ирен это да задача Ну во-первых мы понимаем хотим понять зачем им нужен фай тюнинг вот если это для того чтобы Отвечать по базе знаний то мы говорим Ну извините можно делать не тюнинг а какой-нибудь ра пай построить вот показываем им примеры ответов и если им это нравится то идём к этому сценарию У нас сейчас пока не было сценариев где мы бы использовали полноценный тюнинг понял то есть вы ещ ни разу не было задачи которая потребовал тюнинг а да такие задачки мы либо сводим к более лёгким каким-то способам более надёжным вот э мы что-то до тестировали Вот но до прода Пока ничего такого не докатываю понял спасибо Добрый день Вы упоминали кросс энкодер который вы используете подскажите пожалуйста какой кросс энкодер показал наибольшую эффективность учитывая что они не все хорошо работают с русским языком И второй вопрос это вы говорили по поводу того что использование vlm далось лучше намного эффект чем Лама cpp не проводили ли вы исследования с чем этот эффект связан и ускорение было комплексным или связано ли именно с большим использованием видеокарт а то есть насколько данное ускорения применяется и при инфс без гпо Угу да про энкодеры они у нас исполь используется для двух целей Первое - это как ранжировка и второе - это для проверки галлюцинаций для аранжировщика мы подбирали какие-то орные росн кодеры которые скорее ухудшили точность тем увеличили Вот Но мы сделали несколько подходов по добу этих энкодеров на Дан и они дали рост где-то на 15 пунктов вроде Вот и мы использовали Вроде вот но могу поправить честно не помню вот про Да она быстрее cpp по нескольким причинам забыл почему-то упомянуть во-первых МКА она поддерживает как раз таки динамический батчинг за счёт которого увеличивается пропускная способность и она использует меним ши какие-то токены для ускорения как раз таки генерации Вот И там очень много маленьких трюков которые позволяют это как раз таки и ускорить Вот то есть она использует тоже количество памяти пушки Но более эффективно Спасибо А небольшой под вопрос а Крос энкодера Вы добули по на классификацию или выделяли какие-то кластеры при добу или добули именно на примерах то есть по типу мы обучали его на constraint Ленинг то есть задавали позитивные примеры и негативные Ну то есть бинарка Угу спасибо Здравствуйте вопрос про маскирование как мы видим вводятся новые законы в России про граничен ную передачу перс данных и маскирование не может работать со стопроцентным качеством поэтому два вопроса Первый планируете вы отказываться от внешних лмк в будущем и второй какое сейчас у вас качество работы маскирования хорошо так отказываться Ну явно скорее нет чем да потому что понятное дело закрытые модельки они лучше решают какие-то определённые задачки чем закрытые вот поэтому не хотелось бы отказываться второе качество было что-то на уровне 9597 но могу уточнить и позже направить Вот Но как я понимаю у подрядчиков есть сертификат качества что они умеют хорошо это делать и прошли какую-то защиту вот поэтому мы наверное без зрения совестью можно использовать такие системы если что это их вина вот как-то так Спасибо друзья доклад Чила был очень интересный ваши вопросы тоже я просто наслаждаюсь когда слушаю но к сожалению тайминг у нас заканчивается поэтому предлагаю оставшиеся Вопросы задать мичил в экспертной зоне Я думаю он будет рад на них ответить А сейчас время выбрать лучший вопрос и получить матрёшку Вам слово какое вопрос был лучшим сейчас Дайте вспомнить Вот у этого молодого человека Напомните вопрос я помню что у вас хорошо да про пром и их использование то есть нужно ли их улучшать или нет хорошо поздравляем вас И также у натель пода за замечательный доклад чил Спасибо большое спасибо"
}