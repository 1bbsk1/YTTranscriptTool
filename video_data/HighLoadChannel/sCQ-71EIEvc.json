{
  "video_id": "sCQ-71EIEvc",
  "channel": "HighLoadChannel",
  "title": "Опыт миграции между дата-центрами / Михаил Тюрин, Сергей Бурладян (Avito)",
  "views": 1355,
  "duration": 3388,
  "published": "2017-04-22T14:48:17-07:00",
  "text": "Миша и Сергей поведают о том, как они success success story, да, что называется, классический пример о том, как они тягали все неподъёмные терабайты Авито с датацентра в датацентр. И, по-моему, даже как-то там чуть ли не без простоев или что-то такое. Ну, в общем, Миша и Сергея обо всём этом интересно расскажут. Давайте поприветствуем. Так, всем привет, кто пришёл, взял пиво, всё такое. Значит, ээ, мы представляем Авита. Меня зовут Михаил Тюрин. Справа от меня Сергей Бурлодян. Да. Э, значит, наш доклад менее узкоспециализированный, что ли, наверное, скажем. У нас такой больше happ and successor, как мы мигрировали между датацентрами. Вот я не знал, что мы будем в зале Москва выступать. Это очень символично, потому что мы мигрировали как раз в Москву. Значит, Сергей у нас самый один из самых ведущих специалистов по базам данных в Авито. Я выполнял в данном данной миграции роль такого технического менеджера и координатора, консультанта, архитектора, что-то в таком духе. Вот. И я видел картину более широко. И на самом деле этот доклад можно было как-то и более широко анонсировать. Но вот из-за того, что мы в основном мигрировали базу данных Постгрес, у нас попало всё это, значит, в секцию баз данных. И, собственно, на этом у нас упор, да. Значит, в Авито. Э почему это ещё так перекликается? То есть это основное хранилище. Вот. И, собственно, переезд не може не мог бы быть сделан в нужных требованиях, конечно, если бы они перевезли правильно базу данных, где лежат все наши объявления. Так, где кликер-то у нас? Так, ну, собственно, ещё раз, почему это всё High? Ну, Avвито - это очень крупный проект. Таких проектов в мире всего меньше, чем на, ну, такого масштаба и такой специфики их всего несколько. Вот. То есть можно вспомнить там йigлиist, американская крупнейшая доска. Вот есть у нас европейская крупнейшая доска это вида. То есть хайлода у нас есть. И мы тут немножко коснёмся этого. Вот. То есть хайлод здесь, конечно, присутствует. То есть, но присутствует ещё много чего интересного. Сейчас мы про это поговорим. То есть вот с недавних пор наши законодатели, уважаемые люди, значит, они начинают афектить э нас. Хотя на самом деле, когда я начинал этим заниматься, никто никогда не думал, что вдруг будет зарегулирована наша деятельность. И когда Авито был не в Москве, то есть мы переезжаем между датацентрами из другой страны в нашу страну. И мы, э, когда изначально Авито строил датацентр и использовал датацентр от другой страны, мы, собственно, закладывали риски политические, но другие, да. А теперь, значит, у нас новые инициативы, есть новые законы, и приходится под это всё подстраивать подстраиваться. То есть вот это как раз этот переезд, он инициирован именно законодательными инициативами. Ну и, собственно, наверное, если бы не это, то мы бы никуда не переезжали. То есть это важный момент. И возможно вот там на вашу работу, э, уже сказывается либо будет сказываться то, что надо следить за законодательством, потому что оно начинает касаться м нашего интернета. Что ещё какие-то моменты, значит, указаны тоже сейчас мы это всё расскажем. Вот. И мм также, например, можно вспомнить, что сейчас вот закон о хранении э различных логов вот тоже касается, в том числе базу данных нашу, но это немножечко за рамками доклада. Значит, ну, собственно, ну, мы в итоге переехали. Изначально изначально, когда мы думали о переезде, ну, конечно же, первым первая мысль такая была, что, ну, как мы вообще переедем, да? Вот взять базы данных и перевести. Значит, ну, мы тут вспомнили, что вот есть, я сейчас не вижу, не присутствуют ребята. Вот до этого до нас уже рассказывали про то, как постгресы работают между датацентрами. Ребята из Яндекса. Вот Владимир Бородин, нету сейчас здесь. Значит, рассказывает много уже тоже тоже у него такая истории в Яндексе. Они использую несколько датацентров. И когда нам стала задача, что да, всё-таки мы переезжаем в Москву, мы подумали: \"Ну, Яндекс работает, ээ, постгреспочта между дата-центрами\". Ну и мы тоже как минимум на момент переезда с работы. Вот. Ну, значит, первоначальная мысль она, конечно, была в итоге правильная, что мы переедем. Вот. Но вот есть особенности, то есть когда вы используете свою локальную сеть, это ваша локальная сеть, вы ей управляете. Вот. А когда вы по интернету начинаете что-то использовать там, вообще говоря, вы не управляете э каналом, то есть как он будет вам доступен, как как эта полоса пропускания будет по факту доступна вашим конкретным соединениям, конкретным TCP сессиям. И вот это мы всё тоже столкнулись. Вот у нас получилось, что а это наше было самое, то есть понятно было изначально, что самое критическое место для переезда будет сеть. И вот с сетью мы как раз всё и начали. Всё м у нас всё пошло не так просто. Вот при этом важные особенности такие, что если у вас переезд между центрами, то у вас будет другое оборудование. Вот, значит, и все особенности нового оборудования, потому что, ну, как бы, ну, всё планировалось, значит, в другой стране, там были свои поставщики, наважен как бы какой-то свой канал. Здесь у нас новый дата-центр. Кстати, вот ээ хорошо сейчас надо отметить, что есть выборы и как бы выбор был сделан среди какой-то конкурентной среды. В Москве появились много центров как раз под все вот тренды на возвращение данных на родину, под импортозамещение. сейчас существуют мощности, но всё равно это будет что-то другое. Это будут какие-то новые машины, там какой-то новый порядок их обслуживания. Всё это м всё это вы столкнётесь с этим, скорее всего, если будете так мигрировать. Вот. И при этом будет ново оборудование там, если будут новые нагрузки. И причём вот как наша практика показывала, новое оборудование с новыми какими-то новым софтом - это не обязательное оборудование, которое работает как-то лучше под вашу нагрузку. И у нас это вообще сначала было неочевидно, что мы переедем, потому что, но сам процесс переезда был, а был достаточно продолжительным. Вот мы планировали это несколько месяцев и реализовали потом несколько месяцев. За это время сменилось сменилось сменилось несколько разных версий софта, в том числе поменялась операционная система, которую мы используем. мы используем Deban. Вот. И там менялась версия ИДРА Линкса и производительность прямо скакала. Так что изначально мы думали, что если мы будем переезжать на новую версию ядра Линкса, то вот мы так не умеем, нам не хватает тпсоов, и мы будем менять архитектуру приложения, потому что оно перестаёт работать с одной базы данных, которая отдаёт нагрузку. Вот всё это вот в процессе переезда приходилось решать. Значит, вот ещё вот касаясь касаемо общей проблемы, да, то есть вот, а, вот, да, я вспоминаю, то есть было порядка там 30 сабтасков к какому-то эпику. Ещё эти сабтаски цепляли там порядка несколько десятков dependд задач. 6 месяцев планировали, причём изначально также планировали. Так что за эти 6 месяцев ещё нагрузка вырастет. То есть мы здесь закладывались на то, что в процессе того, как мы будем это всё переезжать, мы будем много-много много фич делать, всё это будет накапливаться, развиваться. Вот, значит, здесь база данных работает в окружении других сервисов. Там были обособленные команды, которые думали, как пере перевозить и другие части площадки. Но вот здесь надо выделить отдельно, что картинки нам очень помогли. У нас очень большой кластер картинок есть. И он, ээ, вот до на тот момент ещё, а он выдавался, все картинки выдавались прямо с площадки датацентра. И поэтому вот первое, что, значит, было такое вот проведено, то есть мы начали тестировать московскую площадку именно тем, что мы кластер картинок перевезли в Москву и начали выдавать картинки оттуда. И тем самым просто вообще потрогали, что вот эта вся махина трафика больше 50% картинки отдавали. То есть у нас канал будет выдерживать, значит, там все роуты будут правильные. машины там, ну, они реально существуют в этом центре, что вот они вот там там файлы лежат, они отдаются, всё это какое-то время обслуживается, всё прокачивается. И вот когда мы это всё вот сделали, то мы даже в итоге дошли вот до этого. То есть с учётом сложности всего переезда вот и с учётом того, что всё-таки этот центр работает, и мы видим, что он работает и картинки отдаются, то мы решили, что мы даже не будем этот процесс ээ делать э оборотным. То есть к момент, когда мы только начинали это всё планировать, мы думали: \"Ага, давайте сделаем так. А если вот мы сейчас всё переключим в Москву, настроим все репликации, все стриминги, а все миграции, а система в какой-то момент времени будет работать в двух режимах, то есть будет вся пишущая нагрузка идти в заграничный дата-центр, всё будет как-то стримиться, зеркалироваться в Москву. Э вот, а потом мы переключимся в Москву. Вот и и Москва не взлетит. И что делать? Вот. И мы решили, что такого не произойдёт, потому что иначе прихо момент приш при переводить шведскую площадку в такой же, э, ну, как бы в некотором родейв Москвы. И вот этот последующий последующее такое переключение всей эпонул шведскую площадку, да, мы схотили из Швеции, то есть перевести всю шведскую площадку в некий глобальный сйф Москвы за момент до онтайма. Вот это всё мы не решили сделать, и мы так в итоге не делали. Вот. Ну, собственно, да, то есть мы интуитивно подумали, что, да, репликация будет работать у Яндекса. Она работает в синхронном режиме, э, как минимум иногда, это я точно знаю, да, между центрами. И, значит, ну, мы решили, что уже синхронные наши все практики сработают. Значит, ээ, так. Ну, и сейчас я потихонечку буду передавать слово Сергею, но да, да, да, да-да. Значит, но скажу то, что вот, ээ, э у нас много есть таких вот внутри текущей площадки архитектурных решений, которые нам позволили, применяя их, э, ну, в некотором роде прозрачно весь этот интернет видеть. То есть вот мы, допустим, можем подключать ээ несколько несколько реплик к нашим узлам, подключать их логические, физические потоки, и это всё не афктит ээ не аффектит источник. То есть мы можем на самом деле были могли бы могли бы в некотором роде переезжать там сразу же в несколько центров, просто подключив больше аппликационных потоков. Вот. Но то есть я к тому хотел сказать, что вот наша вот это переезд и мы сейчас это должны увидеть, он во многом опирался на уже на готовые наши решения по репликации. По репликации. Сергей, дай сюда встану. Давай, а то меня не видно. Правильно. Так. Ну, первый вопрос, который возник, когда мы собирались приезжать, как вообще можно посчитать вот этот трафик, необходимая ширина канала в Москву. То есть у нас использовалась вот такая вот вот такие типы репликации. У нас есть стриминг, есть валархив, а есть логическая репликация на лондесте и есть система вызова процедур, ээ тоже основана на Sky Tools, на PGQ. Ну вот мы решили, что самое простое можно просто взять вот и в Мунине посмотреть трафик наших серверов на тех интерфейсах, через которые идёт репликация. То есть вот мы просто брали в Мунине график, смотрели среднюю там полосу максимальную и из этого оценивали пропускную способность общего канала в Москву, которая нам будет необходима, чтобы запустить туда репликацию. Ну вот у нас получалось, что вот 12 серверов, которые по вал архиву работают, генерирует суммарно где-то 100-150 Мбит. А плюс три сервера логической репликации давали в сумме меньше 50 Мб. Ну вот мы, как Миша уже говорил, долго это всё подготавливали, и поэтому мы ещё закладывали там небольшой запас с возможностью последующего роста. Ну а всего вот мы переезжали ещё сами данные изначально копировали через ПЖ Backup. И где-то мы вот перевозили, если суммарно сказать, то где-то базы от 10 ГБ до 3 ТБ. В сумме, наверное, всю площадку можно грубо оценить там 10 Требт, я думаю, не сильно ошибиться. Ну, где-то так, да, наверное. Ну, NFS, конечно, мы не стали через интернет запускать. Это как-то очень сильно круто было бы, да. Поэтому первое решение было идея просто мы сейчас включим стриминг в Москву и как бы всё заработает, да. Ну вот, если так схему эту представить, то есть у нас есть слева Швеция, справа Москва. И вот в Швеции мы единственное, что меняем, это добавляем стриминг валов в Москву на стендбай. Ну вот здесь возникает множество проблем с с таки с такой схемой. Ну первая проблема - это то, что мы читаем валы с мастера. Если у нас, допустим, лагает сеть, допустим, час нет связи с Москвой, эти валы на мастере ротейтятся. Не, не ротейтися. Почему стриминг? А ротец у нас там я внизу написал 92 постгресса. У нас нету слотов. То есть чекпоинт проходит, файлыв заротейлись, и всё, мы потеряли стендубая в Москве. А если мы делаем архив конд, то здесь возникает другая проблема, то, что валы как раз не ротейтятся. И вот этот часовые там или более задержки с отправкой валов, они вызывают накопление валов на мастере, и может просто переполниться диск. Ну какой вывод отсюда? то, что ээ читать валы с мастера отправлять нам не подходит. Нужно придумывать какое-то другое решение. Ну вот второе решение было сделать отправку валов непосредственно с машины, на которой вал архив. А, ну для этого нужно как-то понимать, какие валы мы уже отправили в Москву, какие ещё нет. И просматривать весь архив с валами там каждую секунду, допустим, это будет очень накладно. Поэтому мы чуть-чуть доработали наш архив Command и добавили туда просто создание специальных пустых файликов с именем Вала, которые ещё не отправились в Москву. А при отправке в Москву просто эти файлики удалялись. Таким образом мы отслеживали, какие уже отправили валы в Москву, какие ещё нет. Ну и плюс архивкоман у нас, она сжимает валы. И таким образом мы даже вот чуть-чуть уменьшили трафик, необходимый для передачи в Москву по сравнению со стримингом обычным. То есть схема выглядела вот приблизительно так. на архиванном, на архивном сервере мы запускали просто наш баш срипт, вот этот Sentw. Он в цикле просматривал этот каталог с пустыми файликами, с именами и отправлял необходимые валы в такой же архив уже в Москве. А я ещё забыл упомянуть, что вот, да, у предыдущей схемы ещё одна проблема - это то, что здесь получается нет архива. То есть есть только один стенбай, архив оставался в Швеции. Ну, а когда мы начали уже синхронизировать архивы, то, соответственно, у нас архив уже в Москве появлялся. Ну вот, ээ, с такой схемой тоже у нас не заработало. Здесь была другая проблема, что вот у нас был канал 2 гигабита в Москву из Швеции, но вот оказалось, что у этого канала такие интересные особенности, что для одной TCP сессии мы можем разогнаться только до 2 Мгабит. Ну и через 2 МГбита мы уже не успевали прокачивать валы из Швеции в Москву. Вот пришлось придумывать ещё другое решение. М. А, да. В чём она заключалась? Ну вот схематично она где-то так выглядела. То есть нам пришлось вот доработать наш sendт, э, вот функцию sendes, заменить там вызовсинка напрямую на вызов нашей обёртки над Рсинком, которая отправляла файлы параллельно. Вот у нас появился такой башевский скрипт Parallel RS, который отправлял файлы в несколь запуская несколько экземпляров Рсинка. А, ну вот схематично это так выглядело. Итоговая уже получившаяся схема. То есть для синхронизации постгрес серверов, которые используют вал архив, мы использовали вот этот скрипт для отправки в несколько потоков валов из Швеции в Москву. А для логической репликации, так как она у нас всего там суммарно 550 Мбит, и каждый отдельный из этих серверов потреблял меньше вот этого ограничения двухмегабитного, у нас получилось их завести просто так напрямую. Ещё момент добавлю. Здесь вот там Сергей упоминал, что если если интернет начнёт лагать, то мы могли бы в случае вот стриминга взрывать мастер по размеру, потому что валы бы не могли отратироваться, и они бы занимали бы всё свободное место, пока интернет, который нам предоставил внешний неконтролируемый, по сути дела, провайдер, он бы не работал. Вот. Но вот здесь мы видим, что такого вот буфера и параллельно как бы нового механизма для очередей логических нету. И вопрос такой задаётся: \"Да, а что будет с вашими теми репликациями? Они так разве не взорвутся?\" И оказывается, что они не взорвутся. Да, Сергей? Да. Почему они не взорвутся? Потому что это логическая репликация. Она логическая в том плане, что она логическая до того, что мы реплицируем таблицу. И по сути дела мы берём только очень ограниченную часть данных, гоним по этим репликам. И это ограниченная часть данных даже за большие интервал времени, там до суток. Это наши обычные аварии. Они такие бывают и в продакшене. Мы это сопоставили и поняли, что даже если до суток будем терять канал, что уже как бы, ну, совсем неприемлемо, да, что будет означать, что надо тормозить вообще весь переезд и зана всё перенастраивать, то наши вот эти логические буфера для логического для логической репликации они прожуют такой такой лаг, да, что они, конечно же, не мо невозможно сделать при потоке валов, которые мы имеем на отдельных продакшн узлах, да, постгреса. Ну вот ещё одна из особенностей, которая позволяет это реализовать, это то, что вот в Sky Tools, PGQ инфраструктуре его можно на одну очередь подключать несколько консюмеров, несколько потребителей. Поэтому вот можно было сделать такое дуплирование в Москву одной и той же очереди. Так, ну, следующая проблема, с которой мы столкнулись, это вот у нас ещё в Швеции была такая проблема с тем, что мы не могли обновить одни из наших серверов, самых нагруженных нового Дебина. То есть у нас там стоял Deban шестой, туда приходил трафик. Там, допустим, он сколько? Ну, ну где-то пять там 5.000 транзакций в секунду, допустим. А когда мы обновляли Deban 7 максимум, сколько мог выдать сервера, там 2.000 транзакций в секунду всего лишь. И у нас, соответственно, всё падало. Поэтому у нас эти сервера работали долго времени на Deban 6. А в Москве ожидался Deban 8. И был вот такой вопрос: сможем ли мы вообще их включить в Москве, чтобы они смогли выдать такую же производительность или придётся переделывать приложение? Ну вот мы потестили ПЖбенчем ещё в Швеции сервера с Deban 8, но на них по получалось падение производительности не такое катастрофическое, как с Deban 7. Всего лишь там в 20% падала производительность. в DN8 постродились с DBН6. Тут важный момент там, да, на графике будет видеть, что мы в тот момент как раз и имели запас по ЦПУ 20% и получалось, что мы всё равно как бы вот уже не пролазим в не пролазим, да. Ну, в общем, мы решили, что да, нам этого, в принципе, должно хватить. Не, ну там не сейчас там дальше потом это дальше коснётся. Здесь мы просто применили, ну, сейчас забегу вперёд, сразу же просто применили два. Мы сначала ещё немножечко, э, ещё приложение замодифицировали, чтобы оно меньше потребляло ресурсов ЦПУ. И тогда мы вроде даже пролазили 20%. Конечно, мы двухкратное производительность не могли потянуть, но заоптимизировав приложение и дальше мы сейчас покажем марки, мы всё-таки смогли переехать. Да, вот когда мы потестили на новом железе в Москве, это Debн восьмой, оказалось, что он даёт наоборот больше производительности, чем у нас шестой деби в Швеции. То есть там получалось почти в два раза, даже больше, если вот такая вот схемка получалась. То есть лево - это Швеция, Debн Debн. Вот мы тут очень долго не могли график нарисовать, потому что он, смотрите, тут old DBN6, old DBN8, New Debn 8. Тут какие-то много измерения, но в итоге что видно, что Debn 8 даже на на а как сказать даже более худший Debn8 у нас всё равно в Москве давал больше производительности, да? По железо другое, конечно, да, там другое железо. У нас в Швеции были Аптероны, а в Москве Intelкн и там чуть побольше мегагерос. Да. Мы и мы думали, что что мы такого такого нашли такого потрясающего, что вообще с такого произошло. Тут надо отметить, что это за наши такие постгресы были. У нас это так называемые inmмеory наши постгресы. Вот которые что только и делают, что ходят в память и дальше выдают в сеть. Вот. И мы потом взяли, чтобы как бы понимать, что происходит. Мы взяли БВ тест, есть такой BCHMK влинсе. Просенье, да, простенький бичмарк. Мы взяли там такие же примерно пачки по 8 Кб, что-то типа того. Начали бенчить, увидели, что, да, память гоняет Intel в два раза быстрее, собственно. Ну и, соответственно, тут мы поняли, что всё нормально, мы как бы не какую-то аномалию ловим, а в принципе это соотносится с тем, что по памяти Intel работает в два раза быстрее. Угу. Так, ну ещё следующая проблема, с которой мы столкнулись, это уже переезд на Debн 8. Там по умолчанию был System D. И в общем, мы обсуждаю пообсуждали с нашим отделом DevOps решили, что мы будем пробовать систм D использовать. Всё, всё-таки, да, невозможно против против ветра невозможно всё равно. Ну и возникли вот такие проблемы. А то, что мы смигрировали в Москву, запустили там сервера и они начали выключаться. Это вообще потрясающе. Вот первая проблема, да, оказалось, что наш скрипт, который по SSH ходит, отправляет валы, а подключается к серверу стендбая, пишет туда вал, отключается и как только он отключается от сервера, систем D убивает там, а постгрес косвенно через Да. через ресурсы. Он считает, что пользователь отлогинился. То есть у него была настройка такая, да, по умолчанию remove IPC. И вот систем де такой настройкой просто берёт и удаляет те вот, как он, да, system V или как они, ресурсы, которые используются постгресом, и постгрес падает. А здесь ещё это вызвано тем, что вот у нас так по по идее по умолчанию систем D так поступает только для пользователей, у которых большой номер user ID, там больше тысячи, по-моему. А у нас как раз вот использовался не маленький номер для как для системных пользователей, а большой. И мы как раз вот столкнулись с такой проблемой. Ну вот на неё тоже есть ссылки в интернете, в том числе вот рассылки постгреса. Так, а следующая проблема, с которой мы столкнулись - это то, что у у Юнита System D по умолчанию есть время, необходимое для выключения его. А у нас есть сервера, которые полностью в памяти держатся. Там unloged таблицы, и когда их выключаешь, так-то они unlockт таблиц, они не чекпоинтятся, а когда выключаешь, происходит большой всплеск чекпоинта, они все пишутся на диск, и это занимает много времени. И поэтому выключение постгреса занимало больше времени, чем тайм-аут дефолтный в System D. И в процессе выключения постгреса system D его убивал. Соответственно, получали битую базу, все аналоги таблицы очищались. Ну и было много проблем. Нет, ну тут не то, что мы получали битую базу, мы получали битые так, ну если совсем поточнее, битые аналоги таблицы. Ну что, конечно, битая база, но это не битая база, потому что база развалилась, потому что есть такой баг. А просто аналогии таблицы, они все пустые. Во, она даже не битая, они пустые. Но в логическом смысле они, конечно, битые, потому что у вас нету данных. То есть это не позгрыз падает, понимаете? Да. Это это просто логически всё разваливается, но system D виноват, да. Ну вот можно настроить system D, в принципе, вот там есть такие настройки kill и send. Но вот ещё до этой настройки можно так вот с помощью вот указанных команд выключить постгрес. Так как мы используем Deb, там используются специальные враперы над штатными утилитами постгреса для управления. И вот если их вызывать с такими ключами, то эти враперы не используют систем D, а выключают постгрес через ПЖК. Ну, ещё одна проблема связана с тем, что вот оказывается в System D нету команды старт. Э, то есть она там как-то захардкожена в сишном кодем D в виде последовательного вызова стопстарт юнита. А для пжбаунсера нам необходима команда рестарт, потому что при рестарте пжба баунсер не останавливается у нас, а наоборот запускается специальным ключом мину R, который позволяет перезапустить PG баouncer без разрыва соединений. Да, PG баounce в нашей архитектуре - это очень такой центральный проксикомпонент. И вот эта особенность, конечно, давала нам головной боли. И поэтому мы запускаем PG баouncer не через unit system D, а через папет, через наши скрипты. Здесь тоже случилось, да? Вот оказалось, что мы так запускаем его, а систем D благодаря CГрупс отслеживает дерево м родители дочерней процесс, да? И вот наш парбасов, запущенный через Папет, оказался дочерним для Папетгента. И когда вот однажды нам потребовалось перезапустить папет агент, Системд убил и наши баунсеры тоже. Ну вот мы это решили с помощью ещё дополнительного костыля в наших скриптах, которые после перезапуска Баунсера переводят их в другую группс в более верхнего уровня. Блин. Ну вот это, да, вот это нам добавило даунтайма, честно говоря, конечно, да, потому что когда систем начал убивать реплики, то есть у нас саш команда копирует ну бинарный файл с логом транзакции, чтобы standby работал. Снbй под нагрузкой, значит, standby вырубается всё как бы. Ну и вот так чик-чик-чик-чик-чик. Да. Ну вот можно сказать всё, что вот благодаря переезду в Москву мы получили такую замечательную картинку. Да. И вот здесь, собственно, моё как раз пояснение, чтобы совсем вы не считали за идиотов. Вот вот здесь вот вот здесь вот мы сначала прежде переезжать мы поняли, что мы вообще рискуем всем, и мы здесь вот два раза сняли нагрузку. Вот здесь вот в самом краю такое двухкратное падение, поэтому в целом мы в четыре раза выиграли, но это как бы уже более-менее соотносится. То есть пристальное внимание к приложению позволило регулярку вынести на с базы наверх. И ЦПУ появился. Ну вот. А потом он ещё совсем появился. И как бы мы здесь вот не можем больше утилизировать, больше нету трафика. Да. Ну давайте я вот начну итоги подводите, мы потихонечку подведём итоги. Да. Значит, ээ мы как бы поставили на две половинки, что у нас как бы не принялось, что завелось. Значит, ну вот НФС, вся наша репликационная штука, она в лоб всё-таки не завилась, как если уж совсем быть точными, да? То есть NFS мы не стали между по интернету гонять, потому что у нас и не по интернету, когда сеть лагала, даже внутри этого центра с НФСом могли быть проблемы. Вот, значит, ну, можно отдельно поговорить. Значит, дальше текущий архив, э, он тоже в лоб не завёлся, потому что оказалось, что он был, оказывается, завязан на пропусную способность, которую мы не смогли выдержать э из-за нашего особенности вот этой сеть какраздкой скоросеть, она двухмегабитная, нам и дала такую проблему, мы её не смогли в лоб применить. Вот тут хороший вопрос вот должен быть из зала в дальнейшем. То есть как так происходит, что 2 гигабита превращаются в 2 Мбита? Вот. Ну и производительность железа. Значит, мы на неё, э, мы сомневались, хватит ли нам производительности. То есть на самом деле вот этот ээ проблема с с нашей с нашим постгасом, нашим нагрузкой и Линуксом, она довольно странная. То есть мы подре подряжали консалтеров, всё разбирались, тести тестировали, тестировали и не не дотестировали, но нам обошлось, как бы новый Intel оказался быстрей. по памяти, именно по нашей нагрузке нам мы выиграли, да? Ну вот дальше, значит, что нам успешно, что нам повезло, да? То есть вот наш такой эффект с тестирования площадки, он оказался оправданным. И, собственно, и сеть в итоге именно московского центра работала хорошо, и железо долгое время гоняло э работу с с картинками, работу с памятью, всё это, э, успешно работало. А, и более того, ещё такая штука, то есть мы же, когда настроили площадку московскую, она стала вот с теми всеми ухищрениями стала своего э значит исходной площадки, и мы тоже могли с неё, значит, также там читать, выполнять запросы. То есть в режиме свое она отработала, и это было нам было достаточно, чтобы даже отказаться от варианта вернуться назад. Ну и, собственно, наши в целом наша логическая репликация вообще переехала без особых изменений. Вот. И принципиальная схема с Питером тоже сработала. То есть встроившись, ведозменив наш наш колбк на архивкоманде, э добавив туда вот эти вот этого вот ээ Сергея с башем, с параллельным рсинком, э мы в целом, э имели то место, куда мы врезались и не меняя в целом вообще. То есть мы не не запускали стриминг, мы не там делали каких-то новых патчей в Постгрес, но в целом просто через архивкоманду там может поскольно настраиваться, может там любая программа работает, мы смогли пере перевести перевести наш наши базы. Это, конечно, удивительно, что, да, вот в Постгресе такая есть возможность просто взять и сделать вместо одного потока вот несколько для архивирования валов, да. Ну вот это особенности особенности скрипта, который Сергей написал. Вот у нас Ну и Постгреса тоже. Ну Постгреса тоже, да. Постгрес там форкает свою программку. Значит, тут ссылка ещё может быть важна на наш прошлый доклад. Его, значит, ээ и он на прошлом на прошлом Хайлоде Сергей делал доклад. Сергей прав? Да. Да. Его даже вынесли на Хабор. Это лучший доклад был, один из лучших Сергея. Там как раз потроха именно вот этого архивирования, которое работает внутри, ну, не только, но и и тоже касается, которое работает внутри постгреса. Это может быть актуально. Потом я ссылся на ребята из Яндекса, а между центрами работают. Ну, я ссывался на то, что вот Авито большой и как бы он может быть перевезён. Вот что на самом деле мне с опытом изначально как бы с моим опытом почти 10 лет разработки в интернете я так и до сих пор вот и не знал до этого момента, что можно взять такую большую штуку и просто передвинуть и всё заведётся. Оно завелось, ночью выключили, утром включили, всё работало. Так, ну вопросы, на самом деле, мы по-быстрому всё управились. Всем спасибо. А сейчас вопрос А вы знаете, у меня вот на самом деле два вопроса. А первый вопрос: написали ли вы благодарственное письмо Ленарту Потерингу? Ну это как бы и второй вопрос. Ну вот больше по теме, если как бы вот настолько много было проблем систм D, почему вы не использовали какую-то другую систему национализации, там Астарт, например, или ещё что-то? Ну, мы с Сергеем, ну, во-первых, тут смотрите, значит, ну, чисто просто административно мы, ээ, не принимаем такие решения. То есть мы в данном случае работали в рамках вот, ну, данности, да, то есть, ну, там ещё куча других систем и вообще, ну, как бы против ветра, да, ну, всё равно все идут на систему, да, ну, как нельзя. То есть рано или поздно не сейчас, сейчас мы от него откажемся, но через какое-то время просто вообще без него работать ничего не будет. И мы здесь, конечно, эти риски приняли и думаем, ну, вот мы там дока стреляли. Но если вы заметили, мы ПГбаун так и не смогли. То есть он пока сбоку стоит и там запускается с командами не через А какой второй вопрос был? А вот этот вопрос. Ну да, почему они не исправили? Ну потому что всё-таки это стандарт и он как бы как Ну не писали мы письмо никому. Ну какая разница, кому там писать? Вот. Ну тут ещё такая штука. Вот же может мы вы к этому всё так вот осуждали в этом. Как это так быть? Нам как бы вот наш нашу систему, которая вот там, которую мы все понимали, она вся больше перестала существовать, как с этим быть. А поняли, что, ну, похоже, мы как бы такие просто какие-то мы старички, что ли, как бы все люди живут как-то, вообще никто нет них проблем. А у нас вот были, но вот сейчас мы А, спасибо большое за доклад. Следующий вопрос. А систи вот я здесь могу сказать. А вы не пробовали всё-таки в system D описать relло? Там как бы можно отдельно описывать? Нет, там нет такого отдельного релоуда. Это все за все зашиты. Сергей накопал как как бы не Подождите. Релоуд есть, а рестарт нету. И делать релоуд вместо рестарта в своих скриптах. Ну, честно говоря, не знаю. У нас на почему-то нужно было именно рестарт там делать. И релоуд там как бы тоже вроде можно по-моему есть. Рестартта нету. Сейчас мы тамписать можно. А ещё вопрос, э, как бы понятно, что с базой данных, а как вы перетаскивали, ну, как бы там картинки, вот ещё что-то вот вот здесь что, ну, я могу пояснить. Это немножко выходит за рамки, да, этого доклада, но там с картинками, видите, какая штука. То есть вот эта база данных, она как бы почему ещё так интересно, потому что, ну, это транссованный движок такой, да, и надо было переехать с точностью до согласованности базы данных. С картинкой немножко попроще. Мы даже как бы без переезда у нас есть допущение, что иногда что-то может пойти не так и какая-то картинка где-то не покажется у вас. Вот это бывает очень, это очень редкое событие, но вы иногда можете быть таким человеком, который наступит на то, что вы загрузились что-то, у вас оно не загрузилось. Ну как-то, ну это интернет, как-то что-то не загрузилось. Тут как бы мы так, но это нет, нет, это нет. Это, ребята, это всё очень мониторится хорошо. То есть с очень высокой степенью доступности. ээ, всё нормально работает, но в целом там так сказать написано, что есть есть там нету там местами там нет чёткой транзакционности и как бы на это идём. И поэтому программа, которая сохраняет картинку, по сути дела, она сохраняла в два места. Вот. И это довольно надёжно работало. Вот таким образом была сделана как сказать, ну, в неком случае репликация этого контента. Он сразу сохранился это место. А дальше, ну, а как это доставалось? Ну, там просто был какой-то каскад инжинсов, которые смотрели сначала туда, потом туда, и таким образом файл ушёл наверх. Это что касается картинки. Вот, значит, под под другие вещи. То есть там я упомянул, ну, допустим, такие вещи, как эдис. У нас есть понятие кластеро Авита. Это такие объединённые, как бы, это не то, что кластер в терминах какого-то там, ну, это, конечно, что абстрактное. Есть кластер, есть кластер рейдисов, кластер сфинксов. И там вещи такие, там либо там есть подобная репликация на уровне как бы самой самой машины, например, редис, у него тоже есть бинарный лоб, например, можно копировать. Вот есть монго, там вот сложнее было, там выстраивались такие квестера, которые часть вот этих реплик было также в Москве, но нечто подобное. Они тоже прокачивали свои свои свою репликацию. Вот тут есть какая важная особенность. Мы в этот мы в этот переезд физически выключали э площадку. Мы вешали вешали такой хороший м хороший хороший лендинг pageй, где говорили, что мы в процессе переезда. Вот. И поэтому часть вещей в этот даутайм, который мы запланировали, мы смогли, это нам помогло все вот эти вот шероховатости обойти для, ну, как бы для небольших, ээ для небольших, э, каких-то хранилищ. А некоторые некоторые хранилища просто целиком полностью воссоздались за короткий срок на московской площадке. Например, то, что уже было вторично производно от данных, которые лежали в базе данных, например, оно просто перестроилось там, как в обычном, как некотором режиме заменутайма. Его не надо было переносить, например, поисковый индекс, он просто построился и он готов. Вот. То есть здесь ну такие типовые схемы, какие-то тонкости, это можно отдельно поговорить или я дам контакт ребят, кто может пояснить. Вот. То есть ну вот опять же говорю, то есть упорная подраст, потому что, ну, это какая-то такая центральная центральное хранилище. Вот как бы такой некото центральное ядро вот этого большого макро-микросервиса, который вот Авито. Вот. Михаил Сергейвич, спасибо за доклад. Два вопроса. Первый, смотрю, постгрец 9:2. У вас есть ли планы по переезду? Да, конечно. А мы нет, ну не только 9:2. Серёга, да, расскажи, что А, ну у нас ещё был 94 там, да? То есть 92 и 94 переезжали, но мы планируем, да, тут смотрите, вот такой есть тут такая как бы вот такой у вас 92, значит, вы там какие-то Но это же неправильно. Во-первых, смотрите, ну как давайте 92, да? Что такое 92? Это LTS- версия, она поддерживается. У нас последний у нас последний 92. У нас последний у нас 92 там 92 там какой-то 85. Вот. То есть самый последний 92. То есть мы не то, что мы используем старые версии, мы просто используем надёжные версии. Вот, значит, и мы используем довольно консервативную политику обновления, особенно от этих вот. Сейчас у нас будут только две штуки, а у нас сейчас три. Вот мы вот консервативно обновляемся. Вот и когда надо обновляться, да? Вы говорите: \"А чем плохо 92? Что вам не хватает?\" Вот нам, например, э чего-то уже не хватает, да? И мы пошли на 94. То есть одно из зачем надо обновляться, когда вам чего-то не хватает. Если вам всего хватает, то, как правило, админа, если работает, то, ну, не трогает. Ёлки-палки. Вот следующий момент, когда ещё надо обновляться, когда кончилась поддержка. Вот 92 кончится в семнадцатом году или в восемнадцатом году, да, Серёг? Ну, можно посмотреть. Они публикуют, когда в семнадцатом 92 закончится, ну, к тому времени таск, который в жире стоит, скорее всего, будет сделан. Вот, значит, то есть запланировано полностью обновить на 94, а 94 ээ а и потом ещё почему вам нужно обновляться, если есть какие-то новые хорошие фичи. Вот. И вот там уже я сейчас могу точно молчах не сказать, но какие-то фичи нам уже нужны, да, и в том числе во многом пере 94 нам продиктован новыми фичами какими-то там что-то касается, причём, по-моему, как раз репликации, да, Серёг? Ну, Gonщ все хотят. Ну, Jon все хотят, но как бы он у нас это не ЦПУ какая-то критикал вещь. Мы как бы работаем и поштор там, и где-то по Gсону на на текстовых функциях и так далее. Вот. А где-то 94, да? То есть где-то у нас есть у нас есть такой экспериментальный во многом экспериментальный, потому что он как бы для нас такой свежий вещь. Мы с ней сча экспериментируем, потом выкатили её. Мы используем синхронную репликацию в некоторой инсталляции. У нас есть инсталляция, где синхронная репликация для того, чтобы не потерять ни одной транзакции даже на момент recovery. И вот там используется синхронные репликации, конечно, там используется 94 какой-то последний тоже. Вот. То есть мы используем 94 в бою. Вот по поводу апдейта вот очень хорошее вот консервативный этот апдейт нас несколько раз спасал, да? То есть мы пропустили версию 91, мы пропустили версию 93 и до сих пор, значит, содрогаемся, когда в 94 до сих пор дофикшивают там какие-то эти мультикзак там какие-то штуки, что они там наломали. Недавно была новая новый новый баг был с free space map. Да, это с 93, по-моему, до 96. Мы как раз с ними с ними с этим багом столкнулись вот в 94, когда переезжали. такая хорошая 92 штука, что мы как бы с неё не это, то есть мы в своё время в Авито был на доисторические времена, там на 83, на 84, на 9:0. На 9:0 такой баг был конкретный, когда мы, когда севолок похерился, что после этого 9:0 запустился дальше и решили всё, вообще не будем обновляться. Значит, 91 пропустили, там были какие-то ещё более страшные баги. 91, я уже сейчас не помню. Вот, значит, потом 9:2 вышел, тоже там вот 9:2 какой-то там большой, там уже 926, мы на него переехали и вот сих пор на 92 сидим, всё хорошо работает. Вот и новых фич как-то нам э не особо пока там надо было. А, ну и по, ну, они как бы все дыры латают, апдейтят также баги какие-то 92. Вот. А а 93 пошли пошли революционные изменения. Они, смотрите, что сделали. Они сделали, э, fork сделали, сделали блокировки форенке, тем самым сделали очередь на мультикзакты. Вот, значит, ну, это прямо вообще там такие революционные вещи, ещё какие-то были революционные вещи, и мы как бы с Серёгой так их видели их и так как бы откладывали кому-то дальше. Его дождались сейчас уже по по сути дела вот 94, 95, 96 - это в некотором роде то же самое 94. Собственно, поэтому там ребята из IT или 10 сделать, потому что как-то надо людей пушать, чтобы они дальше дальше самохвалов об этом говорю. Дадада. А мне это не нравится, конечно. То есть, ну, что это такое? Каким-то маркетингом 10.0 там как-то что-то все скрипты сломаются. О'кей, спасибо. Ну и второй вопрос. Почему всё-таки 2 мбита-то? А, 2 Мбита, потому что, короче, мы сначала бешено грешили на наших провайдеров и думали, что за такой нам quity of сервис, короче, что вообще оны делают. А их же ни зачаться там, ну, как там, какая служба поддержки там, ну, пишешь им там, у нас там сеть не работает, ну, ништяк, ребята, не работает у вас сеть. Чёт. Ну, короче, мы писали нашим семстратам, говорим: \"Ребята, вот, ну, смотрите, мы видим два ме два два этих 2 мбита заместо 2 гигабит\". Они говорят: \"Ну, давайте смотреть\". Ну, что-то смотрят, смотрят, ничего не до насмотрели. В общем, в итоге мы переехали и так и не знали, в чём проблема. И мы думали, что это, значит, у нас нам нам мы думали, что у нас это нам так провайдер интернета какой-то транзитный. Думали, да? Да, мы думаем, что где-то на транзитном уровне что-то нам как-то выправляют, что мы в целом могли утилизировать, но какой-то конкретный поток не забивал всё. Вот. Но потом, э, мы с этим докладом выступали ещё на ПС-конференции, и к нам подошёл как раз человек из Яндекса. Мы с ним, на более детально разбираться, в чём проблема, потому что они между своимицентрами не видят такой проблемы, что по интернету только низкий трафик. И потом уже через сколько там год, ну, чуть меньше после переезда я пишу одному из главных системных администратов, говорю там, э, мой товарищ, что что у нас было-то? Он говорит: \"Да, слушай, говорит, мы, говорит, дополнительно шифровали у себя на своей стороне. дополнительно шифровали и шифровальная программа больше чем 2 Мбита не не не не не выдавала. То есть у нас ещё был дополнительный внутри маршрут для шифрования, и мы, собственно, упирались. Вот. Но вот тогда просто в таком, ну, как бы понимаете, всё довольно так на бегу переезжается. То есть вот мы там говорили, мы там 6 месяцев переезжали, мы 6 месяцев не только переезжали, да, к как бы к счастью, к сожалению, собственно, вот мы там вот я тут не отметил положительные вещи, которые у нас были. Вот это надо было отметить. То то, что мы закладывали, что мы в два раза по трафику вырастем, это было очень правильно, потому что мы так и выросли в два раза. И наших способностей хватило там тоже в обрез буквально. Вот потому что мы мы росли и вот это да. И то есть мы параллельно переезжали. Ребята, которые дефопся, они не только занимались этим внешним каналом, они занимались и текущей поддержкой, и развитием новых сервисов. И тогда, да, реально, как бы никто не знал. Ну, ребята, ну вы же программисты, разберитесь, ну что, мы крутые дбашники, ну, мы сделаем хоть 2 мегабита, хоть один, какая разница. В итоге всё сделали. Вот. Ну всё, Сергей молодец, всё сделал. Ночью все, все не спали. Вот там и систем д нам подваливал, и ещё что-то. Вот. Ребят, вопрос такой. Вот. Вот я здесь. А скажите, пожалуйста, вы упомянули монгу у себя? Да. Скажите, а большой трафик у вас к монге? А значит, ну тут смотрите, когда мне говорят про трафик, я всегда тоже тут как бы зависит же от смотрите, вот если график вернуться, Серёг, мотни не между монгами, а, ну, я понимаю. Сейчас я вот смотрите, вот смотрите, вот этот график, вот, вот смотрите, вот сколько там трафика снаружи, да, ну как бы, ну, вообще говоря не очень интересно, если вот в итоге бкэнд вот так утилизирован, да, то есть смотрите. Ну да, значит, то есть вот, допустим, вот эта инсталляция, я просто хочу качестве примера хочу подвести к рассуждению, которое я сейчас вам расскажу. То есть вот эта инсталляция на CPU bound. И, собственно, вообще не очень интересно, сколько там сверху трафика идёт на одну из, ну, как бы вот это вот то же самое и для монги, да, если мы берём какую-то одну ноду, если мы весь трафик монги, а мы его делим на конкретные ноды, вот, то, в принципе, если эта нода справляется, то, собственно, для вцелом для кластера, ну, как бы, ну, какой-то большой трафик идёт. То есть надо мерить в рамках какой-то конкретной инсталляции. У нас монго работает как вот этот наш монго кластер прило основной один из основных его нагрузок, которые большие. В него идёт такой большой поток логов в CET collection. Ну ты, наверное, спрашиваешь про Монгу, ты знаешь, что это такое? Речь была контекст такой, что можно ли от клиента или службы сервиса какой-то, да, до монги сжать трафик? А не знаю, честно говоря. Вот поэтому и спрашивал. большой. А не, ну опять же смотри, то есть ограничение большое. Смотри, если у тебя не справляется вот эта штука, да, то мы тогда вот наше приложение так написано, что вот эта монго, она обрабатывает какой-то некоторый набор логов. Логов, их много, очень много логов. Вот какой-то конкретный набор мы мы ставим на этот как бы на эту инсталляцию монги. Вот если у тебя дальше не прокачивается, да, и ты не можешь контролировать, то ты просто меньший набор, либо уже начинаешь горизонтально этот набор логов пилить. Вот. То есть мы как бы мы можем вот сколько у тебя там пропускать способность, столько как бы максимально выжирать, да? То есть такая архитектура квастера мон, что приложение конкретный лог, кокретный тип лога пишет в конкретную инсталляцию мончитает. А а читает ну читают как бы вот в данном случае монго используется как буфер, читают его другие потребители. Это уже не ОТП нагрузка. То есть ЛТП в неё как-то там туда через систему буферов туда пишет. Те, кто читает, из неё трафик большой между А ну он тоже, видишь, он такой, что на него как бы 100 мбит, гигабит. Ну это да, это это в пределах сетевой карты там, то есть это не и решаете просто увеличением. Ну опять же, видишь, ты ты можешь, если тебе не хватает этого сивого трафика, ты делаешь их ещё на говорю, наша архитектура может ты можешь меньшую часть данных описать, это не неважно. То есть повод до того, что ты можешь какой-то тип лога, а там чётные события лого писать на одну ногу, что-то нечётные на другую. Итоговый потребитель параллельно потребляет эти эти буфера, которые уже капет collection монги, и перекладывает их, ну, например, в дальнейшие буферы на ДВХ систему. Спасибо. Вот, то есть, то есть и опять же про трафик надо всегда рассматривать с точки зрения, а когда вы упираетесь, да? Вот если если я никуда не упираюсь, то я даже не знаю, сколько там запросов у меня. Но это не самое не самые. Я упомянул, что очень важная у нас система - это PG баунсер. И самый наши главный, конечно, график - это не CPU на бэкэнде. А количество ожидающих коннектов на ПГ баунсере, если там всё хорошо, то как бы, ну, не особо даже никто не смотрит, какой там трафик сверху, он большой, маленький, там кши как-то работает. А версия погбаун. Ну какая-то одна из последних последняя 17 какая-то там там были какие-то хорошие фичи, я не помню сейчас недавно права доступа до пулов. Да, у нас погбаун используется как он один из элементов очень центральных в разграничении прав доступа. Пока далеко от монги не ушли. Насколько я знаю, работал с мангой. Там очень легко и просто можно переезжать любыми репликасетами, любыми шардами, куда угодно, как угодно. Почему это не было сделано так? Почему? Так, так и было сделано в итоге. То есть всё плавно переезжало по по очереди. Все шаги вон Рома. Рома можешь помочь, да? Вот это монго человек тут. Рома, пожалуйста, там не всё так просто. Я просто сам недавно буквально переезжал из одного датацентра в другой и перевозил мангу. Нече, нечек. А микрофон, да? О, там мо, да, у вас у меня стоит, значит, соответственно, четыре сервера на на четырёх серверах там и реплика сет, и конфиги, и мон стоят. перезагрузить. Всё это, соответственно, стоит в двух дата-центрах. Половина в одном дата-центре, половина в другом дата-центре. И вот сейчас я один дата-центр перевозил и всё очень так спокойненько. И в течение времени перевозил плавно, без каких-то, если у вас не упирается трафик в диск и в сеть, то, конечно, всё переедет. Но вот про вот этот чудесную балансировку монги между нодами в монго, оно всё тоже прекрасно работает, когда кластер не нагружен. Собственно, там же вот этот вот внутри монги есть ээ балансик, не балансик, чанки, да, чанки. Правила, по которым где они лежат. Да, вот это правило может в один прекрасный момент, когда у вас кластеры нагружены, устроить решардинг, потому что вот эта нода перебалансировалась, ей надо вот эти чанки перенести туда, и в процессе вот этого речанга, в пике трафика у вас так пумс и нечаянно не хватает ресурсов либо сетевых, либо железа. Ну ли либо по памяти. Ну короче, в процессе переезда в из Швеции в Москву несколько раз мы слышали, а короче монга всё зрения как бы монк и переездов, там тоже было всё достаточно просто. У нас как бы и сети хватало, и дисков хватало, и оно реплика с этом с отставанием всё прекрасно катится, но была одна особенность. То есть у т коллекции реплика сета, они же как сделаны. То есть вся репликация в Монге, она построена тоже вокруг коллекции. IT коллекция - это вот если у вас, э, скорость между датацентрами и размер базы позволяет, ээ, в до цикла оплога этого в Capitко успеть прокачать всё, прокачать всё, да, то оно накатится, потом будет догонять. Если цикл успевает проходить быстрее, чем придут все данные, которые у вас были, то, ээ, реплика как бы не заведётся. То есть тык, не успел, опоздал. Он там ошибочку напишет, что всё, всё, не взлетело. Собственно, тут от трафика зависит. Если у вас стоит, например, на гигабт и ну а вы не успеваете гигабай прокачать, дагибай, да, там и и и запись такая, что вы там больше гигабайта в какое-то время, которое по сети надо 100 Гб прокачать, убираетесь, то тогда не взлетит. Ну, о'кей. Одна реплика не взлетела, там две других работают вперёд. От трафика зависит. Да. У нас тоже не не все взлетели с первого раза, и нам пришлось на старом датацентре не порезать даже, а размера оплога увеличить в тех, которые не успевали. То есть там порядка в одном случае до, по-моему, до 100 ГБ. То есть там примерно по 250-300 ГБ в день в одну монгу прилетало событий. Вот, собственно, они не успевали прокачаться и вот этот круг проходил. А просто, допустим, выкидывать реплику и добавлять новую. А какая разница? Новую добавляете, она с мастера начинает читать по сути сnпшот и запоминает облоге ээ позицию, с которой как бы операции надо потом накатить. Вот пока снапшот катится, блок круг пройдёт и его позицию, от которую он начал, затрёт и он дальше не сможет операций накатить. Вот им надо прорекламировать Авитоти Tтич. Вот там мы как бы тек Авитотек. Там, значит, можно вопросы задавать и по монгу, и по редис, и по всем, как бы, всем, всем, всем, всем, всем, всем, всем, всем. Это Twitter, знаешь, такая штука есть. Пока заблокирова, я вообще компьютеров боюсь, там много кнопок. Так, ну, ещё вопросы. Монга немножко в стороне всё-таки ещё по вот по этой по нашей тематике есть что-то ещё? Нет, нету. Время подходит к концу. Всем спасибо. Тоже вот время подходит к концу. Давайте нам команду. Угу."
}