{
  "video_id": "y8diUsNPc6w",
  "channel": "HighLoadChannel",
  "title": "Строим отказоустойчивую инфраструктуру приложения в Kubernetes / Олег Вознесенский (Газпромбанк)",
  "views": 1096,
  "duration": 3038,
  "published": "2023-10-06T07:20:54-07:00",
  "text": "Здравствуйте друзья рад всех вас видеть Я Олег Вознесенский и я руководитель разработки в отделе Газпромбанка который занимается эксплуатацией email и Big Data решений мой Газпромбанк это очень крупная организация с огромным it которая сейчас переживает цифровую трансформацию У нас очень много амбиций и очень много сложных интересных задач проблемы с кликером проблемы с кликером Да мой доклад посвящен паттернам отказоустойчивости приложений в губернаторс То есть некоторое количество типовых решений которые мы можем реализовать у себя чтобы увеличить отказоустойчивость нашего решения и прежде всего я думаю нужно стоит Прежде всего я думаю стоит разобраться что такое отказоустойчивость отказоустойчивость это способность системы сохранять свою работоспособность После отказа одной или нескольких частей Да соответственно отказоустойчивого архитектура это способ построения отказоустойчивых систем которые сохраняют работоспособность возможность понижением эффективности при отказах компонентов Здесь ключевое про отказ про понижение эффективности то есть лучше плохо работать Чем хорошо лежать и отказоустойчивость мы можем делать на нескольких уровнях а базовый уровень это уровень аппаратного обеспечения Да мы можем вести избыточность и обмазать ее некоторым количеством алгоритмов позволяющих нивелировать проблему Ну например рейд или память с контролем чётности далее мы можем делать устойчивость на системном уровне операционной системы и связанных с ней сервисов связанных с ней сервисов думаю Многие слышали про herbit Pace Maker Да и в частности губернации очень много возможностей для построения отказоустойчивости на и мы можем писать отказоустойчивый код Ну вот допустим методология 12 факторного приложения в ней Ну по крайней мере два фактора напрямую повышая связанность с повышением отказа устойчивости на самом деле мой доклад он был вдохновлен докладом другого человека а Александр кривощеков на metape c+ систем очень хорошо рассказал про паттерны отказоустойчивости и выдал на горам Некоторое количество кода демонстрирующего их реализацию вот я оставил ссылочку на Его доклад ознакомьтесь и вообще по докладу я буду раскладывать ссылочки все кто заинтересован посмотреть поглубже А в на те вещи о которых я рассказываю могут скачать презентацию и уже спокойно пройтись по ссылочкам ссылка на презентацию будет на последнем слайде так вот Александр кривощёвков выдал Некоторое количество кода демонстрирующего реализацию паттернов отказоустойчивости а мой же доклад будет про то каким образом сделать отказоустойчивость не написав не строчки кода Но если не считать инфраструктуру как кот кодом на Привет Я удивилопером и так что не сделаем Окей значит вот список паттернов открытости про которое я хотел бы рассказать Мы сейчас их разберем все и начнем мы с Watch Dog и Health Check Да допустим я прошу прощения что смешиваю Watch Dogs Check Она в моем рассказе Это примерно близкие вещи и так Допустим у нас есть пара серверов есть клиент который к ним обращается на серверах крутятся сервисы сервера абсолютно равнозначны То есть клиент может обращаться к любому из серверов сервис Давайте да спасибо сервис один опубликован То есть именно к нему обращается клиент сервис 2 связан сервис один находится внутри сервиса и служит для обеспечения работы сервис 1 но я думаю такая классическая архитектура Да и допустим одному из сервисов стало плохо он начал выдавать какие-то ошибки поэтому Ну примерно 50 процентов запроса нашего клиента фелица Каким образом мы можем это исправить мы можем добавить дополнительную сущность которая называется Watch Dog он делает проверки наших сервисов И когда один из сервисов начинают сбоить он это регистрирует и делает управляющие воздействие на сервер Ну допустим перезапускает сервис и все становится хорошо следующий паттерн Health Check в нашу инфраструктуру добавляется Но это умный лот балансер он самостоятельно проверяет наш сервисы и когда все нормально маршрутизировать на них запросы от клиента но когда случаются проблемы на лот балансер это регистрирует и исключает из балансировки этот сервис на этот сервер нам Вы наверное уже догадались про что я говорю я говорю про губернатор спроус то есть Watch Dog это у нас Lines Pro Health Check это ridenis пробы таким образом вот это вот наша схема для примера превращается в следующее вместо балансера у нас губернатор сервис вместо серверов-поды сервисы Наши обёрнуты в контейнеры и все э проверки все управляющие воздействия делает у нас кьюблет а вероятность пробы это ну как бы часть гораздо большего механизма который называется кубернация Spot Life сайкл и прежде Первое что я хочу сказать про пробы это то что они независимые то есть очень многие люди допускают вот это вот ошибку на Они думают что сперва проходит реле нас проба потом проходит рейденец проба под становится доступным и все становится хорошо это неверно и это приводит к неправильному выставлению параметров проверки да то есть иногда случаются вот такие вот ситуации когда риггинец проба у нас прошла Вот на трафик пошёл Потом решила что не всё в порядке по перезапустился и мы потеряли Некоторое количество запросов которые уже начали выполняться поэтому помним что Ларина серией с пробы это независимая пробы и выставляем их настройки правильно вот это вот ошибка Про которую я говорю она сейчас встречается реже потому что появилось так называемая стартап проба стартапробы действительно отсрочивает выполнение Live на середине из проб и она служит для проверки приложений которые долго инициализируется что это может быть Ну например у вас в контейнере javas включенная жидкомпиляцией это когда виртуальная машина Java получает программный код на вход а перемалывает его мелкого бинарный код попутно оптимизируя таким образом в итоге наш код начинает работать быстрее чем безжиг компиляции но она занимает время и достаточно большое часто Кроме того джависты часто делают мигрейшен а при инициализации контейнера - это очень плохая но к сожалению очень часто проба и для того чтобы справиться с ситуацией у нас существует старта проба Что делают наши пробы стартапы онлайн-пробы делают рестарт контейнера и с этим нужно быть как можно более аккуратным то есть допустим к нам пришла высокая нагрузка Да и нашей инстанции наши поды начали медленно отвечать лавинность проба не получает ответа и перезагружает под как раз тогда когда это неуместнее всего то есть перезагружать под и когда точно понимаете что делаете ридниц проба она управляет трафиком она изменяет DNS записи и en Point для губернатосервис и она должна быть настроена всегда когда есть такая возможность для настройки проб У нас есть некоторое количество возможностей Некоторое количество проверок ну прежде всего это экзек проверка это возможность запустить какую-то команду в контейнере Ну например прогнать запросы с командной строки чтобы понять что база данных которая находится в контейнере работает хорошо и с этим нужно быть осторожнее потому что exe-проба она часто порождает Зомби процессы если используете язык пробуем настройки мониторинг Зомби далее У нас есть grpc http пробы на любой вкус и цвет для того чтобы сделать хорошую пробу Какая должна быть хорошая проба прежде всего ваша проба должна учитывать бизнес функционал приложением то есть а если допустим у вас админка в приложении API Вы должны проверять именно API то что отдается пользователям Да причем Вы должны проверять механизмы которые непосредственно участвуют в обработке запросов пользовательских то есть не просто отдать какой-нибудь а сети с докер имиджа Да а прогнать скрипт близкий к тому что делают пользователи при этом нужно быть осторожнее со внешними зависимостями Потому что если внешние сервисы откажет Вы можете получить Каскадный сбой и наша проба должна учитывать особенности приложения о чем я говорю вот наша схема допустим в наших контейнерах крутится классическая связка Джинкс И phpm какая может быть хорошая речь проба для хорошей реденье из пробы нам не нужно проверять оба - это оба этих контейнера По отдельности мы можем проверять на джинс А локейшн который проектирует phpm таким образом за один запрос проверить оба контейнера и оптимизировать ринис пробу на сделать сквозную проверку далее пешки в Мы в нем настраиваем пулы так называемые то есть группы воркеров которые обрабатывают наш запрос и в них мы настраиваем максимальное количество доступных воркеров и если все вот эти вот воркеры будут заняты высокой нагрузкой ладно спроба может не получить вовремя ответ и перезапустить пот тогда когда это будет неуместнее всего поэтому для Lives пробы для хорошей Lines пробы мы делаем отдельный пул с ограниченными ресурсами И направляем Лабинск пробу именно туда окей с этим разобрались Давайте поговорим про ретро и тайм-ауты также известный как дедлайны я опять же прошу прощения что их смешиваю но это как бы достаточно близкие понятия и так вот наша инфраструктура один из контейнеров испытывает сложности клиент делает запрос и к сожалению попадает именно на этот контейнер рода могут быть проблемы это могут быть ошибки протоко ну скажем 500 это могут быть проблемы сетевого соединения это может быть просто долгий Ответ когда под подвис и не отвечает Поэтому нам бы хотелось чтобы клиент понял что что-то пошло не так он не стал ждать до упора или получил ошибку и повторил свой запрос потому что ну наше приложение написано по twellfactor Up Клауд на этих Да у нас скорее всего несколько экземпляров запущена нашего приложения поэтому Вполне вероятно что следующий экземпляр может нормально обработать запрос от нашего клиента в именно вот этот паттерн как раз ретрейт таймаут и в экосистеме губернатор мы можем реализовать это при помощи ингресс и сервис в случае с ингресс в нашу инфраструктуру добавляется так называемый ингрисконтроллер фактически прокси интегрированный с губернацией API который становится на границе нашего кластера и принимает трафик от клиента дальнейшее объяснение я буду вести на примере Ingress Джинкс с одного из самых популярных ингр с контроллеров и для настройки ретраев и тайм-аутов Существуют следующие настройки для ретраев мы можем настроить прокси-некста обстрим это условия повтора условия ретро прокси-некс обстрим тайм-аут общее время в течение которого мы можем делать ретрая и прокси общее число ретраев которые мы можем сделать отдельно хочу остановиться на условии повтора в условии повтора мы можем указать Некоторое количество случаев ошибок при которых мы хотим повторять запрос и ну как бы это позволяет гибко подстроить под нашу под наш случай для настроек тайм-аутов У нас существует возможность выставить максимальное Время ожидания коннекта максимальное Время ожидания чтения и записи в наш сервис для настройки ретрит тайм-аутов У нас есть несколько возможностей первая возможность мы можем сделать их в глобальный конфиг Map на весь контроллер которая находится в наймспейсе контроллера тогда она будет влиять на все ингрессы которые мы настраиваем далее мы можем выставить их аннотациями в конкретном агрессии который нам нужен и следующий момент вот эти вот настройки это некоторая абстракция над возможностями самого GX поэтому мы можем поправить конфиг джинсы логично при помощи функционала Джинкс configuration сниппет Да но это как бы достаточно низкоуровневая вещь поэтому я не рекомендовал бы с ней слишком сильно увлекаться что мы должны помнить когда мы настраиваем тайм-аут и ретро и на Ingress Джинкс прежде всего по умолчанию Ingress Джинкс retride только и демпатентные запросы То есть те запросы которые мы можем прогнать по нашей инфраструктуре любое количество раз то есть вот эти вот глаголы они должны на их реализация должна быть импотентно по спецификации для ретраев не депатентных запросов мы должны выставить отдельный ключ Что такое допустим у нас интернет-магазин и мы делаем заказ заведение заказа через пост запрос мы послали его в один получили там тайм-аут отвалились в другой инстанции там тоже тайм-аут обработал нормально Потом мы смотрим в базу и видим там три одинаковых заказа что случилось на самом деле наши инстансы они приняли А вот этот запрос и записали его в базу они просто протупились ответом и Ну вот эта вот ситуацию наверное нельзя назвать ситуацией для преодоления вот этой вот проблемы существует так называемые ключи когда мы в запросах конкретно встраиваем хешек который Уникальны его идентифицирует то есть мы не говорим Заведи пожалуйста заказ мы говорим Заведи пожалуйста этот заказ и мы можем его завести только один раз но к сожалению ключи импотентности Мы можем реализовать только на уровне программного кода поэтому прежде чем разрешать запросов сходите к разработчиками уточните что их реализации импотентно и спросить еще за недепатентное запросы потому что ну кто знает что они там накрутили далее ретро и тайм-ауты они увеличивают нагрузку на нашу инфраструктуру Поэтому нам нужно делать разумную политику ретрит тайм-аутов что я имею в виду прокси со Стрим тайм-аут это общее время в течение которого мы можем делать ретрое то есть допустим клиенты вашего сервиса не готовы ждать больше двух секунд нам обработку своего запроса Поэтому вот этого значения не стоит выставлять значения больше прокси общее количество ретраев допустим у вас опять экземпляров ваших инстансов опять кодов и Ну соответственно общее ретровить больше наверное смысла нет Потому что если уж никто не ответил То не стоит усугублять а по поводу тайм-аутов есть хорошая практика когда конкретный тайм-аут мы выставляем в 99 presentill соответствующей метрики то есть ставим мониторинг а считаем значение в которое попадает 99 процентов действий на вашей системе и Именно его выставляем таким образом вот эти вот одним процентом мы отсекаем случай когда действительно что-то пошло не так и последний момент когда мы настраиваем сервис для контроля rangress сделаем кластер райпин Он ему совершенно не нужен на айпишник как бы отдельный хоп маршрутизации трафика далее тайм-ауты на сервис mash Я буду рассказывать на примере одном из самых популярных контроллеров часто наши клиенты находятся внутри кластера и у нас совершенно нет желания там ставить ингресс и каким-то образом публиковать их или вот как-то пропускать для этого служит сервис Мы можем поставить кластер сервис которая состоит который делает примерно следующее есть Control plain и этот Control plain собирает информацию со всего кластера о состоянии поводов о состоянии сервисов плюс получает Некоторое количество дополнительных конфигов специализирован для сервис-смешек которые мы можем подложить далее Control Plane агрегирует эти настройки и рассылает по так называемым sate Car Proxy контейнером которая находится в каждом поди который попадает туда Ну чаще всего автоматически через Motion webhook кстати минутка занимательных фактов словом сайт Car обозначается коляска мотоцикла я вот не знал до того как начал делать доклад Итак сервис наш разложила конфиг по нашим сайт carproxy контейнером клиентское приложение хочет сделать запрос к сервису но сайт carproxy перехватывает этот запросик Да там есть Донат и пытается разрешить его сама то есть всё для приложения конкретно происходит незаметно совершенно прокси посылает запрос в неисправный подик регистрирует ошибку или тайм-аут далее становится хорошо ну как бы успех мы можем настроить в кастомном ресурсе который называется Виртуал сервис и прежде всего Первое что я хочу сказать истину по умолчанию retride все То есть если вы не хотите ретровить запросы вам нужно написать дополнительный фильтр под это а-а собственно настройки самих ретраев мы можем настроить Количество попыток мы можем настроить тайм-аут на ретрай то есть в отличие от Инка с контроллера когда мы сдаём общее время здесь именно тайм-аут на попытку и также Как выиграть контроллере мы можем настроить условия ретрая Кроме того в истео есть настройки тайм-аутов но в случае если на ретраях достроена перетра и тайм-аут это настройка тайм-аут поэтому тайм-аут стоит использовать когда retrai не настроены Идем дальше Вот наш принципиальная схема и если Вы заметили то связи контейнерами Они двунаправленные то есть сервис может получать какую-то информацию с нашими прокси Каким образом мы можем это использовать вот как бы Идет работа а-а запросы на одном из кодов начинают фелиться сервис - это регистрирует и исключает его на время из маршрутизации трафика Ну то есть допустим пришел какой-то там тяжелый запрос в наш плодик ему нужно просто постоять его переварить Ну или за это время придет Watch Dog перезапустит наши контейнер и все станет хорошо Да ну то есть вот это вот паттерн оставить в покое на время чтобы все восстановилось Да он как бы есть оркетбрейкер он неплохо работает причем Ну часто даже с людьми и как бы особенность все бреке заключается в том что он предполагает только пассивные проверки активных проверок нет то есть для того чтобы понять что наша инстанс восстановился мы опять пускаем на него трафик опять видим что все фелится опять убираем его потом через какое-то время опять пускаем всё восстанавливается Мы возвращаемся к нормальной работе в истио стёрты брейкер настраивается при помощи э кастом ресурса destinational здесь мы можем указать общее количество ошибок после которого мы э исключаем под из маршрутизации трафика Причём здесь считается как ошиб такого так и тайм-ауты и тайм-ауты мы можем настроить Как в destinational Так считаются в тайм-ауты которые настроены ну скажем на Ретро вертел сервис далее мы можем указать период за который мы считаем ошибки то есть вот в данном конкретном случае после семи ошибок за последние 10 секунд под исключается из маршрутизации трафика Basic jaction Time время на которое мы исключаем под из маршрутизации Макс и Джексон персент это максимальное количество кодов нашего сервиса которое мы можем отключить от трафика и мин хелсперсент это минимальное число здоровых razy кодов после которых начинает работать то есть брейкер он не влияет на статус контейнера Да рейтинг Но может учитывать его в своей работе Окей рейд лимиты вот схема на которой мы рассматриваем пример и она Ну наверное немного не точно неправильная клиентов как правило у нас много и часто эти клиенты могут делать большой трафик причем это могут быть как локальные какие-то дневные пики Да когда все пришли на работу и пошли на наш сервис это могут быть какие-то злоумышленники которые пытаются положить наш сервис с большим количеством запросов это может быть просто ошибка программистов которые там забыли обнулить переменную в клиенте и клиент начал вести себя неприлично В общем через какое-то количество запросов Да через какой-то размер э-э трафика наше приложение обязательно ляжет и нам бы хотелось отсечь то количество запросов которые мы не можем Вам работать и пускать на Back and только те запросы которые мы можем обработать гарантированно это есть паттерн рейд лимит и как Мы помним из определения отказоустойчивость инфраструктуры Да ухудшение качества сервиса для нас В некоторых случаях приемлемые чем простой вот рейд лимиты мы можем настроить в губернатор с ингрессом Джинкс Да в нашу инфраструктуру добавляется ингресс который встает между клиентом и сервисами и под принимает нагрузку на себя но часто количество экземпляров контроллера у нас большое у нас больше одного Да и мы должны это помнить когда настраиваем рейд лимиты в ingressings есть два типа рейд лимитов это локальный рейд лимиты и глобальный рейд лимиты локальный рейтинг они действуют в пределах одного экземпляра и для того чтобы выставить мы можем через аннотации сделать специальные настройки Давайте их разберем прежде всего мы можем настроить сами лимиты лимит на соединение лимит на количество запросов в секунду и лимит на количество запросов в минуту причем они срабатывают именно в таком порядке на соединение rpm rps далее часто наши игры с контроллеры в свою очередь находятся залог это может быть IP основанный Облачный LED balancer Ну например Amazon elastic lot balancer или это haproxy в openstek который mail.ru называют своим облаком далее У нас может быть http основанный блок balancer и для того чтобы мы могли применять лимиты к клиентам мы должны знать их адрес в случае балансера мы должны настроить прокси протокол то есть указать протокол В глобальном В глобальном конфиг Мы ингерсколера в случае с http основанным балансером мы должны согласовать хитры в которых будем передавать адрес клиента по умолчанию но мы можем переопределить это поведение при помощи специального ключа далее мы можем настроить так называемый бёрст лимитов Что такое бёрст лимитов часто запросов клиентов приходят такими большими пиками вот как на первой странице на первой картинке на левой и отбросив их мы можем Ну во-первых не догрузить нашу инфраструктуру а во-вторых сделать клиен довольным несчастным Смысл оберт limit в том что мы берем так сказать в долг Некоторое количество некоторое Разумное количество запросов и пытаемся обработать их позже в рамках существующих то есть ну они как бы обрабатываются чуть медленнее да чуть больше времени но обрабатываются то есть в данном конкретном случае ухудшение качества сервиса для нас оправдано это смысл лимит далее мы можем настроить лимитрейты то есть ширину полосы пропускания для каждого отдельного соединения при помощи но они работают только если у нас выставлены и настроены включенные настроены прокси-буфера и Кроме того мы можем задать лист IP адресовка которым никогда не будем применять лимиты но настраивая локальные мы должны помнить что с изменением количества экземпляров индекс через которое Идет трафик мы меня не только полосу пропускания мы изменяем и лимиты и для того чтобы это преодолеть Да у нас существует глобальный рейд лимиты в индекс они реализованы при помощи библиотеки луаресле Global trotal и сервисы ммкэш для того чтобы начать с ними работать Мы должны настроить моем кэш указать его параметры в глобальному конфиг ингрисконтроллера и настроить аннотациями параметры глобальных лимитов в этих параметрах мы можем настроить количество запросов и временной период за которых за которые мы готовы их принять Но кроме того мы можем настроить whitelist далее Rate limite и на сервис Machine настраивая рейд лимиты на сервис мышея мы должны помнить что прокси которая принимает решение о наложении она может находиться в любом поди на сервере на клиенте везде в любой щели и первая возможность для настройки рейд лимитов в estio это секция Connection Pull в Destination ru разработчики истио считают это частью серкан брейкера но я думаю это не совсем корректно в рамках лимитов на Connection Pool мы можем настроить лимиты на количество активных tcp соединений и для http максимальное количество запросов в обработке и максимальное количество запросов в ожидании Ну то есть такой аналог berlimite Но это правило оно применяется на клиенте и оно локальное внутри конкретного сайта прокси то есть настроив лимит в 100 соединений при двух клиентах на приложение мы получаем 200 запросов соответственно Окей в истио Мы также можем настроить так называемое envoy Rate limity они также бывают локальные и глобальные и для их настройки используется nvid-фильтр на мой взгляд это функционал который очень похож на configuration сниппет в ingress-контроллере то есть разработчики из него не стали заморачиваться созданием каких-то абстракций для рейд лимитов они просто отдали нам конфиг нвоя и сказали вот пожалуйста и документацию по этим ряду там наверное лучше искать в документации а что мы должны помнить когда настраиваем рейд лимиты они могут быть для tcp и для http на источники на назначение трафика для входящих и для исходящих пакета соединений запросов я не буду наверное перегружать какими-то конфигами сложными просто рассмотрим возможности локальная рейд лимиты они организованы по принципу token bucket То есть у нас есть какой-то багет с определенной емкостью токенов Да эти токены могут быть Ну запросы в обработке есть правило заполнения этого Бакета то есть мы можем указать Какое количество токенов За какое время мы готовы принять Но если наш багет заполнен мы отбрасываем все вот это так работает глобальный для того чтобы начать с ними работать мы должны установить специальный сервис для работы с рейминтами будут ходить по grpc который будет хранить бэкэнд будет хранить информацию о соединениях в редисе то есть ну radiss нужно поставить рядом с глобальным сервисом и для настройки рейд лимитов глобальных нвоя мы можем указывать время и количество запросов которые мы за это время готовы принять вот то есть очень близко к тому что есть в Ingress Джинкс и как бы при настройке глобальных лимитов стоит понимать что это распределённая рейд лимит и вот это вот поход в сервис который держит наш соединение Да он занимает время он увеличивает время А на обработку трафика а-а нашими И это тяжёлый механизм поэтому разработчики что ингресс а-а что они рекомендуют использовать локальные глобальный рейд лимиты вместе чтобы оптимизировать использование ресурсов Вот и 3 возможность настройки рейд лимитов это называемая API getway то есть специализируя прокси которая предназначена для настройки для публикации и контроля за Web API вчера Виктор Попов делал отличный доклад по поводу того нужны ли вам эпигивает и ну как бы мы можем настроить рейд лимиты в эпиги твои Но чисто из радиатов тащить это в свою инфраструктуру это наверное поэтому Посмотрите доклад Виктора он обоснованно расскажет вам как это делать как когда это уместно Да следующий паттернако устойчивости выкаты то есть диплои новые версии нашего сервиса Если вы читали Google serebook то можете знать что Google говорит что примерно 70 процентов ошибок С обработкой пользовательского трафика они случаются когда выкатывается как раз новая версия приложения поэтому отказоустойчивый диплой это Ну примерно 70 процентов успеха если у вас хорошая инфраструктура хорошие процессы 70 процентов успехов достижения отказоустойчивости но у нас лимитированное время Да я наверное так уже перегрузил доклад поэтому Давайте поступим следующим образом вот qr-код для голосования за доклад Проголосуйте пожалуйста если я увижу интерес сообщества к теме открыто устойчивости в частности оказывающих выходов я не обязательно сделаю этот доклад Вот и расскажу его на следующей конференции qr-код сейчас появится опять а пока минутка благодарностей Спасибо команде defreel Газпромбанк за помощь за подготовкой конференции и Адель из команды defreal недавно сделала доклад Каким образом они это делают я оставлю ссылочку посмотрите это интересно и Большое спасибо Андрею павлову со основателю компании flant за постоянные консультации по поводу истила по ссылке доклад Андрея где он анализирует производительность его это ну как бы ну очень мощный очень интересный материал Вот спасибо за то что смотрели слева голосование справа ссылка на презентацию готов отвечать на вопросы Олег прекрасен высок и красив Друзья давайте вопросы зададим пожалуйста вот раз Кто быстрее отлично поехали Спасибо за доклад все-таки вопрос по поводу мониторинга внешних сервисов внешних зависимостей Есть ли какое-то хорошее красивое решение и желательно чтобы оно было внутри cubernatis а не каким-то внешним балансером смотрите внутри губернета сам мониторя нашей нагрузки мы ограничены Да всегда лучше дополнительно мониторить ауксбокс для мониторинга как бы со стороны клиента инфраструктуры которая клиентской инфраструктуре на внутри губернатора У нас есть прекрасный про миссис У нас есть прекрасная экспортеры которые показывают позволяет снимать множество метрикс-контроллера с и мы можем видеть все ошибки мы можем видеть все тайм-аут и время ответов и насчет этого строить наши какие-то действия Спасибо еще вопросы друзья Вот первый ряд вот центр первый ряд второй Олег привет Спасибо большое за доклад Спасибо за алаверды У меня на самом деле первое У меня вопрос и комментарии такой что Господи не Пользуйтесь ниппетами контроллеров я говорил про можно сломать буквально все там невероятная куча проблем просто запрещайте Вопрос такой кто это все делать должен потому что вот всё что ты описала все очень правильно и здорово но не очень понятно Чья это зона ответственности То есть это инфраструктурные какие-то требования хардкорные это разработчик Ну то есть те же не знаю либо среднесроба они требуют поддержки со стороны кода у тебя банально должен быть Ну там Слышь свой шкафунт Поинт в идеале например вот кто это делает у вас И кто это должен делать на твой взгляд это должен делать мой рассказ был посвящён как раз возможностям которые могут использовать в постоянной работе и вот эти вот поддержка со стороны Да да я покривил душой что ее может не быть но именно devops должен идти к разработчикам и продавливать дайте мне хороший inpoint для line с пробы причем ребята Это должен быть анкоинт до который запускает скриптик легонький Желательно без внешних зависимостей Вот это так то есть ну обычно процесс строится итерационно мы выкатились мы мониторим нашу систему видим проблему думаем Каким образом решить эту проблему Да мы её решаем там приносим дополнительные механизмы улучшаем всё это и этот процесс непрерывный Спасибо будьте добры вот Да вы а потом Да Добрый день Хотел еще возможно некоторые вопросы уточнения по поводу того используете ли вы в качестве решения таких проблем скейлеры и всякие стоп хоки обработки сектернов и настройки типа соотношения эквестов к лимитам и разворачивании поводов потому что иногда возникают проблемы Что у вас ищется под на основе эквестов он залетает в лимиты дохнет снова начиная скелет и так далее а сейчас речь идет про селфилинг систем Да это вот как бы мейнстрим обеспечение отказоустойчивости селфилинг приложение они самостоятельно регистрируют проблемы и могут их исправить селфилинг бывает реактивный и превентивный реактивный с алхиминг это ну допустим у нас случилась какая-то проблема отсохла базы данных мы пока пишем в очередь Да когда база данных поднимается мы перекидываем туда все проактивный селфилинг это когда вот описанный случай Мы регистрируем недостаток ресурсов Да мы понимаем что мы уже не можем справляться мы идем в облако заказываем там дополнительные инстанции масштабируемся на них и таким образом перевариваем нашу нагрузку вот Будьте добры Олег спасибо за доклад было очень интересно а такой вопрос есть ли проблемы с масштабированием Ну то есть у вас микросервисная архитектура множество разных команд и у каждой свои сервисы и например одной команды может быть 40 сервисов другой 20 сервисов Они реально легко разбираются как им работать вот совсем с этим как вы их не знаю обучаете этому как вы эту культуру создаете у них ну очень хорошо когда человек когда он интересуется тем что происходит у нас на инфраструктуре какие у нас связи существуют Какие возможности по эксплуатации приложения Да но очень часто бывает так что команды с другими командами общаются только через спецификации API и роль как раз дебопса в том чтобы ну во-первых Connection People дам сводить разных людей чтобы они договаривались во-вторых Ну как бы самостоятельно во всем этом разбираться и если разработчики не вывозят Ну делать это самостоятельно вот 17 ряд пожалуйста Добрый день спасибо за доклад Я бы хотел развить один из предыдущих вопросов про Hells Check Ну вот хорошо пробежал devops в разработку Я здесь посерединке Прибежал его с разработку сказал Дайте мне там слэш Hells and Point разработчик спрашивает что там надо сделать собственно то есть Как должна выглядеть сама проверка Я приведу пример что вопрос был более понятен есть для Джес библиотечка стандартная которая предоставляет какой-то набор стандартных проверок там типа диск достаточно места оперативной память достаточно места базу данных Select единичка сделать что-то такое Существует ли какие-то рекомендации Надо ли проверять внешней зависимости все-таки или не надо если надо то какие не надо я сейчас занимаюсь этим вопросом у тебя в компании и не могу найти наши пробы смысл их в том чтобы понять можем мы обслуживать запрос клиента или не можем соответственно мы должны проверять Вот именно тот функционал который позволяет делать ответы клиенту да можем мы выполнять скриптики А если у нас все завязано скрипты или нет по поводу внешних зависимостей их стоит проверять Если ваше приложение полностью зависит от внешних зависимостей А если внешней зависимости несколько Да и возможность потери одной из внешней зависимости у вас Ну просто там что-то не будет дорисовываться на глобальной страничке вот завязываться на внешнюю зависимость в этом случае не стоит Ну соответственно проверять диск Там или нагрузку на процессор совершенно неправильно да потому что в общем случае это не влияет на то можем обслуживать клиента или нет продолжаем вечеринку вот пусть добры Здравствуйте спасибо за доклад вопрос хотел спросить есть ли какие-то особенности при масштабировании инфраструктуры на несколько дата-центров То есть если какие-то без практики по этому поводу и два аспекта вопроса первое Как должен выглядеть там сам кластер это должен быть кластер который распределен по низкой центрам или несколько независимых кластеров и второе если особенности при работе приложения в таких условиях то есть насколько вот рекомендации из доклада хорошо работают когда вот а большое спасибо за ваш вопрос но он тянет на отдельный доклад действительно очень большая Тема и начать нужно с изучения кап теоремы то есть вот как теорема когда доступность согласованности актуальность данных строим на нескольких центрах это вот Большая головная боль а если у вас ваше приложение Cloud native если оно сделано по принципам и мюта была инфраструкции Да оно стоит лес с этим гораздо проще управляться Да и Ну вот эти вот описанные паттерны для имита был инфраструкта лес приложений они работают лучше чем для стейт-фу Вот но Везде как бы нужно понимать математику Да матча часть то есть принципы вот эти вот базовые принципы компьютер Санса на основе которых мы строим распределённое отказоустойчивая система А если у вас Ну действительно а-а накопился порог задач которые перешагнул вот за этот предел да Ну надеюсь Я ответил на ваш вопрос коп теорема да Вопрос Из чатика сейчас друзья все спросим а вакуумф Георгий спрашивает Доброе утро Спасибо большое за доклад вопрос про Джава и ликвел Бейс Как вы выходите из подобных ситуаций У нас есть ликви Base но там пишется только миграция которая выполняют выполняются гарантированно быстро относительно долгие миграции в зависимости от ситуации а миграции лучше всего в выносить в отдельный степ диплоя вашего приложения Ну то есть это вот как раз тема для будущего доклада грубо говоря Liqui Base миграции нельзя выполнять при инициализации каждого повода Ну допустим у вас 5 пудов Да один под решил перекатиться Ну допустим я не знаю по Watch dogue по Lives probia Да его перезапустила ликви Бейс миграции начали прогоняться да И опять что-то пошло не так Ну допустим это был амазон и у нас отстрелила эту ногу это была спот нода но Локи от этой миграции они остались Они будут жить какое-то время и допустим вот у нас осталось 4 экземпляра наше приложение у нас отстрелило еще три ноды и соответственно сервисы Ну как бы подвиги съехали на другие ноды Но они не могут инициализироваться из-за того что у нас один подвиг который дохнет это вот проблема поэтому прокат миграций Это первый степ перед выготом нового новой версии приложения и ну как бы миграции они должны быть построены правильно Надеюсь ответил про Amazon вчера была шутка на автопати что если бы он успел прийти и уходил то он бы стал облака И точкой Спасибо за доклад Спасибо предыдущим автором вопросов про зону ответственности между разработчиками можете рассказать как у вас выглядит дистрибутив как он должен выглядеть это зимник какой-то это докер образ это готовый конфиг поскольку я работаю в отделе который занимается эксплуатацией решений для машин Learning и Big Data Да у нас очень часто бывает Ну настолько большие артефакты Да что доставка их превращается в нетривиальную задачу и очень часто в Ну хорошо доставлять докер имиджами это вот абсолютно стандартно абсолютно хорошая решение Но мы часто выходим за рамки нам приходится делать свои костыли и велосипеды по доставке вот весов моделей это тянет на отдельный доклад возможно я когда это расскажу но хорошо и правильно доставлять доктор контейнерами когда это возможность кончается но включаем смекалку по факту разработчиков дает вам докер плюс нет разработчики дают нам комменты далее система сборки диплоя дособирает наш артефакт прогоняем его по релизному циклу Тестируем автоматические циклы при про там прочее прочее вот и автоматически доставляем на продакшн оттестированный артефакт докер ремешка который собран на по коммиту пользователя Спасибо пока разработчика Итак слово за слово исправил исправил работу конкурента Молодец Спасибо за доклад будет добры при наличии такой раскидистой мониторинговой структуры нет ли опасности уйти в то что она будет слишком много особенно высоконагруженных системах занимать место и следите ли вы в процессе эксплуатации со своим сервисом сколько отнимает рабочих мощностей вот собственно это мониторинговая нагрузка нет ли опасности уйти чтобы весь пар ушёл на свисток так скажем Всем спасибо да спасибо за вопрос на самом деле мониторинг логирование трейсинг Вот обермобилити это достаточно дорого и здесь вопрос целесообразности Если наш мониторинг позволяет нам исправлять ошибки и как делать наш сервис лучше экономить деньги на сбоях он оправдан соответственно как бы если он разрос и тогда когда бюджет на него превышает выхлоп вот тогда это не разумно Да поэтому везде нужно соблюдать баланс это очень сильно зависит от конкретного уровня Ну и операционный процесс мы сталкиваемся с новой ошибкой с новой проблемой покрываем ее но анализируем покрываем ее каким-то тестом исправляем и это постоянный процесс улучшения ведете наблюдение в процессе обязательно обязательно регистрации инцидентов обязательно постмортом Вот все вот это вот Друзья давайте за две минутки зададим три вопроса вот Раз два три и остальное в кулуарах А4 вижу Да но Давайте ускоримся тогда будьте обрывы давно с микрофоном в центре в желтом Да пожалуйста Ну давайте так Ладно такой вопрос вот Ну это же все паттерны отказоустойчивость можете привести какой-либо из двух например примеров когда допустим при их использовании вас что-то пошло не так и все наоборот упало второй Ну либо альтернативный пример если такой голову не приходит когда Возможно они вам не помогли сами по себе retrike один раз я положил про третями просто выставить их там какой-то неразумное количество все стало плохо все легло ретро это неудобный вопрос Когда тебе супруга задает каждое утро в течение полугода вот там у кого-то микрофон был Да вот да пусть добры Олег Да здравствуйте Спасибо за доклад У меня вопрос такой вот паттернов несколько Да и собственно Маст хэв с чего начинать Ну не считая например Health чеков Да как правило и Watch Dog of с чего начинать построение Вот это селф хилинг отказа устойчивой системы их нужно начинать с формулировки да то есть понимать какой уровень ошибок Вы готовы терпеть А вот если он меньше того чего вы можете построить позволить себе сейчас нам нужно приносить паттерны отказа устойчивости быть добры Спасибо за доклад Подскажи пожалуйста как вы оцениваете пригодность новые конфигурации по требования надежности допустим решили Вы перейти с локально хранить лимитов на глобальные добавили МКС по идее единая точка отказа и как будет вести системы если мы конечно контроллер откажется принимать трафик или перейдут локальный режим как вы верите документации либо какие-то испытания В отдельной лаборатории проводите мы не использовали у нас было фиксированное количество ноты на каждый был контроллер и мы вот посчитали что балансировка равномерно и мы используем локальные мы тестировали глобальный и это было Достаточно давно система ложилась когда ммкш отсутствовал в принципе там как бы тайм-аут можно настроить на монтаж Ну тогда оно ложилась сейчас возможно что-то поменялось сейчас вот ввели эту возможно ввели механизм когда лежит все запросы проходят Ну не знаю это надо проверить Спасибо надо посмотреть друзья значит через 10 минут на экране будет тех ток Зачем МТС вкладывается в Open Source в datascience в 11:10 Через 20 минут доклад про кубер а прямо сейчас мы поймем кто получает подарок за лучшие вопросы а я думаю это вопрос у человека который спрашивал что нужно сказать разработчикам чтобы получить хорошую пробу махните рукой где вы или просто подойти Подойдите за за подарочком Да друзья а если кто-то не задал вопрос который Вас интересует А я через 10 минут в 11 часов буду на стенде на Газпромбанка в дискуссионной зоне подходите можем обсудить интересующие вопросы супер тебе Олег тоже памятные призы от конференции"
}