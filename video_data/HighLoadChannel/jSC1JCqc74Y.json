{
  "video_id": "jSC1JCqc74Y",
  "channel": "HighLoadChannel",
  "title": "Data Sketches — как съесть слона целиком (даже если он бесконечный) / Сергей Жемжицкий (SberDevices)",
  "views": 103,
  "duration": 2818,
  "published": "2024-10-29T02:56:40-07:00",
  "text": "Всем привет очень рад вас всех видеть здесь как уже было рассказано сегодня мы поговорим о дата скетча Да ну или о том вот как есть слона целиком обрабатывая потоковые но и не только потоковые данные Итак я занимаюсь данными 12 лет да и меня зовут Сергей надеюсь что вы тут тоже все любите и заниматься обрабатывать данные вот за эти 12 лет Я научился устанавливать дуп и делать ещё несколько полезных вещей вот Ну я надеюсь что знаю кое-что об обработке данных Зачем вообще нам обрабатывать данные Да ну и Понятное дело что мы хотим обрабатывать эти данные Вполне себе эффективно А если посмотреть на график роста данных в мире то мы можем увидеть что как бы скорость ростана экспоненциально и в каждой нашей системе точно также количество данных прирастает и всё это мы хотим обрабатывать как обычно там быстро дёшево качественно Когда речь заходит об обработке а зачастую большие данные становятся маленькими а и мы стремимся минимизировать то количества данных которые мы будем обрабатывать путём разных хитрых ухищрений таких как использование различных форматов данных например orc паркет что-то ещё используя разные способы хранения данных по колоночная А может быть даже какие-то особенные базы данных при этом мы не забываем про компрессию партиционирование шардирование индексы ключи и вот всё что мы привыкли делать в традиционных реляционных базах данных как уже упоминалось Хотим мы всё обрабатывать быстро дёшево качественно Да но нужно выбрать что-то Ну даже так два пункта из трёх кажется что невозможно использовать как бы и получить все три эффекта То есть в принципе задача утопи поэтому мы с вами попробуем поговорить про то как обрабатывать быстро и дёшево эти самые данные нос допустим некую оговорку что Нас устраивает не качество Да там а вполне качество Ну или не супер точность а вполне точность то может оказаться Так что мы получим все как бы три качества обработки данных мы сможем их обрабатывать быстро дёшево Ну и вполне точно да недостаточно вполне Вот и посмотрим как реша задачи вот которые представлены на экране Да там как считать уникальные элементы разными способами Как определять наиболее популярные элементы в потоке считать квантили гистограммы достаточно точно и очень быстро и всё это можно использовать как ещё один из способов быстрой обработки данных вот при помощи такого инструмента как вообще мне кажется получили распространение не так уж и давно Ну или по крайней мере там за последние пару лет что-то стало про них слышно А как бы сама теория возникла ещё в Восьмидесятых годах прошлого века и по сути это такие структуры данных вот да сч - это такие структуры данных которые находятся на стыке компьютер Sci и математики часто их ещ могут называть вероятностные алгоритмы вот ну или чески алгоритмы которые в первую очередь а как бы позволяют приблизительно вычислять некоторые интересующие нас значения Ну и они обладают различными интересными свойствами которые на самом деле являются их самым важным преимуществом Да вот они как правило очень компактны масштабируемые Что это значит Это значит что к примеру мы можем посчитать на разных узлах нашего кластера в условиях распределенной обработки данных а затем смержить просуммировать вот эти значения уже на каком-то мастеру узле получить наш результат Да ну вот это вот свойство аддитивности Они обладают точностью с достаточной или с приемлемой ошиб м решать вот подобные задачи то эти самые даке или вероятностные алгоритмы Они как раз нам Вполне себе очень могут подойти если нас устроят какие-то приблизительные результаты вычислений если мы хотим получить скорость обработки данных в чём она будет заключаться мы посмотрим чуть позже Если мы обрабатываем данные в потоковом режиме или если мы хотим получать результат здесь и сечас то использовать Ну или обратить внимание на вот эти самые дата скетчи как они нам помогают вот допустим мы хотим посчитать количество уникальных слов не обязательно в потоке данных ну или каких-то элементов Да не обязательно слов каких-то элементов в потоке данных или просто у нас на как бы диске на разных серверах в корпусе данных там в чём угодно и наши данные распределены по куче узлов традиционно для того чтобы выполнить такую задачу решить задачу Нам необходимо перераспределить все данные по в соответствии с ключа с заданным ключом по другим узлам затем на каждом узле просуммировать что-то А и как бы получить в итоге суммарный результат здесь есть у нас одна неприятная особенность Да там как указано на слайде это шал то есть передача данных по сети перераспределение данных по сети мы знаем что такая это достаточно дорогое удовольствие А при помощи дата скетчей подобная задача решается чуточку по-другому А у нас точно также есть все наши узлы мы на каждом из этих узлов считаем а некий результат Да некоторое промежуточное значение а затем объединяем это промежуточное значение Да там благодаря свойству аддитивности дата скетчей и получаем в общем-то плюс-минус Да там с некой долей погрешности то что мы хотели то есть если возвращаться к почёта уникальных элементов Мы полностью избавляемся от шафла и в общем и целом значительно экономим на ресурсах на сетевых ресурсах вычислительных а дисковых в том числе результаты могут улучшаться на порядке е Один очень Любопытный пример если мы хотим делать обработку данных скользящим окном если допустим мы хотим считать уникальных пользователей нашего сайта в разрезе 5-7 дней и при этом за каждый предыдущие 7 дней делать тоже самое то оказывается что данные за каждый день мы обрабатываем столько раз сколько каков размер нашего скользящего окна и Наверное это не очень здорово поскольку опять-таки тратит все наши вычислительные ресурсы и вот собственно если мы решаем подобную задачу Да там скользящего окна при помощи дата скетчей история заключается в том что мы обрабатываем все наши данные за каждый день только один раз а затем за необходимый период времени мы объединяем результаты и получаем а то что мы и хотели получить Ну и Раз уж мы начали про подсчёт уникальных элементов А давайте поговорим про него чуть подробнее про то какие способы А есть э для решение этой задачи вообще А задача самого широкого профиля может применяться А в разных местах а я встречался с применением в большинстве случаев в рекламе но подходит и для для других кейсов например там кейсы по безопасности если мы хотим посчитать сколько тех или иных запросов в рамках какого-то источника приходит или даже в машинном обучении если мы хотим посчитать количество уникальных слов в корпусе данных и так далее и в общем-то вот таким нехитрым SQL запросом А можно решить подобную задачу но как бы есть одно но вот ключевое слово дикт которое в большинстве фреймворков или инструментов распределённой обработки данных приводит к этому самому эффекту шал то есть мы теряем на вычислительных ресурсах теряем память теряем цпу и так далее и Роме того могут возникать разные неприятные эффекты в виде данных Когда у нас частота встречаемости какого-то одного ключа слишком большая что ещё сильнее замедляет нашу обработку если взять датасет гитхаба всех событий которые генер хабом и попробовать посчитать по нему количество уникальных репозиториев то вот на Мом ноутбуке такой запрос сильный выполнялся Ну плюс-минус там 350 секунд а Вроде бы да данных было что-то около 3 млрд записей и наверное около 70 ГБ сжатых данных в раскованно виде Это около терабайт составляет и вот если попробовать это всё посчитать другим способом при помощи Дета скетча а там или вот вероятностной структуры данных G Log Log то а вычисление а становится значительно быстрее при условии конечно что у нас узким местом является шал а Хотел наверное спросить у вас А вот поднимите руку пожалуйста кто слышал что такое перло Log Ну отлично А кто же использовал Но ничего страшного те кто слышал для них Повторение мать учения А те кто Ну и будет повод может быть по использовать В чём заключается как бы в ЧМ заключается механизм работы вот этой структуры данных если мы представим что мы подбрасывают и мы хотим чтобы 10 раз подряд выпал РЛ Ну или решка там какая-то одна сторона монетки вот вопрос в том сколько нужно подб эту монетку что Ну больше чем 10 раз да там в принципе мы вот вероятность выпадения орла или Решки 1/2 там двух Орлов или двух решек 1/4 и так далее То есть для того чтобы получить 10 Орлов или 10 решек нам нужно подбросить монетку на что-то около 000 раз Ну плюс-минус и значит он применяет такую хитрость вот есть у нас поток данных из разных мы возм и каждый из этих элементов Зару получив некое его бинарное представление из ноликов едини и вот представим что единички - это орлы а нолики - это орешки и собственно вопрос в том Да там сколько нужно встретить таких бинарных представлений каждого нашего элемента чтобы количество ну вот ноликов или едини вле было бы там 10 Да там и интуитивно кажется что наверное плюс-минус такое же количество раз как и в случае с подбрасыванием монетки в этом алгоритме мы считаем количество лидирующих нулей то есть вот количество того сколько наша монетка А упала на не ребро на какой-то одной из конкретных сторон выбираем максимальное и плюс-минус это и будет очень грубая оценка того сколько элементов у нас в потоке данных часто Ну вот в других докладах Я смотрел упоминает что очень крутая структура просто потому что она требует а всего лишь 2 КБ а точнее пол КБ памяти на плюс-минус погрешность 2 про Но если подумать А то при большом количестве уникальных элементов к примеру миллиард там 2% это уже наверно Ну вот порядка 20 Милн то есть достаточно большая погрешность получается и может быть Для нас это неприемлемо и вот у нас есть график Да там и различные формулы Каким образом можно посчитать погрешность от размера если мы хотим собственно говоря значительным образом уменьшить погрешность этой структуры данных остаётся достаточно демократичным то есть на 1% погрешности нам требуется всего 10 КБ место а на со про 50 Мб что в общем-то может оказаться приемлемым при условии что мы обрабатываем гигабайты терабайты а может быть и больше данных да и всё это для произвольного потока Отлично вот очень рад за те ребят США про вот вопрос в том Есть ли что-то лучше чем эта структура данных или этот алгоритм и вот краткая история собственно того Как развивались различные алгоритмы и как они появлялись на протяжении нашей истории Вот оказывается в 2017 году появился новый алгоритм То есть если простить интернет там достаточно часто встречается называется compressed probability Counting он основан на вот алгоритме в честь FM 85 Яго так буду называть Flag Martin авторы кажется вот и он работает лучше Вот Давайте попробуем разобраться в чём он лучше собственно название идея абсолютно такая же а то есть многие вот эти вероятностные алгоритмы работают похожим образом то есть снова та же Идея про подбрасывание монетки мы хэширующая грубую оценку которая а и будет оценкой количества уникальных элементов нашей последовательности Если сравнивать скорость работы вот на той же самой задаче подсчёта а количества уникальных репозиториев на гитхабе то compress probability Counting работает плюс-минус той же скоростью и погрешность а зависимость погрешности от размера а тоже условно говоря оставляет наш размер в очень как бы демократичных рамках всего 2,5 КБ Да если мы вспомним то локло требовал за 1% там около 10 КБ вот здесь чуть-чуть поменьше и 20 Мб за % погрешности для произвольного потока данных на самом деле там зависимость нелинейная и если внимательно посмотреть на графики то как бы вот можно заметить что probability coun он чуточку получше Да а а именно в среднем на 60% а потребляет А меньше а места чем пер Log Log при сравнимой погрешности на самом деле погрешность даже немножко меньше остаётся и собственно вопрос наверное у меня А почему его до сих пор не внедрили во всякие другие модные опенсорс платформы фреймворки обработки данных есть одно но а все эти структуры данных Они аддитивные то есть их мы можем объединять друг с другом а И поэтому они позволяют решать не все кейсы которые нам могут быть интересны например да если вернёмся в нашу любимую рекламу У нас есть сайт А на котором мы знаем количество уникальных пользователей и вот мы вдруг захотели посмотреть а сколько же наших уникальных пользователей пересекается с уникальными пользователями партнёрского сайта Или допустим сколько наших уникальных пользователей не посещала партнёрский сайт то есть здесь у нас уже появляются другие операции над множествами Да там разность множеств Мы хотим смотреть пересечение множеств и как будто бы и бы мощ нам в этом но нет при попытке делать подобные операции вот исследования показывают что Ошибка растёт уже до неприличных размеров и вот придумали ещё один сч который называется тета скеч который позволяет решать подобную проблему интуитивно можно этот алгоритм понять следующим образом допустим у нас есть очередь 100 м и в ней стоят люди Каким образом нам понять как нам оценить сколько Вообще людей не зная их количества вот в этой очереди вообще кажется что мы можем взять среднее расстояние между каждой парой и поделить длину очереди на Вот это среднее расстояние и так мы в общем-то получим оценку вот если мы в качестве среднего расстояния возм например пятку и подем на то 20 больше чем у нас на самом деле вот представлено на слайде вот а что если взять не как бы одно расстояние Да вот в данном случае минимальное расстояние а несколько минимальных расстояний и взять от них среднее и использовать это среднее как вот среднее расстояние между всеми людьми в этой очереди и в этом случае оказывается тогда уже наша оценка она будет приближаться к реальному количеству людей в очереди вот на данном синтетическом примере получается 5 п 10 п 15 п 10 40 да Там мы взяли четыре разных расстояние то есть среднее расстояние будет 10 100 дем на 10 Ну и получаем 10 плюс-минус столько сколько людей у нас действительно в этой очереди находится вот это как бы интуитивное понимание как работает этот опять-таки Если сравнивать по скорости выполнения он работает плюс-минус также как и другие скетчи Если попробовать поза эти структуры данных в кликхаус там есть уже готовые агрегатные функции то на м результ становится е более интересным есть не в разы А на порядке И как бы вот почему имеет смысл обращать внимание Да кос пробили Катин тут нет потому что в Хаусе его пока что нет Ну может когда-то занесут снова у нас погрешность зависимость погрешности от размера хорошо себя ведёт Если сравнивать с другими структурами данны данных такими как ll и compress probability Counting то можно заметить что размер Вот тета скетча сильно больше здесь здесь логарифмическая шкала для наглядности Но это необходимая плата за то что мы можем делать пересечение и Объединение множеств да Не объединение пересечение и разность множеств и стоимость она вот такая то есть в 16 раз больше требуется места по сравнению с перло логом при сравнимой погрешности и в 40 раз больше чем compress probability Counting но зато мы можем к примеру А делать всякие замечательные штуки Когда нам необходимо поджо две таблицы которые большого размера мы можем не пересекать их а а посчитать вот скеч по одной таблице посчитать Кеч по другой таблице и пересечь тем самым получив как быте щий нас результат сильно дешевле что ещё хотелось бы вам рассказать вот есть ещё а один класс задач интересных которые при помощи скетчей решается частота встречаемости снова куча кейсов нашей любимой нелюбимой рекламы например наиболее часто устанавливаемые приложения какое-то ограничение показов рекламы или даже определение перекосов в данных Да когда мы хотим определить что какой-то ключ в нашем потоке или даже в нашем датасете встречается наибольшее количество раз Ну или больше чем другие ключи если мы решаем задачу традиционным сиквела то запрос кажется весьма нехитрым Но в нём снова есть неприятные операции вот такие как Grow by которые в случае распределённой обработки данных как правило приводят к дороще шафлу снова ВС тоже самое большое количество вычислительных ресурсов требуется для решения подобной задачи сетевого ИО и если подобную задачу подобный сикл запускать вот всё по тому же дасе гитхаба пытаясь найти в какой репозитории Коми наибольшее количество раз медленнее чем такая структура данных которая называется mech поднимите пожалуйста руки кто слышал про неё что-нибудь Отлично тогда мне будет что для вас рассказать суть этой структуры Ну и интуитивное её понимание заключается в следующем мы возьмём ш функций и для того чтобы составить вот такую табличку в каждой табличке записан нолик Это изначально как бы по сути некий каунтер Да там некий каунтер который будет соответствовать значению нашей хш функции и вот когда к нам приходит очередной элемент мы хэши этот элемент и по хэу для каждой ш функции выбираем соответствующую ячейку в нашей таблице и записываем туда единичку то есть по сути инкремент её на единичку Так мы делаем для каждой функции То есть если их у нас три вот как в данном случае то там в первой строке мы Запишем что-то в какую-то ячейку там во второй и в третьей когда к нам приходит следующий элемент наши хэш-функции определяют ровно те же самые ячейки мы их находим и прибавляем туда единичку и так мы делаем для каждого последующего входящего к нам элемента Интерес Интересно начинает быть тогда когда у нас для разных элементов наши хэш-функции попадают в одну и ту же ячейку и здесь мы на самом деле делаем вот ровно всё тоже самое мы просто прибавляем единицу до тех пор пока мы не захотим получить некий результат или до тех пор пока поток данных не закончится тепер если ВД наиболее популярные элементы Ну или даже не наиболее популярные сколько конкретный элемент встречался раз в нашем потоке данных мы делаем ровно всё то же самое мы берём наш элемент и хеши каждый из имеющихся у нас хэш-функции а а затем определяем В какие ячейки эти хэш-функции попали и выбираем все соответствующие ячейки а потом просто берём а минимальное значение из а тех ячеек вот в которые мы предварительно записывали во время обработки потока данных наши результаты это кажется интуитивно понятным Почему минимальное значение Да потому что одна и та же хэш-функция оно действительно может приводить к коллизия а и для разных значений выдавать одинаковые для для разных значений потока и давать одинаковый результат если Ну для следующих элементов мы делаем а плюс-минус тоже самое выбираем просто минимальный вот такой вот нехитрый алгоритм а очень демократичный размер у него а при небольших значениях ошибки и всего 11 КБ занимает эта структура при фиксированной погрешности там с некими оговорками в 1% как правило потребляет а потребляет фиксированное количество места есть ещё один очень интересный алгоритм который решает похожую задачу называется мисра Грис в честь тех кто его придумал он работает а чуточку быстрее чем вот этот самый Count Me Sketch А на моём конкретном примере а возможно на ваших примерах это всё будет работать в случае распределённой обработки данных сильно быстрее или на порядке быстрее суть его заключается в следующем вот если мы хотим из потока извлечь два наиболее популярных элемента не зная сколько элементов у нас в этом потоке заранее то давайте заведём просто Два счётчика и будем проводить следующие нехитрые мероприятия вот к нам пришёл некий элемент если у нас есть свободные счётчики вот в данном случае их две штуки мы назначаем наш элемент этому счётчику и увеличиваем счётчик на единичку Теперь если нам приходит следующий элемент он совпадает с каким-либо из счётчиков Ну то есть вот мы видим что нашему элементу уже был назначен счётчик мы просто берём этот счётчик и увеличиваем снова на единичку опять к нам пришёл элемент есть свободный счётчик мы назначили элемент на счётчик и увеличили счётчик на единичку Теперь когда к нам приходит совсем незнакомый элемент А у нас все счётчики заняты нужно применить небольшую хитрость вот мы количество таких ситуаций когда все счётчики заняты и пришли элементы которых мы не видели Ну или по крайней мере которые для которых не существует счётчиков мы будем количество таких ситуаций тоже считать и для такой ситуации все счётчики которые у нас уже были назначены мы уменьшаем на единицу и те счётчики которые приобрели значение ноль мы освобождаем теперь вот да снова пришёл какой-то элемент был свободный счётчик мы его назначили на единицу увеличили на единицу и вот собственно алгоритм завершился то есть мы уже определили два наиболее популярных элемента потоки то есть мы определили что в данном случае Алиса и Боб они встречаются чаще всего вот в том нашем потоке данных и если мы хотим оценить количество встречаемости этих элементов то очень грубая оценка - это как раз значение этих счётчиков но чуточку Точнее можно получить значение если мы э прибавим сюда а вот количество ситуаций когда к нам А приходили незнакомые элементы размер частоты в данном случае у нас размер зависит не совсем от ошибки а от того насколько часто встречаемые элементы Мы хотим детектив в потоке то есть Нам нужно к примеру 10 КБ для того чтобы мы могли обнаружить объекты которые встречаются в потоке больше чем 1% раз И вот какой алгоритм выбрать Это задача нашей конкретной как бы задачи да А если мы хотим уметь детектив низко встречаемые элементы то нам больше подходит Count Min Sketch Зато мисра Грис даёт точный результат без всяких погрешностей если мы заранее знаем сколько уникальных элементов в нашем потоке и создадим столько счётчиков сколько уникальных элементов есть ну к примеру какой-то справочник и так далее Count minch как правило компактнее а для для разрозненных потоков данных но зато мисра Грис быстрее если количество Вот таких коллизий Когда нам необходимо декремент счётчики невелико Ну то есть по сути количество элементов совпадает с количество счётчиков совпадает с количеством уникальных элементов Надеюсь не заскучали и не устали потому что хотел ещё одну тему затронуть про квантили гистограммы которые тоже встречаются в рекламе или в задачах например посмотреть распределение данных по каким-то колонкам или в поведенческой аналитике когда мы хотим посмотреть к примеру сколько пользователей там проводят определённый процент времени на наших сайтах Вот и для начала ведём как бы несколько понятий есть у нас ранг это просто порядковый номер нашего элемента есть у нас нормализованный ранг это вот этот Сайн пон на количество известных нам элементов квантиль - это значение которое соответствует рангу вот наиболее интуитивно наверное можно представить Это следующим образом Наверняка вы видели там фану и часто мониторят Насколько быстро работают те или иные сервисы Ну или плохо и вот например 80% запросов не превышает по времени выполнения 40 миллисекунд это как правило как раз квантиль от ранга если мы хотим посчитать сиквелов таким сложным запросом который я нал в Интернете для пост это можно сделать здесь важен не сам запрос а в том что в нём встречается Order в случае ра обработки данных это шал что плохо то есть нам нужно все наши данные отсортировать Перед тем как мы сможем определять вот эти наши квантили там строить Гра и так далее вот не репозиторию о датасета с событиями гитхаба а почёт медианы по количеству коммитов А в репозитории работает достаточно долго больше 5 минут А вот э скетч который делает то же самое работает в 10 раз быстрее есть совершенно разные алгоритмы а которые сейчас мы посмотрим как работают на пальцах они все характеризуются двумя важными свойствами размер Сколько занимает этот дата скетч и Можем ли мы его объединять для того чтобы можно делать можно было делать параллельную обработку данных на разных серверах и вот возьмём для примера посмотрим как работает алгоритм mrl есть у нас поток данных а возьмём буфер размера к и будем все элементы этого потока запихивать в наш буфер пока он не заполнится как только Буе заполнился мы сортируем всё что в нём есть и удаляем нечётные элементы после этого увеличиваем вес этих элементов освобождаем буфер и Вот оказывается если взять не один такой буфер а определённое количество этих буферов то весь наш поток данных а в последнем буфере А можно схлопнуть а в 11.000 раз при погрешности всего в 1% Ну в случае определения кванти зачастую этого более чем достаточно вот все те алгоритмы которые я показал до этого они работают очень похожим образом названы по-разному просто потому что количество буферов отличается их размер каждый из буферов может быть своего собственного размера и вот снова поведение погрешности в зависимости от размера очень демократично всё по сравнению с тем сколько нам нужно было бы места в случае традиционной традиционного способа решения задачи занимают вот эти самые квантили вот если кто-то хочет почитать про дата скетчи здесь есть список литературы это не всё далеко не всё но какие-то вот базовые вещи Откуда можно почерпнуть много полезной информации каждый из них представляет большие математические выкладки на 50 страни насчитать можно использовать они имплементировать c+ Plus ну или вот есть библиотечка Stream lip которая делает Тоже самое для Java Итого А вот если нам подходят приблизительные результаты наших вычислений А если А количество ресурсов сокращается на порядке а скорость возрастает на порядке да то зачем решать подобные задачи другим способом если есть отличный инструмент на этом собственно всё что я хотел вам рассказать намного больше хотел рассказать но вот влезло Вот отведённое время буду рад ответить на ваши вопросы а да спасибо Сергей пока пока вы думаете над вопросом можете ещё перейти сразу и по QR коду проголосовать за доклад и сразу же вот первый вопрос в начале зала Сергей Большое спасибо за доклад очень интересно ну могли бы вы прокомментировать когда вы оценивали погрешность сложилось впечатление по крайней мере у меня что погрешность сильно зависит от стохастической занимает 90% а осталь остальные по мелочи то погрешность при таких скетча должна резко возрасти й ре арит по если Спасибо большое за вопрос очень хороший если мы говорим про задачи определения наиболее часто встречаемых элементов в потоке Ну как я понял из вашего примера это так то а окей то есть если если у на распределение примерно равномерно то там понятно которые занимают много остальные очень мало то Tin coun будет давать больше ошибку на скетча Ну складывается такое впечатление Хороший вопрос У меня прям точного ответа нет а на него я бы с вами наверное обсудил в кулуарах его мы посмотрим документацию А в моём опыте зачастую вот значимой погрешности не возникало возникает Вот то про что вы говорили оно хорошо видно например когда мыс наем об скетчи у которых есть большое количество элементов и небольшое тогда погрешность начинает расти и в общем неприятные эффект с которыми нужно жить да там но как бы скорость мне каже это немножко ограничивает Круг задач когда у на есть тако СТ выделен это обсуждать дискуси Следующий вопрос Давайте вот на дальнем ряду передам спасибо за доклад Скажите пожалуйста в вашем примере вот проте скеч что собственно хранится в этой структуре данных и как они потом тся приме про количество людей очереди Угу тоже отличный А вопрос Большое спасибо Ну давайте я не буду перематывать Аа вообще он работает чуточку посложнее Да там Аа чем вот этот пример в очереди А каждый входящий элемент он мапи в отрезок между нулём и единички А и ну по сути А вот а то как работает этот эта скетч А мы берём а минималь несколько Да там а значений между нулём и единицей минимальных считаем среднее расстояние между ними и а по сути нам достаточно хранить Вот только такое количество А как бы минима минимальных расстояний между нашими элементами оно достаточно большое на самом деле но не знаю там вот те ошибки про которые я рассказывал они начинают значительно при порядка наверно там 450 вот этих вот значениях элементов то есть вот такое количество нам нужно плюс-минус элементов не элементов Да там ше от этих элементов хранить и мангов в отрезок 0 и этого будет уже достаточно для того чтобы как бы ну вот считать достаточно низким уровнем погрешности вот на первый ряд пожалуйста Вопрос Ага да спасибо за доклад маленькое замечание я правильно понимаю что вот эти тесты с графиками они были локально на компьютере Да они были действительно на локальном компьютере Аа за счёт этого А ну как я считаю а разница не настолько велика как могла бы быть Да мне кажется тоже на порядке должна быть а то там по-моему КМС там в три раза выигрыш это даже не на порядок и зачем Нам жертвовать тогда погрешностью А да всё-таки э возвращаясь к первому вопросу Э вроде как ээ там пытались затронуть э мощность в целом э ну Должны ли мы обращать внимание на мощность вот условно колонки от которой мы делаем А я больше хотел бы спросить про не знаю в общем есть какое-то опасение в плане вот этих аппроксимации в алгоритмах то что условно если у нас есть какой-то перекос в данных в стандартном алгоритме мы это заметим сразу и у нас просто медленно работает или не знаю По памяти падает что в этом Ну а что происходит типа вот вот в этих алгоритмах то есть мы можем получить полностью нерелевантные ответ какой-то Или или всё-таки нет Я бы да спасибо большое Я бы здесь ответил это не бак это фича а которая заключается в том что Ну вот смотрите большинство из них вот этих алгоритмов оно влечёт в себя некий элемент Ну вот операцию хеширования один и тот же элемент Мы всегда см спим в один и тот же хэш и если у нас какое-то большой элемен количество элементов в потоке Да там вот В задачах C distinct оно всё всё равно будет попадать в один и тот же хэш Поэтому собственно как бы здесь не возникает а никаких проблем с точки зрения перекосов Да как бы через C distinct мы не определим А количество а наиболее популярных элементов Да наиболее часто встречаемых но в этом и суть А что если весь Т то есть мы же берём хэш если там вот Прошу прощения у нас очень мало времени мы как раз подискутировать можем в дискуссионной зоне А сейчас последний вопрос и будем выбирать лучший А спасибо большое за доклад у меня такой в общем-то вопрос в общем-то алгоритм понятен то есть мы грубо говоря агрегацию переносим на момент появления новых данных как бы предс делаем но с уменьшением точности вот единственно зачем мы декрети счётчики При появлении нового элемента на самом деле вот в классическом алгоритме мистра Грис этого не происходит и здесь очень упрощённые версии на слайдах были приведены когда мы счётчик Ну даже даже так А зачем мы декрети основные счётчики Да Дада А если у нас поток данных разреженный Ну или даже так для того чтобы мы имели возможность найти более популярные элементы Ну вот если логические которые встречаются в потоке данных позже чем у нас были заняты Все счётчики уменьшаем вес его скажем так но по сути да да спасибо большое за вопрос коллеги К сожалению мы уже по времени не укладываемся и можете продолжить дискуссию выйдя через левую дверь Да и там будет дискуссионная зона дождитесь пожалуйста спикера А мы переходим к вручению подарков за лучший вопрос Скажи пожалуйста только один подарок да Так но я бы вот по мужчине вручил Да зань Два подарка мне вот только что сообщают что у нас на самом деле Два подарка поэтому один мужчине вот здесь из второго ряда а второй кому вот второй наверное парень сейчас уходит который спрашивал как работает ТТ скетч в подробностях или нет а вот вот молодой человек кажется вы Да спрашивали как работает Это скетч в подробностях Да вот вот он отлично подарки от наших партнёров Газпром нефт нашли своих а Победителей и у нас Сергей для тебя тоже есть подарок от наших спонсоров сейчас его вручим Спасибо большое за твой доклад очень интересно ждём тебя снова с ещё более интересными тематика и копани вглубь ещё больше А сейчас мы снимем с тебя микрофон и отправим в дискуссионную зону Спасибо большое за выступление Проходи пожалуйста L"
}