{
  "video_id": "Yf48-6Ce3_M",
  "channel": "HighLoadChannel",
  "title": "ML для ML в задачах качества данных / Денис Занков (Газпромбанк)",
  "views": 358,
  "duration": 2245,
  "published": "2023-01-19T06:59:52-08:00",
  "text": "коллективы очень слышно до меня ну последний доклад надеюсь будет самым интересным и как минимум веселым давайте начнем меня зовут дени звонков так слайды уже должна показываться слайды сейчас все починит я думаю так но такие красные слайды о которых кровь глаза кровь из глаз мы будем стараться быстро пролистывать тогда меня зовут для не зенковки я работаю управляющим директором в газпромбанке больше восьми лет 8 лет опыта в аналитике разработки больше пяти лет в крупнейших банках россии занимаюсь разработкой моделей в розничном бизнесе проведением тестирования и оценкой бизнес эффекта давайте передём к постановке задачи что такое качество данных качество данных это обобщенное понятие отражающая степень их пригодности данной задачи но какие банки задачи наверное пояснять сильно много не надо это как кредитный скоринг так и не знаю мы сами конкретно занимаемся маркетинговым моделями привлечения ток и так далее мне нравится определение качества данных по стандарту iso это набор эпитетов такие как достоверность полнота актуальность еще там ряд но вот основные из них теперь такие но и актуальность проблемы в том что работа с качеством данных актуальна не только тем кто строит имели модельки но и наверное тем кто сейчас как модно говорить принимает решение на основе дата гривен подхода ну как я уже говорил что мы строим модели в маркетинге именно эта модель и характеризующие продуктовое поведение клиента это очень большой объем данных на какой-то срез средств может быть днем недели и месяцы мы так далее но давайте абстрагируемся от нашей задачи представьте что это как здесь докладчики говорили могут быть там не знаю фича по сообщество вконтакте там да много фичей лайки просмотры и так далее это может быть как данные водители в яндекс такси там тоже можно много справиться врать данных данные пользователя в такси и так далее представим что пользователи много данных таких тоже много собственно они данные имеют свойство ломаться кто то зачем ты не уследил кто-то выкатил новую фичу из все мама могла поехать ну соответственно возникла у нас такая задача поиска такого решения но давайте у нас это все кресты так скажем проблема кристаллизовалась на моделях оттока ну собственно чтоб собрать фичи по витрина оттока нам нужно пройти вот такой направленный а цикличный граф то есть ну дак такой вот мы его из примерно из какого-то airflow достали ну и где-то что-то тут если что-то тут произойдет наверное понять тяжело да данных становится все больше и больше вещей но фичей признаков характеризующих поведение клиента тоже становится все больше и больше а еще и меня есть бизнес процессы ну например у вас появляется какой-нибудь новый продукт или какой-нибудь какой-то список действий привязанных к одному какому-то классу могут может поменяться и как с этим работать вручную уже становится непонятно то есть но бизнесу тяжело объяснить что у вас нужен отдельный ресурс на то чтобы мониторить качество данных они говорят нуждаться scientist и или там инженеры вы сами должны как с этим бороться но в какой-то момент человек уже с этим не справляется и как правило на подобные задачи ресурсов всегда не хватает потому что там задачи конкретно влияющие на прибыль не должно быть в приоритете но а мы еще и банк а у банка есть регулятор к а если у вас что-то сломается на регулировки то наверное нам будет чуть хуже чем кому-то либо другому ну здесь я привел статистику по недавно закрытым банкам не факт что они там из игры из-за регулировки у них это за власть лицензия но все же это могло влиять как было раньше раньше были эти проверки на уровне гитель ну вот здесь я вот даже бортик наш скинув из нас такие проверки такой контроль объемы изменения наверное там какие-то статистике первые первоначальные там сколько данных пришло сколько там у перегрузилась на следующий слой и так далее это делалось до модели и что-то мы делали уже по результатам по результатам когда мы строим модель или уже когда его лидируем то есть это обычная проверка распределений бен и яндекс стабильности популяции пьеса я наверное многие знают разрабатывала модельки какие-то стаб проверки ну например вы берете там рандомно из центрирования и делать хаттестад тест на две выборки и так далее смотрите как выбор карты периода к периоду изменилась но вот как то так но по сути вот здесь я уже такая проблемка есть да что эти проверки видимо не полностью покрывают то что мы хотим покрыть в качестве данных и что-то заметить не то а ручные проверки распределения это и все остальное у нас уже происходит уже постфактум куда она что-то сломалось ну например поехал пища и пас кору от месяца к месяцу что делать ну давайте вот обсудим мы конечно же занимаясь д сам решили что нам нужен иметь чтоб помочь в мире собственно такой порядок переменных мы обычно видим у себя в банке наверно в комнате и яндексе или в к их гораздо больше но допустим пойдем от быстро и на у нас есть некая витрина или фьючерс top это допустим там более 1000 колонок более тысячи клиентов и по ним еще каждый по каждому клиенту состояния за 40 плюс месяцев ну собственно давайте попробуем подумать и как-нибудь имели алгоритм сюда засунуть ясно что у вас любой алгоритм кендо нам даже исправляться ну не справиться это он априори потому что вас сразу по памяти такой объем не не оценишь ну собственно было мы стали думать что с этим делать нам наверное избавиться от какой-то от какого-то среза но наверно логично избавиться среза где больше всего от среза клиентов собственно давайте вычеркнем сделаем такой некий некое снижение размерности ну идея в том чтобы не оценивать каждую переменную построчно а представить ее в виде описательных статистик которые будут характеризовать данное распределение в конкретном месяце сейчас дальше покажу как это работает ну во-первых мы выбираем куда гранулярный и можем посчитать какие-то статистике простыми средствами ну допустим на ходу при средствами иску или почитать просто количество значений количество уникальных значений количество пропусков минимумов минимум максимум распределения средняя медиана стандартное отклонение и так далее список может быть расширен что мы это 2 получим мы получим в каждом временном срезе какое-то описание распределения которое характеризует поведение там во всей наши базы клиентов в конкретном временном периоде и так по каждой применение так как по каждой переменной и так мы храним уже на каждый момент времени не там миллион 10 миллионов 100 миллионов строк а уже какие-то там статистике характеризующие это распределение далее ну собственно вот так примерно можем представить какую-то переменную да здесь у нас на это на наши банковские какие-то перед значения представьте что там в каком-нибудь такси это не знаю там количество заказов в преф премиум не знаю там количеством отмененных заказов и так далее вот вы так представляете по всей клиентской базы базе ваши распределения далее но собственно далее нужно модель мы долго думали думали дули оценили алгоритмы поиска аномалии и решили использовать модель основана на деревьях так называемый изолированный лес но модель без учителя а нам лень что-то какой-то таргет выбирать что-то размешивать поэтому мы пошли там сразу в модель где которая работает без какого-то торги то и нам сразу понравилась модель основанный на деревьях почему скажу позже теперь до применив такую трансформацию представив каждое состояние всей клиентской базы в какой-то промежуток времени мы можем уже в принципе работать с кем-то mlg ритмом так как мы сжали размерность и размерности уже становится более менее адекватные да у нас не вычет моя самая большая там массив данных мы представили в виде описательных давайте немножко поговорим как работает этот алгоритм это нам будет важно далее до ази лишь in forest или так называемая зале изолированный лес но алгоритм основаны на на ансамбле ансамбле деревьев каждое новое дерево строится на подвыборки для случайного признака считаем минимум и максимум выбираем случайную точку между минимум максимум делаем сплит на 2 под дерево разбиваем это дерево пока в каждом листе вот до конца не к не останется по одному наблюдению после построения ансамбля не каждого дерева нужно посчитать сколько у нас какой у нас путь до каждого наблюдения средний путь до но идея просто чем короче средний путь тем более вероятно что ваше наблюдение будет аномальным но то есть вас до если наблюдения лежат плотно они не являются выбросами то до него как бы много-много случайно разбивать и до него будет более длинный путь потому что там большое плотное скопление точек а как правило такие аномалии будут отделяться достаточно быстро по пути но когда вы там рандомом выбираете точки много-много раз просто понятно интерпретируем но собственно почему выбирали именно этот алгоритм как я сказал что объяснить его можно сейчас буквально за две минуты но главное что это деревья они не требуют никакого дополнительного шкалирования почему не требуют нам это важно но как нам грубо говоря сравнивать количество пропусков с количеством не знаю с максимум в данном распределение вот деревья засовываешь без всяких предобработки и все отлично работают не требуют это чистый выбор киданей кто это там чтоб распределение были там примерно плюс минус от месяца к месяцу одинаковые ли там на меньших периодах но как всегда всегда есть несколько но вам необходимо накопить какие-то данные мы так посмотрели что два года три двадцать четыре периода уже норм для дневных наверно больше там 90 дней лучше поднакопить далее будут у вас обязательно ложные срабатывания на резком изменении какой-то бизнес логики но может даже это на самом деле не так плохо потому что вы как бы вам бы и хорошо понимать что у вас какая-то бизнес-логика изменилась и сможет наверно стойкий самую модельку перестроить уже там на новой бизнес логики да здесь у вас не будет отсечки здесь у вас будет чистая функция ранжирования аномальности и то есть но придется с мун аналитический искать эту отсечку но и плюс эмпирическая настройка гипер параметров благо гипер параметров не так много ранжируя чая функция если мы зайдем в код в документацию и злейшим фореста то увидим что но в принципе все параметры нам известны такие как бы bootstrap количество количество деревьев количество фичи boost репе но вот я тут выделил контаминация параметр да это грубо говоря если вы задаете числом какой-то дробной дробной цифрой то он означает какую долю он выбросов мы будем считать примерно аномальной но мы люди ленивы и мы понятно оставим его на авто но понимаем что всегда будут ложноположительные срабатывания при этом но давайте попробуем все-таки быть продолжать быть ленивыми и тогда за вас алгоритму будет выбирать что будет считать ноликом что или что единички ну грубо говоря аномалии не аномалий но нас это не очень устраивает поэтому мы хотим не просто классифицировать все а отранжировать по уровню аномальности поэтому мы не определим алгоритм а используем режим function дезинформация тут интересно так работает что она в отрицательные значения кандидатов на аномалии отбрасывает а в положительный что но более или менее средний путь длинный и поэтому я его не считаю их аномальными поэтому вот чтоб как-то вот распределение будет выглядеть примерно как вот так вот да от нуля грубо говоря поэтому давайте сделаем из незамысловатую еще дополнительно аранжирую функцию которая меньше 0 будет братьям по модулю значений функций функции и брать от нее кори корень или кубический корень а все что больше нуля мы еще будем до нажать на само себя так чтобы значение ниже нуля у нас больше стремились к единице зачем это и почему я чуть больше чуть дальше покажу так но если нас есть ранжируя щая функция то соответственно мы можем оценить ошибки первые 2 рода что уже хорошо да собственно задать ошибку там где мы считаем что наилучшее престижен или иной плюсны и лучший прикол ну грубо говоря это просто аналитическая работа один раз посмотреть сколько там какая доля среднем аномалий у нас появляется и на каком уровне у нас установить отсечку большой bus нам дало когда-то мы нашли библиотеку диффе одессе умеют показывает значимость переменных в таких алгоритмов как излишен forest ну то есть грубо говоря мы подсвечиваем какие переменные привели к тому что наши если искомое там значение какого-то признака стала аномальным что это нам дало почему это большой куст потому что мы смогли на основе этого давать подсказки уже какие-то каким-то даты инженером дастер там и сейчас мы работаем над системой подсказки что в переменной пошло не так и куда обратить внимание на что ну например мы уже можем дать конкретным подскажет что у вас полностью средств не рассчитался что у вас дела там что-то за двоилось и на основе вот этих вот скоро интерпретируем асти от этой библиотеке диффе мы уже просто не не даем какую-то систему которая просто ранжирует а уже ранжиру с каким-то подсказками давайте какой-то под итог мы презентации ещё продолжится что мы умеем и не умеем находить но пропущенные значения если они массово начинает пропускаться больше обычного мы явно находим дубликаты находим тоже отлично какие-то противоречия в данных когда у нас какая-то логика бизнес-логика меняется находим аномальные значение выброса тоже находим если не не массово про если они происходит масло шум плюс минус потому что шум бывает естественной или когда я сезонность сюда прокрадывается но если ты сезона 100 деревья как выглядит сезонность потому что у нас накопленный большая статистика и поэтому нормально сама сезонность как бы в аномалии не попадает отсутствие полноты данных мы отлично находим нарушение целостности вот здесь вопрос что сейчас считать на нарушение целостности под этот пункт может попадать то что как бы ключевые клиенты у вас отъедут или там какой-то сегмент здесь мы поставим плюс минус потому что здесь тоже зависит от массовости некорректные форматы и представления данных дам мы нашли конкретно что у нас поле которая должна была которая там со ставкой и чистовое она у нас в базе хранится как текстовая от нее считается минимум когда вы считаете от текстовом поле минимум у вас минимум полировать цифры идет и тем самым вот мы нашли конкретную некорректный формат данных эффективное значение плюс минус потому что фиктивные значения тоже бывает массовыми не марса вами ошибка ввода данных нет потому что ошибка данных это единичный случай когда у нас происходит операторы поэтому я поставил скорее нет но и нарушение структуры плюс-минус потому что структура как бы структура может также как и быть когда он полностью формат есть так может быть структура на ком-то под сегменте случится и вы этого не особо заметите но мы уже у нас есть идейка с этим бороться применение алгоритма да давайте пройдемся что мы делаем мы считаем стране на нашего hadoop чека средствами sql наши описательные статистики в разрезе каждой переменной витрины для статистики по каждому временному срезу происходит обучение применения модели изолированного леса для принятия решения вносят дополнительную ранжируются цию которая работает на disease in function алгоритма и позволяет нам отсортировать перемены от возрастания к убыванию по аномальности ну и дальше интерпретируемые значимость статистик которые привели как аномальности чтобы дать какие-то подсказки триггеры или отчет для конкретных там дат инженера до тортов условных что пошло не так так но на этом слайде вы видите стандартную лямда архитектуру у нас есть модели которые ну и данные который крутится в бочке есть данные которые плюс в онлайне с бы чём собственно мы справляемся отлично с онлайном пока думаем как но пока идея что накапливать мини baci и через них тоже эту модельку пропускать то есть пока полноценная модель к работает на бочок их данных но и для онлайна решения в принципе как мы вводим модель здесь должно быть отдельное наверное какой то про нашим эллипс можно отдельным презентацию сделать отдельный доклад но наверное думаю плюс-минус как их всех вывели мы на в купюрница кластерах докер контейнеры просто заводим параметры какие-то на вход модельки работает она в режиме переобучения на какой вид на какую там with ring усмотрит какой фьючерс тура лазиет перри обучаясь перри обучением без переобучения и так далее как устроен бизнес процесс на всей доступной истории мы отсчитываем питательность каждому атрибуту обучая модельку на каждую фичу как минимум такие модельки очень маленький поэтому смело можно обучать вообще на все на весь наш вечер stor при получении данных по новому временному срезу снова считается уже на обычной модели или перри обучается модель если какие-то сильные изменения пришли и применяется функция ранжирования на основании аналитические установленной ранее аптечки мы отдаем как какую-то часть среза на проверку именно топ n переменных для каждого кандидата на проверку сработает триггеры и подсказки алгоритм используется также для приемки новых течений здесь наверное у вас не может быть возникнуть вопросы а почему какой от этого просит ну во-первых уже с этого слайда видно что профит что у вас допустим есть дашборд где вы видите описательные статистики но даже бортики я вам покажу но представьте что у вас 5 тысяч свечей и вывоз приходит новый средств и человек сидит и щелкает 5000 графиков из и находят какие-то скачки да то есть мы даем конкретные уже примеры сейчас дальше к этому перейдём вот давайте у нас есть конкретные переменная да как я уже говорил это наши банковские три переменные связанные с правом гоце и здесь представьте может быть там количество поездок в такси по каждому человеку здесь может быть количество посещенных сообществ и так далее в зависимости от business unit а у нас есть допустим 5 тысяч переменных условно говоря за 140 месяцев то есть вот у нас грубо говоря должно быть 2 миллиона записи или сколько там 200 тысяч не знаю вот у нас есть витрина фьючерс торт и мы его отранжировали по score балу нашего аномальности то есть вот смотрим топ n переменных и видим что вот такие перемены нас попали там стали самыми аномальными давайте посмотрим на эти фичи и что мы то что с ними произошло смотрим первую здесь самое интересное вот у нас конкретный пример работы алгоритма от месяца к месяцу мы видим что у нас резко снизилось среднее какой-то месяц количество уникальных значений выросла и так далее но мы стали разматывать это кейс и видите что у нас просто тупо не добежал с источника какое-то огромное количество данных соответственно ну то есть на все буквально на всех переменных со стола просадка тут уже можно сделать вывода что даже проверки идти не отработали и и дали добежать вот целая перемены и посчитаться вот так вот но вы представьте до что было бы с моделью которая получила бы такие данные а если мы еще и строи строим на этих фича какой-то трансформер или там к интим беден то он был вообще бы вы куда-то в космос улетела давайте кейс номер два кисть номер вас здесь наверное произошло резкое изменение бизнес-логики мы его так и не до копали но видим что резко увеличилось среднее стали но те кто вовсе увеличилась при этом количество пропусков осталась на том же уровне увеличится стандартное отклонение и видимо в справочнике характере но немаловажно что еще следующий месяц тоже стал так подсвечиваться грубо говоря желтым что как бы кажется что это и не единичный случай и что-то явно произошло на системном уровне или где-то в справочнике явно что модели работающий на этом признаки которые там нужно бы переобучить или как минимум там на них посмотреть повнимательнее но и самый простой кейс когда у вас просто фича вылетает тоже все отлично подсвечивается от месяца к месяцу все нулями ну что тут полностью вся переменная в на нах но пропусках это как видите выглядит наш скоро интерпретируем асти на основе которого мы даем к подсказке да то есть но я уже расшифровал что но вот просто показать эту лист скоро интерпретируем асти то есть видим что когда у нас вылетает фича минимум почему-то максимум подсвечивается но здесь мы можем судоку бизнес правилами подсветить количество уникальных значений когда у нас происходит бизнес-логика и так далее вот как то так это наш дашборд качество данных собственно вы скажете что здесь такого появилась благодаря нашей модели ну во первых вот этого списочек который здесь выпал он уже от ранжированный в конкретном месяце по аномальности то есть мы уже не смотрим вот так вот тупо все графики артом человек знаешь что ему надо смотреть первые 20 график по аномальности вот он идет уже ему не нужно вот эти терапии все пять тысяч переменных каждый график осматривать ну и собственно слева есть значение так для вас и тогда тоже слева а именно на какое там такие перемены ему надо смотреть вот здесь скоро интерпретировать мы тоже для каждого графика подсвечиваем еще кстати идейка что пони сортировались по скорой winter цитируемости мы делаем ну и плюс графа надо уходить что она не очень красиво выглядит ну ладно это уже техника так как мы хотим развиваться дальше от бизнеса пришла задачка как оценить качество рассчитанного среза целиком но на ум приходит идея через край интерпретировать и усреднить загар за каждый срез но это так с виду не очень хорошо делать надо как-то делать умным вот мы сейчас думаем как сделать но надо добавить простые проверки описательный статистика на правилах но грубо говоря вот кейс номер 3 да я вам показывал и подобные кейс можно находить параллельно еще и тем что просто установить что у вас количество пропусков равно количеству общих числовых значений там да и не надо поставьте модельку дверь этого но моделька плюс правила будут работать явно лучше чем просто модель филипп или просто плюс правило а про эти правила пишется легко так дальше мы хотим расширить список расширить список описательных статистик для работы алгоритма хотим чем такое интересное прикрутите паску фактор или там какой-нибудь меж квартир на расстоянии но это надо на там нам на ходу чеки написать java вот эта вот агрегацию ну что это нестандартные риску или иные функции я напомню мы считаем склоним чтобы посчитать это очень быстро и не считать это в питоне не загружать целый клиентов целиком и так далее дальше планируем развивать систему подсказок что конкретно пошло не так и уже давать подсказки что наш куда тебе идти что с этим делать уже конкретному человеку их и конкретно в к и делаем уйти какой-то источник возможно и так далее зависимости от бизнес замена переменных увеличение блок гранулярный хотим прекратить кинг кластера чтобы еще не только внутри как бы внутри среза читайте внутри некоторых кластеров клиентов чтоб как бы вот как я говорил были минусы что мы не есть что-то не умеем отлавливать куда на ком-то этом микро сегменте произошло вот собственно с этим боба росса чтоб как бы добавить дополнительно гранулярный от этого группировки но дальше просто автоматически рассрочки параллельным из мы вместе с этим алгоритмом сейчас следуем подход с временными рядами ну грубо говоря у нас есть какой-нибудь библиотечка профит ну или как анти римаса рима и так далее и мы предсказываем на следующий месяц как предполагаемое значение сколько рассчитается переменная притом рассчитывая рассчитываем не конкретное значение границы доверительного интервала и уже дальше будет но хорошо ли хорошо сочеталось наша переменная допустим это сумма остатков по всем счетам вот он при посчитали какой-то сколько пар спрогнозировали на следующими тескоко это будет с учетом там сезонности каких-то колебаний шумы и так далее предсказали не просто значение доверительный интервал попала эта сумма депозитов наши предсказано доверительный интервал или нет вот будет тоже топ но какое то дополнительное фича к нашей общему алгоритму вот этой модели какой флажок не удар даже можно наверное как сказать дополнительную колонку сделать кругу аномальности попало не попала в доверительный интервал ну и объединить все наши старости витрины и все на едином дашборде у нас есть там уже заготовочка под в какую-то бизнес область это все рассчитано так но и и завершая я хочу сказать что вообще зачем нам нужен вообще этот рэппер и все эти модели когда у нас модель обучиться вообще на некачественные данных как бы смысла нету типа наверное я должен был сам начали выступать а потом уже про модельки но вот видимо закрыть в заключение хочу сказать так ну и вот у нас такой некий хайло по данным всем спасибо буду рад ответить на вопрос а где нет спасибо большое ребята задавайте вопросы ценить руку есть ли у нас ? давайте пока вопросов нет наверное я задам вопрос здесь я вот много раз в своей практике видела как люди значит сделали какой-то поиск аномалий своих данных один раз аномалию наш я больше короче это дело бросали вот вопрос как вы от внедрили в продакшен регулярно используется насколько я поняла из доклада сейчас это происходит из как этап и рэнди он закон закончена он подтвердил свою результативность то есть конкретные вот взял и там наш вид наш петерстар витрин к его проспорил грубо говоря вот сейчас он вынес реальные данные на седанах и вот собственно сейчас там мы внедрили этого через кубер как но по законам жанра и сейчас уже учим людей чтоб они как бы но у нас просто вот текст сыграть данное не такие люди скажем так тяжело и они говорят как с этим работать а что будет если ты уволишься кто это будет поддерживать ну как бы вот пока из это по аренде мы показали что этому можно верить и теперь как бы надо из этого еще выстроить бизнес процесс что все как бы смотрели кто-то смотрел конкретно и оценивал я могу дать вам только в этом желать что я тоже хочу задать вопрос от себя вот ну очевидно что бывают ситуации когда ты какие-то аномалии ожидаешь ну даже из списка того что ты приводил там видно что ошибки в формате данных или отсутствия данных там где не должны быть то есть есть какие-то ожидания которые находятся но наверняка же были случаи когда находились какие-то аномалии которые потом можно было интерпретировать который вообще совершенно никто не ожидал увидеть в данных было ли такое были ли интересные находки в результате анализа самое интересное находка я это подсветился это резкое изменение бизнес-логики когда-то меняется состав продуктов или выходит новый продукт от об этом даже не уведомил ся но ты хотя бы об этом узнаешь то есть даже это я и сказал это наверное даже и хорошо потому что твой модели не смотрели на тот бизнес процесс который был раньше тебе наверно задуматься что уже без изменился вот такой способ понять что модель уже устарела и надо ее менять тело бизнес поменялся бизнес менялся бизнес стали писать в тоже поле что-то другое никто об этом не знал появился новый продукт который там допустим или начала писать и начал писать что-то свое да нет просто или изменение распределения изменилось стали больше покупать а они тоже но там распределение не может гарантировать как бы те качества то качество которые на котором распределение она не видела спасибо большое я дойду хотел спросить про такой вопрос рассматривали ли вы ситуацию когда данные представляют собой временной ряд и если да то пробовали использовать всякие подходы из области чем шплинт detection и так далее чтобы выявить такую ситуацию когда аномалия заключается просто в том что свойства процесса изменились ну вот я говорю чтоб вот подходит сейчас тестем именно конкретно библиотечка и не будем называть компанию под названием просто для анализа временных рядов и краз будем смотреть по доверительным интервалом наверное это нормально для вот этого подхода или не про то имели ввиду нет так тоже можно но от себя могу сказать что наш опыт использования профита ужасен грация это хорошо распиарено и я когда от не называемой компании но ее эффективность очень плохо это очень узкий класс временных рядов и процессу где она хорошо отрабатывает могу посоветовать попробовать использовать html он хорошо работает мы красных и делаем секрет ну как конкретно мы в лаборатории разрабатываем фреймворк федот который позволяет временные ряды прогнозировать но есть другие библиотеки там автор с еще что-то тоже будет интересно посмотреть на спасибо большое так ну что давайте еще раз похлопаем докладчику denis"
}