{
  "video_id": "xyMN1EA9p5Y",
  "channel": "HighLoadChannel",
  "title": "Базы данных в облаках / Владимир Бородин (Яндекс)",
  "views": 1887,
  "duration": 2380,
  "published": "2019-05-15T04:17:18-07:00",
  "text": "всем привет зовут меня владимир я в яндекс облаке занимаюсь базами данных вчера я рассказывал в целом про индекса блага только что рюрик рассказывал про instance группы я буду рассказывать про базы данных причем не только в одном индекс о благе которые мы недавно запустили в том числе про наше внутреннее облако в котором этот сервис начинался доклад во многом такая история как мы к этому пришли с какими граблями столкнулись и как их решали давайте с этой самой истории начнем я пришел в яндекс в 2012 году тогда это было такое феодальная русь много разных сервисов совсем своим свои отдельные сервера свои команды там разработки и эксплуатации тестирования ну и понятное дело со своей инфраструктурой для деплоя для хранения данных для всего на свете и у этого были плюсы но прибылей очевидные минусы мало 4 использовалась к году примерно 15 16 у нас случился общей на всю компанию объектный сторож куда переехали вот все все все там я не знаю почтой диска и все другие сервисы которым нужно хранить какие-то объекты она уехала в одно место у нас случился общей на всю компанию мо придешь и случилось случились облака для запуска стоит вас приложение их случилось больше одного по историческим причинам ладно тем не менее как бы у-у-у типового разработчика индекс была возможность запустить за тепло и свой сервис вот в одном из этих облаков положить данные в объектный сторож если ему нужно там что-то молотить и считать был мой при deus а вот чего-то такого куда можно как сервис прийти и положить то что обычно кладут в базы данных этого не было и в том же шестнадцатом году мы перевезли метаданные яндекс почты из oracal в поиске я например об этом рассказывал на холоде в 2016 году можно почитать на хабре или видео посмотреть вот у нас появилось довольно много экспертизе в паз гресь и и как бы ну такая довольно громкая история получилась в том числе внутри компании к нам стали приходить разработчики яндекса и говорить поднимите вот базу не только для почта еще вот для моего нового сервиса или там уже существующего мы хотим к вам переехать но мы подумали подумали решили что вот подход которым мы жили с почтовыми базами он такой плохо масштабируемый на сильно большие порядке когда мы внимательно следим за схемой внутри базы какие там делаются запросы помогаем их тюните так далее мы решили что надо делать что-нибудь и за сервис тогда это было особенно модно но и вот поскольку мы знали позарез мы решили делать прогресса за сервис причем мы концентрировались исключительно на нем фокусировались по двум причинам если ну первое как я уже сказал у нас было очень много экспертизы уже к тому времени во вторых внутри компании чем меньше технологии используется тем на самом деле с некоторой точки зрения лучше но унификация помогает бежать быстрее ну и когда мы решили такой сервис сделать мы сформулировали для себя следующую постановку задачи то есть что мы хотим чтобы этот сервис решал ну кроме того что ты можешь там через api или по кнопке запустить базы данных у тебя должна быть возможность ее вертикально масштабировать ну и по ресурсам и там по месту на низкий доступному этой базе конечно же сервис должен заниматься обновлениями и минорных и мажорных версий конечно же сам сервис за вас должен делать резервные копии давать возможности из них восстановиться на произвольный момент времени должен обеспечивать high visibility в яндексе тема важная дата центры выключаем регулярно для учений и конечно же сервис это должен переживать ну потому что базы данных в основном в real time вещах использует сервис должен предоставлять какие-то инструменты мониторинга чтобы понимать что вообще говоря с базой происходит ну и разумеется должна быть возможность некоторые настройки крутить управлять там пользователями базами данных extend шинами еще чем-нибудь вот мы значит сформулировали начале это делает дизайне делать но пока мы значит дизайнеры и делали к нам все еще приходили разработчики из разных углов yandex.ru и говорили что ну вот пока вы там еще правильное хорошее решение не сделали поднимите нам пожалуйста вот руками то маленькую башку-то ну не знаю гигабайт на 10-100 вот и мы на на много кушать просить не будет и в общем вы нам сильно облегчите жизнь но мы подумали подумали решили сделать такой костылях который назвали коммуналке значит что они себя представляли в очень простую штуку берем и в один кластер под gresso засовываем несколько баз данных разных проектов now that is на разной базы разные пользователи праздная аутентификация ну и каким-то образом пытаемся их разделить чтобы они друг другу не мешали понятно что раз уж мы их внутрь поселили то не все можно разделить но то что можно нужно например сделали там hard лимиты на количество соединений пир база софт лимиты по занимаемому месту процессы одной базы загоняли в одну фигуру пуэ и там урезали по ресурсам вот тут на мониторе почти ничего не видно я думаю здесь тоже по сути единственное что пользователя было кроме вот непосредственно строки подключения которые можно подключиться из базой работать это вот там графики больше на самом деле из тех задач которые мы себе поставили этот сервис мало что выполнял ну и у него очевидно было много проблем самая основная из них это отсутствие нормальной изоляции ну например если у вас там в в одном под грешном кластере не знаю там 10 баз по 100 гигабайт и 1 баска на гигабайт и вот в этой маленькой баски кто-то там взял и потёр важные данные то чтобы восстановить ее из бэкапа нужно весь терабайт распаковать это сильно больше времени как минимум я сказал что вы по месту был софт лимит потому что нормальный лимит по месту сделать так чтобы не поломать всех остальных нельзя но в том смысле что средствами позарез механизмов нет если делаешь это средствами там не знаю файловой системы в операционке там какими-нибудь кодами файловой системы то когда одна база упирается по месту складывается весь кластер ну смысле весь instance прогресса для всех вас ну и есть некоторые такие низкоуровневые под системы в прогрессе которые общее на весь кластер ну например очевидно вал пишется в один поток ну все изменения си реализуются или мы например наступили на интересную проблему с под транзакциями когда в одной базе они активно используется этот момент времени начинают страдать запросы в других базах ну потому что блокировки на под транзакции общие и когда им вести snapshot снимается любой процесс в базе даже в другой базе будет заглядывать в структуру в разделяемой памяти это будет медленнее но и кроме того мы еще обнаружили что си группа может прижимать процессы базы данных в неподходящих местах например не знаю какая нибудь базы которая там подбирается к лимиту по циклу берет про карлаг тот момент времени все группа и и прижимает и все остальные ждут когда она и и освободит она не освобождает потому что прижата вот ну и еще много всяких с ними было проблем но мы как бы всех кого пускали сразу и на входе говорили ребята это костыль мы его выкинем и когда мы сделаем нормальный postgres и за сервис это будет ваша головная боль туда переезжать с downtime а мы в общем короче это ваша зона ответственности из плюсов но она там упростило жизнь другим командам вообще по ресурсам очень дешево позволяет сильно номер комитет самый главный плюс для нас который мы получили это то что мы осознали потребности ну собрали фидбек условно говоря не там от одной команды яндекса от множества разработчиков разных сервисах фидбэк получился очень разносторонне но в целом мы понимали что касты который выкинем ну а потом как обычно пришли к тебе с базой в 100 гигабайт какой-нибудь маленький сервис не знаю там яндекс такси я потом он вырос в большой сервисы уже так просто его бизон тайма ну надо как-то без downtime перевести в нормальные базы и в общем мы потратили какое-то количество времени на то что когда вот сделали с грейс келли за сервис перевозили пользователей из коммуналок вы то это сверло какое-то количество ресурсов очень ненулевые но собственно это вот все там примерно не знаю шестнадцатый год условно говоря дальше мы написали уже вот эта картинка с внутренней виде на ней скорее всего ничего не видно самая важная часть вот она для пользователя есть две точки входа первое это на 6 яппи через которое можно базами управлять ну типа создать удалить что-то в ней поменять ну и это вот одна точка входа для управления ну а вторая точка входа это уже подключение полип пышному протоколу непосредственно к базе но только здесь подключение идет не непосредственно к базе а через погас прокси сейчас я про это расскажу значит чтобы все это случилось нам потребовались внутренние платформа для запуска контейнеров и внутренней инсталляции объект на вас тораджа но ис-3 совместимого мы туда делаем бэкап и для изоляции используем порта контейнеры но они у нас в яндексе повсюду это удага и самописных докер ну потому что докер и тогда еще не было когда писали вот для резервной копии используем open source tool волдырь его андрей с володей вчера рассказывали доклад про то как мы его развиваем для отказоустойчивости самописная решение написали звуки пиром на хостах для управления есть погас агент и вот та самая погас прокси про два последних сейчас расскажу подробней что делает погас агент он запущен в контейнеры с базой постоянно ходит в нашей api и спрашивает а какая вообще желаемая ревизия состоянии там этого hasta кластера сравнивается 100 ревизии которая у него последняя применено если они отличаются то получает div и этот div применяет наименее деструктивным образом что это значит но если там меняется какая-нибудь настройка которая не требует restart от он там приводит позарез а если он там меняет какой-нибудь не знаю шарит буфер smax connections или что-нибудь что требует перезапуска то соответственно перезапускает что такое погас прокси это вот но собственно прокси липы кучного протокола которая скрывает от пользователя некоторые детали реализации и дает ну вот для базы которая там например называется тест тебе дает три строки подключения тест baby смотрит всегда в мастер ну а если он умер то поворачивает его всем храм на реплику тест бибера sing смотрит всегда в синхронного реплика если с ней что-то не так поворачивает и в мастера и тест gpl около смотрит в ближайший host ну чтобы если там тебе нужно что-то почитать и и из локального дата-центра это можно сделать ну а если ближайший хвост мертв поворачивает его синхронно реплика это удобно для пользователей потому что ну там снимает часть из них головной боли вот ну и кроме того скрывает некоторые детали платформы платформа поскольку она контейнерами в основном для stateless приложение то при перезапуске контейнера меняется и вдн айпи адрес и соответственно вот без такой прокси для пользователя много более получается технически это очень простая штука это набор какой набор встретилась машин за балансиром каждый из них палит базы знает тот кто кто кто там в каждом кластере мастер кто реплика кто жив кто мертв кто насколько отстал одно и соответственно меняет конфигурацию пиджи bouncer в которой как раз соединение проектируют ну и таким образом поворачивает разные строки подключения в разных с ты мы когда эту штуку дизайнеры мы сразу там нагрузочное тестирование провели я созналась что вот в той реализации в которой мы ее сделали больше чем на 10 тысяч кластеров на один погас прокси она не от масштабируется но на один кластер пгс просто нужно будет архитектурно многие штуки меня ну просто потому что она раз в секунду все хасты палят 10000 кластеров о до 30 тысяч постов дальше уже там возникают всякие нюансы с и полу ну а кроме того у нее есть граблей опять же из-за недостаточной изоляции поскольку это одна точка входа на всех там есть вопросы с точки зрения безопасности но до этого балансира довольно быстро появились дырки и примерно из всех сетей яндекса не совсем достаточно изоляция по ресурсам если какой-то один клиент сошел с ума и на открывает там куча tcp соединений то идите себе соединения кончиться для всех ну и накладывает ряд неудобств для пользователей ими базы должно быть уникальным ну и еще есть там интересные нюансы например если ты проектируешь какой-то длинный заброс например пользователь заливает дампа базы до то может случиться так что вот у тебя базы упирается в ресурсы по максимуму тебя погас прокси палят хост и но не получает от него ответа на холсте решает что он мертв и поворачивается в синхронную реплику соответственно соединение рвется и получается что длинный дам залить вообще нельзя если ты упираешься в системные ресурсы на маленьких базах но с небольшим ресурсам довольно плохо работает но собственно тем не менее мы такое решение сделали там какие-то сотни кластеров к нам у нас появились там всякие разные сервисы яндекса стали использовать но вот особенность этого решения были такие она умела только позарез из коробки она настроена вот не так как там в не знаю у большинства конкурентов там типа rds google cloud иски или это и так далее она уже затюненная под уаль типично нагрузку там довольно высокую но в смысле там не дефолтные настройки мы их подкручивали сразу вот но и в этом решении многие вещи были сделаны средствами субд и из-за ограничений платформы под нами ну потому что как я уже говорил там при перезапуске контейнера и пи адреса и выкидываем меняется не было сетевых дисков только локальные но и там еще ряд ограничений а потом пришел собственной on который вчера рассказывал про яндекс облака и сказал что нужно не только под грез и virtual как и вообще в других сетях и все по-другому вот ну и соответственно случилось 3 транса у нас было примерно два стула делать всё заново или пытаться адаптировать код так чтобы уметь жить на этих самых двух стульях какие тут проблемы но во первых разные системы виртуализации ну смысле в одном случае вообще контейнеры в другом que ему виртуальные машины разная сеть в том плане что в яндексе внутри вся сеть она и пиво 6 он ли уже сколько-то времени люди ну вот во внешнем облаке большая часть людей все еще использую этой пилы четыре разные провайдеры аутентификации рисовалки графиков разная api для dns а редактирование ну и требования безопасности более жесткие во внешнем облике чем внутри собственно ну вот две основных проблемы разные платформы и разные типы субы да и как мы их решали значит с разными платформами мы сделали такой костылях немножко ненатуральный мы стали в контейнерах запускает не вот как контейнер там один или небольшое количество процессов a brem полноценную операционную систему как в виртуальной машине вот но и соответственно deep ловится туда уже не этими самыми образами контейнеров м юта был infrastructure как это модно и правильно стали тепло и ца туда также как и в виртуальные машины с ул стеком собственно solstic мы использовали для почтовых баз до того как стали строить погас вот в пугайся стали делать свой погас агент я про него рассказывал стали моего делать потому что ну более гибкая штука с одной стороны но с другой стороны она много ресурсов у нас от идола ну то есть на самом деле мы там реализовали примерно ту же функциональность что есть в стеке и с учетом того что нам теперь понадобилось поддерживать множество баз мы решили что дешевле и и вернуться к использованию инструменты которые развивают за нас ну и все равно там по возможности все различия стали убирать какое-то количество их небольшое осталось их там спрятали за iv чеки и вот научились жить на двух платформах с разными типами свободы у нас было был тоже выбор пойти как один из наших конкурентов и фыркать там control plain каждого каждой новой базы ну не будем показывать пальцем for control plane каждой новой базы в отдельные сервисы с нуля его развивать или написать один общий на всех вот второй вариант сильно более сложной но мы все таки пошли этим путем потому что на наш взгляд он правильно вот новая картинка с вики я думаю здесь все так же ничего не видно все также основная часть она примерно такая есть сколько то теперь уже больше одной точек входа для клиента ну там api веб-интерфейс ecли но суть их такая же это все еще какое-то api для управления базами непосредственно для менеджмента ну и вторая точка входа это уже непосредственно подключение к самой базе на этот раз мимо погас прокси непосредственно к базе вот все то что для управления мы называем control play нам все то что сами базы запущенное в виртуальных машинах мы называем do to play ну дальше нам это пригодится значит когда есть один postgres очень простая модель данных есть кластер в этом кластере есть хасты ну там на них запущен подогрев вот а когда мы стали думать какая нам нужна модель данных для например клик хауса для например манга' и там еще ряда других решений которые пока не запущены мы осознали что ну не хватает ввели вот такую модель данных у нас кроме кластера есть наверное наименее понятная сущность сам кластер дальше из шарды хасты и на них сервисы проще всего на примере показать вот если мы говорим про кластер манга деби кто когда-нибудь работал с mongo db и представляет себе ну в общем опять же есть хасты которые ну вот которые хранят данные это вот та что справа man гадисов кластер он может быть ордера ванну там может быть несколько шагов вот а есть sap кластер отдельных машин на которых запущены конфиг сервера и man vs в общем то получается что разные со plaster и внутренних шарды внутренних хасты и на одном посте может быть больше одного сервиса на хвостах справа там сервис например непосредственно манга дебиана костяк слева два сервиса конфиг сервера и лемонграсса такая же картина маслом например кликал сам где есть отдельный sap кластер звуки пера для репликации если вы отказоустойчивые инсталляцию использовать ну и для того чтобы вот все эти нюансы вместить мы на самом деле отошли от практики свидетелей и третье нормальные формы мы иди нормализовали но и вещи в базе в дальше например для прогресса он отправим лежала там имя пользователя там зашифрованы и его пароль там лимит соединений какие у него есть гранты и так далее во всех базах эти структуры разные и мы вам местами эти вещи где нормализовали джейсон вы положили но пользователи базы там extensions например для клик крауса внешней словари и так далее в купе с не типизированным api это нам позволило очень быстро этот сервис получить про api вот это кусок из swagger спеки крестовая пи которая у нас была или но там есть некоторые опции которые общий для всех типов зубы д ну например там сколько ресурсов какой размер диска и так далее а есть опции которые специфичны и в зависимости от типа субд и и вот в спике это просто database общем ты какой-то там словарик внутри а какой не очень понятно вот например для прогресса для того чтоб там поменять волновал нужно было вот так вот прийти вопи а для манга деби чтобы поменять там не знаю размер кэша врт гера нужно было передать вот такой джейсон в.п. в общем то rs plus джейсон для пользователей штука довольно понятная но в отсутствии примеров и внятной документации и не очень понятно как этим пользоваться хоть это и удобно реализовать сначала собственно я вчера упоминал что эти и облаке менялась трижды прежде чем мы показали его людям и в конце концов мы пришли к тому что api все-таки должно быть типизированные типизированным его в джерси 6 этаже предоставляется но все-таки мы основную ставку делаем на джер писи и вот то как та же задача решается теперь во первых ну вот красненьким выдели наша матерь для разной базы свою api ну вот в данном случае для паз gresso и на вход принимается мисочки попадает кластер request если мы посмотрим внутри этого места же там есть какой-то набор полей и вот одно из них там config spec прям конкретный тип если мы посмотрим на содержимое этого места jetta во-первых у и нишу для разных версий под греться мог быть разные настройки потому что они имеют право быть обратно не совместим и но если посмотрим вместо уже непосредственно конфига для десятой версии прогресса то увидим что вы там например макс connections это обычно and волновал это вот прям строго типизированные поле которая принимает там набор из двух значений но либо волновал реплика либо волновало jackall это удобнее с той точки зрения что глядя успеху можно понять вообще что говорят твои и умеет принимать и как с ним работать то есть многие вопросы решаются просто заглядывания в песках даже без документации ну она опять же единообразно совсем облаком но эта штука потребовала от нас значительно больше работы в выделении общей кодовой базы которая применима там ко всем субд э то есть можно было там просто скопипастить но мы все-таки абстракции выделяли и вот в этом сценарии это было значительно сложнее самое главное на какие грабли мы наступили или наступаем и как их решаем первая самая популярная проблема эта нехватка ресурсов я не знаю видно тут что-то или нет это график загрузки циpкa то на мастере по моему одного из кластеров и вот тут видно что он очень небольшое у него лимит два ядра все 2 используется и при этом на самом деле еще 90 там по моему процессов стоит в очереди за ядром есть такая метрика у порта контейнеров сколько процессоры не хватает условно говоря ну то есть процесс в полку съеден скорее всего сервис себя чувствует не очень хорошо но встречаются такие сервисы которые говорят нам нормально ну то есть пускай базы отвечает медленно главное чтобы короче работала вот ну сервису то может быть и хорошо но внутри в контейнер из базы еще запущены наши обвязки для этого авто fila вера для бэкапов для мониторинга вот для всех тех для выполнения всех этих задач про которые самом начале я сказал и они вообще говоря в такой ситуации работают плохо когда им процессоры не хватает к счастью ну или вот еще одна ситуация тут еще хуже скриншот потому что это на продакшене была поймана не часто такое случается на чем тут потребление памяти доходит до предела приходит ум of the fall over переключает мастер и на новый хост снова память растет туда снова идет нагрузка снова память заканчивается приходит of the fall over переключает на следующий хвост и так она по кругу то есть веер на друг за дружкой хасты переключаются между друг другом собственно упор то есть две очень хорошие штуки она умеет вложенные контейнеры и она умеет гарантии некоторых ресурсов ну например процессора и памяти значит что мы можем все наши обвязки положить в пот контейнер гарантировать ему какие-то ресурсы ну не знаю там 10 процентов циpкa от того что доступно верхним уровнем контейнеру и когда они ему нужны он их будет получать приоритетнее чем процессами за когда они ему не нужны база сможет их переиспользовать это очень удобная штука мы так сделали перестали с этим страдать с ломом ситуация более сложная и и мы сейчас вообще говоря никак не обрабатываем то есть если у пользователя кончается память будет случаться вот это вот верно и переключение мастера у нас есть планы там того как это делать но к сожалению эти решения уже не на уровне операционной системы там специфичные для каждой субботы получится вторая проблема это когда какие-то потенциально деструктивные операции с кластером выполняются очень часто ну например у тя там есть кластер не знаю мастер две реплики и ты берешь и начинаешь то добавил хосту залил хвост был еще два удалил еще три и дай и и так далее вот вообще все операции над одним кластером выполняются у нас строго последовательно то есть сколько бы ты там через api и изменение не надергал они все будут друг за дружкой последовательно для одного кластера будут выполняться последовательно но есть нюанс в том что в control play не в нашей мета базе мы храним целевое состояние обновляемого сразу и получается ситуация когда более ранняя операция мой увидеть целевое состояние от более поздний и и сломаться по этому поводу ну например вот у нас в control plane есть кластер из 3-х став от vga to play не есть три актуальных х стояли там хоть один мастерхост 2 и 3 реплика например пользователь решил хост 3 удалить вот он из control play на его удалил в дата плане он ещё пока есть ну то есть вертак все еще запущена все еще работает хоть она уже не находится в актуальном состоянии что происходит дальше дальше моя синхронно эту задачу выполняем удаляем has three накатываем необходимые изменения на хост 2 они есть ну например там есть вырвали разрешающие правила для соединения из has the three их нужно удалить потому что бы безопасность вот и дальше будем катить на хост 1 но перед тем как мы приступаем к диплому на ход один пользователь берет и в control plane на клики вает еще один хост 4 и собственно когда на хост один будет случаться диплом мы должны будем добавить разрешающие правила для соединений с 4 х 100 его еще нет и этот диплом очевидным образом свалится равно как и попытка за тепло и что-то на 4 хост тоже свалится потому что он еще не создан пока мы сделали ровно также как сделано примерно у всех конкурентов приклеили подорожник и сказали что если ты пытаешься сделать потенциально деструктивную операцию то подожди пока завершится предыдущие вот вообще делаем versio не рование на стороне control play на чтобы не было ситуации когда чтобы можно было в очередь на ставить там сколь угодно много изменений следующая проблема это внезапно выяснилось что не все настройки базы одинаковые но тоже как я уже говорил есть те которые требуют 3 лода для применения есть те которые требуют рестарта но и это еще не все есть те которой зависит от доступных ресурсов время нельзя поставить там shared буфер спас грейси больше чем есть физической памяти иначе бы она сломается не взлетит и разрешать такое вопи нельзя или есть настройки которые требуют специального применения но порядка применения на разных has так давайте рассмотрим пример есть например настройка макс connections сколько можно открыть соединение кластер вот у нас опять такой же кластер the master две реплики типовой для нас и везде стоит макс connections 512 если пользователь пришел и через api я решил поднять там до 1024 и мы выкатили это в первую очередь на мастер то случится то что это изменение доедет через волна реплики и они свалятся вывод такой ошибкой что макс connections на мастере больше чем стоит на репликах реплики свалятся очень нездоровая ситуация собственно как вообще говоря это делать правильно в случае вот когда мы это делаем мы должны сначала задрать это репликах только потом задрать на мастере это в случае если мы забираем эту настройку вверх а если мы уменьшаем эту настройку то мы должны соответствовать сделать в обратном порядке сначала на мастере потом на репликах и вот таких вот немцев их довольно много по вылезала и но мы там разработали несколько стратегий того как можно где площадь изменения вот есть самая простая которая deployed на всех аст и параллельно есть та которая тепло и последовательно на мастер потом параллельно на все реплики ну и есть которая сначала на реплики потом на мастер тепло и собственно api когда ставят задачу маркером для асинхронного выполнения она в зависимости от того что пользователь поменял указывает какую политику применение здесь нужно использовать собственно какие то итоги значит вот если говорить про внутренняя инсталляцию то с момента как мы там в шестнадцатом году погас запустили и там дальше его допили вали под другие базы ipad то чтобы уметь работать и с внешним облаком тоже тем не менее вот те базы которые там есть они уже там больше двух лет в продакшене ну и там уже больше чем 1000 кластеров тысяч полторы полном последний раз когда я смотрел бредом мы добились того что у нас одна кодовая база и для внутреннего облака и для внешнего и что еще более ценно один control play on для всех субботы ну вот для тех трех что уже запущены и для тех трех что еще в разработке при этом вот та история через которые мы прошли она имеет ряд особенностей относительно конкурентов с которыми вы возможно работали ну там не знаю с rds он с google cloud с гелем с ажуром еще с кем-нибудь во первых мы не отдаем дефолтные конфиге мы их сразу тянем-тянем в зависимости от доступных ресурсов но и исходя из нашего видения прекрасного ну и вообще это зависимости не знаю от типа диском и там покрутим вам брендом . в прогрессе и еще какие-то параметры во вторых авто fila веры бэкапы сделаны средствами субд и они средствами нижележащих там сетевого блочного 100 раджа или там еще чем-нибудь но во первых это там с некоторой степени продиктовано там условно говоря ограничениями которые у нас были с одной стороны с другой стороны многие вещи можно сделать более гибко вот например вчерашний доклад андрея и володе провала вал дельты это вот как раз способ значительно сократить чтение при бэкапов но учти нее с диска при бы капах это можно сделать только средствами базы на уровне там какого-нибудь блочного стороны этого сделать нельзя мы сразу даем пользователям нормальной графики потому что если их не дать так тебе будет приходить вопросы а что не так с моей базы и так далее в общем тут инструменты диагностики на мой взгляд у нас немножко лучше мы умеем и сетевые локальные диски вот ну и у нас есть вот прям ряд особенностей таких которые на первый взгляд кажутся очень странными но во первых вот это самая уникальная модель дано смеши у нас есть кастера внутренних сопла стирая внутренних там шарды хасты ну и мы не даем возможность например создавать базы или пользователей там с помощью кредо the bass или crate ролл это нужно делать через api тоже по первости как некоторое ограничение для пользователей выглядят но зато защищает от многих потенциально опасных штук ну и вот если совсем коротко то внезапно сделать api для управления базами данных мы вообще в принципе как это стоит full часть вашего сервиса сделать как сервис это не очень просто не повторяйте это в домашних условиях если у вас есть желание то можно уже попробовать во внешнем облаке но и у меня вроде бы все спасибо вон там есть вопрос владимир спасибо большая хотел про масштабирование спросить от коллег до вас рассказывал что есть единственно с группы приходит нагрузка увеличивается размер и все такое но это хорошо для стоит стоит ли свищей если мы говорим про скейл баз данных ну конкретно как болит по сгрыз там и прочим всего родни и с этим что scudo христианину продать ну прямо сейчас instance группы с управляемыми базами данных никак не провязаны но можно как минимум провязать в смысле вертикально масштабировать instance там зависимости от нагрузки там под ступая по gps и возможно даже почему надо принципиально здесь никаких технических ограничений нет если говорить про горизонтальное масштабирование то для 200d которые это умеют и у нас сейчас запущены для клик хауса и mongo db мы сейчас у нас стадии тестирования во внутреннем облаке sharding но в смысле эти базы умеют и мы через api даем возможность там шар даме управлять ну смысле их закидывать удалять но в аспекте масштабирования нагрузки оно вам ничего не даст потому что мы ничего не знаем про ваши данные но про вашу схему про то какие то данные лежат и как вы с ними работаете поэтому просто так там взять не знаю докинуть sharp начать таскать данные между ними и там и но точно не в ближайшие пару лет ну вот такая моя субъективная оценка ну то есть тут еще нужно крепко подумать как это правильно делать спасибо добрый день подскажите пожалуйста при использовании вашего облака можно будет привязать сервис географии для того чтобы ближе к пользователю данные были спасибо но ответ скорее всего нет смотри есть один регион вот прямо сейчас запущен центральная россия как уже упоминала рюрик и в этом регионе дата-центры там в пределах миллисекунды ну миллисекунд единиц миллисекунд друг от друга поэтому здесь в плане географией не очень понятно что можно выиграть в плане хранения данных тем более непонятно допустим у тебя есть какая-нибудь там база данных растянутая на несколько дать регионов но вообще говоря недостаточно просто размазывать запросы по гео привязки ну там не знаю например у тебя реплика которая там стоит через океан она может быть сильно отставшая и это твой о приложение должно решать можно из этой реплики сейчас читать или нет но тут как бы это трейдов и которые должен делать сам пользователь сервис это хорошим образом сделать качественно мне может не вопрос спасибо за доклад вот если вот ваш сервис да там базы данных в облаке да естественно виртуа виртуально все дано на пора на вашего аппаратном обеспечении соответственно там несколько баз мы какую-то взяли прям кто-то другой взял другой оба соответственно высокую нагрузку на вот вывод делает да там fuse скан и так далее естественно и моя хочет читать диски естественно как то это можно разрулить там и так далее можно и это разруливает смотри есть диски и сетевые есть диски локальные в случае локальных дисков но это локальная порезанные на через сервер вот и там прям есть разграничения по полосе условно говоря доступной если это сетевой диск то это средствами нашего программного обеспечения ну вот той штуке которая у нас эти сетевые диски реализует можно сделать но пока вот прямо на сегодняшний момент это пока не реализовано спасибо ну в смысле мы это обязательно будем делать ну прям сейчас нету"
}