{
  "video_id": "BtmYjTO1EpI",
  "channel": "HighLoadChannel",
  "title": "Брокер сообщений Kafka в условиях повышенной нагрузки / Артём Выборнов (Rambler&Co)",
  "views": 60858,
  "duration": 2269,
  "published": "2018-01-16T13:18:29-08:00",
  "text": "когда я кому-то говорю что работаю в Рамблере на самом деле люди отвечают довольно однотипной популярность ответом является О я знаю Рамблер это поиск на самом деле мле уже давно не поиск Мы сейчас конкурирует с бигом это вообще неинтересно на этом графике более Интересно что Google наконец-таки Обогнал Яндекс А какой по вашему мнению самый популярный ответ на то что я работаю в Рамблере Да вот услышал правильный ответ Рамблер ещё жив на самом деле Да Лер жив сейчас это крупнейший вте медиахолдинг в него выходят такие известные довольно ресурсы как мблер Лента газета афиши чемпионат жей журнал много-много других месячная аудитория составляет более 40 млн человек ну преимуществе всё какие-то Медиа ресурсы мы на них заход пользователи чтобы по новости почитать статьи надо-то за счёт рекламы для этого для того чтобы это делать наиболее эффективным образом несколько лет назад ле создал отдел рекламных технологий вот собственно я в нём работаю мы собираем данные со всех этих порталов с целью извлечь какую-то полезную информацию чтобы подороже показать рекламу вот мы собрались всех порталов информации что же мы с ней делаем во-первых мы занимаемся сегментации аудитории То есть к примеру класрум пользователей по полу возрасту определяем Поль Какие интересы к примеру каким Какие Люди интересуются там не знаю там автомобилем Ford Кто собирается поехать в отпуск в следующем месяце находим аудиторию похожую на аудиторию какого-то более маленького сайта мы проем трафик предсказываем р мы строим какие-то рекомендации аналитику и многое-многое другое но прежде чем все эти довольно полезные действия совершить нам нужно получить данные для этого раньше у нас был на самом деле целый зоопарк способов здесь проведены самые популярные из них у нас был какой-то Кер на самом деле их было много зде на рикен один который забирал на себя данные локально их складывал а затем перекладываем на спарке после того как обработаны мы хотим получив информации поделиться с окружающим миром нам нужна для этого какая-то база мы используем вот вро хоро самом в этом подходе тся новы каж нового парт это решение отдельной новой задачи как этого парт для на источника данных так со стороны нас потребителей во-вторых это вот здесь озна Как сервер на самом деле этото набор воркеров Но каждый файл должен бы сча пройти через одну ноду где быть сохран на диск а потом уже выгружены Ну предположим нам какой-то партнёр начал выкладывать данные микробами там по одной минуте мы их забрали сохранили на сервер Потом переложили в hdfs потом на hdfs обработали потом выли AOS Spike пока всё это пройдёт Ни о каком Реал тайме речь не идёт То есть это задержки там десятки минут или больше когда всё идёт при этом идеально А что же мы сделали мы всё что находится слева выбросили и заменили на кавка кавка - это такой брокер сообщений для больших данных а он он не обладает такой крутой семантикой доставки сообщений как mir rit он более простой но при этом он более надёжный собственно в кафку много поставщиков данных могут писать и много потребителей с неё могут читать Вот у нас есть кавка нам данные надо каким-то образом доставить до hdfs для этого мы используем Гоблин это решение от создателей кавка компании дово это это M задача которая крутится на ходу и данная ивка кладёт в hfs также у нас пось возможность наконец-таки сделать приблизиться к риал тайму мы его реализуем с помощью Спарк стриминга это такая Задачка которая постоянно висит на ходу от Кафки она откусывает микроба информации как-то обрабатывает и складывает в кве базу самое главное преимущество такого подхода Мы полностью мину hdfs то есть всё происходит в памяти Мы свки считали обработали положили в Спайк А вот При таком подходе при работе микробами у нас задержки между наступлением события и обновлением совей информации впаке там в пределах минуты про что же сегодня расскажу Я сегодня во-первых я расскажу как вообще приготовить кафку и обеспечить с помощью неё семантику во-вторых я расскажу как из-за нашей ошибки у нас выросла нагрузка на каку вченого тоже рассказав Какие же метрики на какие метрики кавка стоит обратить отдельное внимание а для начала просто такой верхувен взгляд на кафку что как она устроена кавка стоит из набора брокеров каждый брокер - это отдельная машина из у кипера для хранения Мета Информации а на самих брокерах собственно и хранятся сообщения в кафку пишут продюсеры с Кафки читают консьюмер А базовым вообще элементом кавка является поток сообщений а он называется топик топик в свою очередь состоит из партиции каждая партиции её можно представить как лист это какая-то последно сообщений где все сообщения пронумерованы причём чем сообщение свежее тем больше у него номер в партиции дописывает продюсер вот ну и портим как-то сообщения дополнительно шарю когда если вы раньше слышали про каку одним из бонусов кавка в отличие от других популярных решений то что она хранит данные на диске И то что она их реплицируемый каким-то образом отстаёт от мастера Это тоже не так уж страшно Но в ка есть такой процесс под названием переизбрание лидеров когда к примеру наш мастер выпал НОД на которой лежала вот мастер этой партиции выпала кавка производит перебирание лидеров и лидером называет какую-то другую Ну какой-то из СВ становится лидером и получается у нас какая-то потеря данных в виде этой Дельта Если вы когда мы читали документацию пов там Есть два вида переизбрания лидеров этон и кажется что unclean - это какой-то плохой то есть вот тогда у нас мы теряем данные аклин - Это хороший что мы не теряем данны на самом деле это не так единственное между ними Отличие в том в размере вот этой дельты если вот слей в момент Приключения отставал от мастера там больше чем на какое-то количество сообщений мы говорим что это unclean если меньше говорим что Клин это очень важно понимать что при переизбрании лидеров Мы всегда теряем данные а также нюанс довольно неочевидный как раз ни в какой документации он не описан переизбрание может происходить Когда у вас всё хорошо то есть вот у вас работает кавка всё стабильно всё равномерно нагрузка кавка отвечает довольно живо пос не никаких проблем как взяла и перебрала лидеров А вы потеряли данные это грустно а что же с этим делать для этого пойдём дальше есть такой стандартный механизм в любой системе Когда вы куда-то пишете данные вам система отвечаем каким-то подтверждения о том что да успешно записаны такая же функциональность есть в кавка к примеру вот продюсер пишет в кавка и если вы параметр на продюсере выставите в ноль то никакого подтверждения не будет получено выставите в единичку то тогда к вам подтверждение если записана была одна реплика Ну то есть это собственно в Лидер продюсер пишет напрямую в Лидер вотре момен там All кавка вышлет подтверждение когда пришла успешная запись во все синхронные партиции А вот что кавка понимает по синхронными партиции на самом деле конфигурируется отдельным ключом на самой кавка То есть вы там для каждом для каждого топики определяете количество партиции которые кавка должна держать в ИНН И если ИНН стоит один это никак не отличается от предыдущего примера А вот если вы поставите и репли в два тогда кавка отправит продюсеру подтверждение только когда обе реплики будут успешно записаны вроде бы всё хорошо В таком случае если будет перебирание лидеров у нас не будет никаких проблем с потерей данных потому что всё синхронно А да бесспорно будет какой-то там оверхед на продюсер То есть он будет просто дольше ждать ответа от кавка Но на самом деле это не так страшно наиболее страшна другая Проблема в том что если вот в данной ситуации один из одна из реплик выпадает то мы вообще не сможем писать в топик потому что кавка ждт что будет записано две реплики а доступна только одна а здесь сонно тоже самое написано и это Это довольно плохо мы используем для этого такую как сказать подход Наско он хорош уже вам решать мы держим Для важных топиков ин реплика высланных в двоечку А репликацию в трире реплика могла догнать и продюсер смог Продолжить писать к нам в кавка дальше одним из фундаментальных вобще понятий кавка являются отступы Как вы помните партиции представляет собой такой набор пронумерованных сообщений и отступ - это как раз и есть номер этого сообщения offset - это номер самого свежего последо добавленного в партиции сообщения offset - это номер самого старого доступного сообщения кавка хранит данные не всё время А кайто риод которы Наро ВМ их потихонечку движется ВД и по суе окно которое потихоньку движется по времени предположим у вас отработал какой-то консьюмер он заел сколько-то данных вплоть до какого-то отступа там обозначен и следующем запуске вашего конмет забрать вот этот выделенный розовым кусочек данных со оста и сохранить отступы А вот весь нюанс именно в том как мы сохраняем отступы ну обновляем их для того чтобы При следующем запуске кон сюр стал считать с нужного момента А для этого есть несколько способов И на самом деле когда вы выбираете коню для кавка первое что вы должны читать это именно как он работает с сохранением отступов самый базовый путь это автоматическое сохранение Но на самом деле он самый плохой как Это пример работает комер Фет какие-то данные из кавка кавка периодически до него стучится жив он не жив если он жив то на сохраняется соответствующий отступ В чём здесь Проблема в том что к примеру конр данные заел кавка проверил что он жив сохранила отступ а он не успел записать данные на диск упал мы получили потерю данных дальше а если пришла другая ситуация К примеру комер успел записать данные на диск и упал кака пришла поть что он жив а он мёртв отступ не сохранил мы получим дуб данных то есть такой подход не обеспечит вообще никакую семантику доставки следующий способ - это сохранение вручную вотка игото банри вот я этот кусочек данных скачал Сохрани отступы а вроде бы всё хорошо но тоже Это обеспечит семантику только at least One least on такая семантика когда мы данные не теряем но могут быть дубли А почему так может произойти Ну собственно точно такая же ситуация предположим мы уже сохранили кусок данных но когда мы делали да кто-то дёрнул ми9 и ваш консур благополучно упал не успел сообщить кафке что нужно было сохранить отступы получили дубли данных на самом деле даже эта семантика не такая плохая большинство по доставке данных рабо после дупликации но мы гонимся зам как же его обеспечить Един способ обеспечить это сохранять и данные и отступы вне кака при этом Сохранение данных и отступов должно быть Амар потому что ЕС оно будет нер получаем целевой баз данных выступает какая-то полноценная база с транзакциями нет никаких в этом проблем в нашем случае это hdfs там транзакций нету И я вам сейчас расскажу решение от собственно опять нна оно используется в гоблине для семантики прие hdfs предположим у нас отработал коню и сохранил данные два файла с данными и Ф С отступами в какую-то папочку затем их нам надо переместить в какую-то прок директорию ходу что мы разбиваем после сохранений на наборных операций и коню их пробует по очереди выполнять То есть первый второй третий предположим мы первый выполнили всё хорошо а на втором мы упали тогда при у него есть какая-то ме информация о том что же должно быть было перемещено и куда и пытается завершить этот процесс и если у него это получится Всё хорошо мы как бы закончили предыдущий фетч данных при уже следующем фе есть либо если это не получается мы можем откатить а таким образом можно обеспечить Эванс при сохранении в баз данных без транзакций А и вроде бы у нас всё хорошо И на самом деле это хорошо продюсер получает ответ об успешной записи данных кавка реплицируемый себя не теряет мы Чим данные в hfs с семантико что я не знаю уже много раз употребляю не пояснил это семантика когда мы забираем данные без потерь и без дублей то есть один в один то то всё отлично но у нас начались проблемы в чём они начали проявляться во-первых они начали проявляться запа логов у нарат данные собрали и запустили их обработку на если данные за вчера доезжают к примеру сразу после полуночи там Чад ночи Зелёная отметка то всё хорошо если они доезжают по причине с большим подани там после обеда может быть вообще с подаем на день всё плохо Мы не успеваем их поработать Мы не успеваем посчитать Ну метрики аналитику прочее собственно ще то есть на эта пресловутая семантика которой мы так дорожим была нарушена Кроме этого Мы заметили очень медленную работу консмед когда минутный батч данных из кавка у нас фел по 2-3 минуты а также косме стали падать с ошибками все ошибки были на мотив я не могу Достучаться до кавка я не могу получить такую-то важную мне информацию от кака и причина на самом деле была общая для этих двух проблем кавка стала очень медно отвечать на простейшие запросы по несколько минут Ну точнее отдель но очень мено отвеча на эти запросы и понятное дело когда мым получа бам информацию то мы эти пару минут даже можем не заметить Запусти только раз полчаса а если мы Чим каждую минуту эти 2 минуты будут намного более затм что на чтение и свка выросла вче раза Нижняя красная линия - это пиковая нагрузка до этих событий красная линия Верхняя после жёлтый график - это как раз график на чтения но ещё более интересную картину мы увидели когда посмотрели на эту же самую информацию в разрезе нот вот здесь просто какая-то лапша её даже сложно разобрать но основное что это чтение снот но основное что отсюда можно вынести что в отдельные моменты времени чтение происходит с двух-трёх нот кластера и с пяти машин а а также здесь ещё отлично видно переизбрание лидеров когда кавка лихорадочно пытается перебрать лидеров чтобы сбалансировать нагрузку но у неё ничего не получается А в чём же была причина причина такого неравномерного плени нагрузки по нодом была в двух вещах во-первых кавка неравномерно расплела партиции партиции по нодом и и лидеров поно у меня здесь изображён такой очень гротескный случай на самом деле он крайне близок к к и то есть примерно так всё и было очень важно понимать что даже когда у вас всё хорошо то есть у вас не вырос трафик в одно частие в четыре раза кавка всё равно может ошибаться и это на самом деле проблема как мы её решали более-менее автоматизированного способа решения мы не нашли единственное решение которое мы нашли - это распределять руками може Мить Кофи в котором описать На каких нох должны лежать Какие партиции Какие из них должны быть лидерами и это позволит построить картинку ближе к идеалу понятное дело вот всё что я говорю это верно для самых толстых топиков через которые вас проходит наибольшее количество информации следующее решение вообще довольно неочевидно повышаем репликацию кажется ребята у вас так трафик на чтение увеличился в четы раза а вы хотите его ещ увеличить чтением на синхрон репликацию собственно сеюсь объяснить почему это помогает Вот снова ная ситуация весь трафик пришёлся на первый брокер и вот на нём все лидеры он красненький ему очень плохо что делает кавка в этом случае кака пом там таймауту брокер не ответил кавка решает блин ему плохо дайка сниму нагрузку Ича с фик приет друго и уже Плохо ему вот собственно именно эту картину Вы видели на графике в разрезе по нотам когда вот дико колбасит нагрузка по нодом так и скакала А если бы у нас была репликация для этого топика к примеру 3 мы могли бы увидеть после этого процесса следующую картину мы помогаем кавка не ошибаться при распределении нагрузки по нодом то есть ей просто Чем больше реплик тем ей сложнее ошибиться И это хорошо на самом деле вот всё что я рассказываю это было для каки 082 можно в новых резах сделали чуть умнее я очень на это надеюсь но для старых версий по крайне мере это очень актуально также у нас случились довольно классические проблемы которые наверное бывают во всех сервисах по обработке данных Мы упёрлись в сеть у нас сказалась внезапно часть нос в стойке с анком в 1 Гигабит Ну если представляете Это примерно 125 Мб в секунду вот на графике у нас там нано приходилось более 150 Мб в секун понятное дело сеть была запру во-вторых мы внезапно узнали что кавка не дружит i5 Почему так получилось Но это жизнь то есть документация читается в последний момент сейчас поясню как это связано если у вас всё работает Если у вас п и всё работает хорошо У вас всё хорошо но когда у вас на теряется какой-то блок данных и начинается восстановление вот тот момент вживается для кавка рекомендуется использовать использовать там типа 1 с зеркалирование то есть какие-то кусочки файлов просто зеркалить по нескольким дискам Тогда просто это дешевле Восстановить всё это поправили что в итоге получили самое главное мы равномерно размазали всю нагрузку разобрались с сетью разобрались с дисками кавка стало ответь реально за нормально время то есть заме минули секунды но нагрузка в мже была проблема как уже не трудно догадаться Проблема была в повторном Выка данных у на собственно бы и корень этой проблемы был следующим мы использовали бачей коню данных это предок гоблина и Он сохраняет отступы для топиков в фикс исправляющий проблему вот по сути все отступ попали в одну директорию из-за этого при этом он сам был заточен под то чтобы работать одним запуском сразу для нескольких топиков поэтому мы иногда как теряли доступ отступы после успешного фе так иногда они успешно обновлялись из-за этого нам было крайне сложно понять что проблема именно в этом месте Вот поили бы так режиме работы перво раза упал трафик пиковый во-вторых Вот видите такие вот периодические пики это запуск нашего ба коню раз полчаса То есть у нас на ходу работают задачи которые каждые полчаса просыпаются Фет все новые данные и обратно засыпают немного цифр к чему же сейчас мы пришли У нас сейчас кавка состоит из тех же пяти серверов каждый день в не записывается поряд 3 млр собы это более информации пиковая нагрузка гиби каждый день изки вычитывает порядка 18 информации и пиковая нагрузка порядка 5 Гигабит это на пять тачек здесь стоит отметить что мы сейчас уже держим нагрузку большую чем та при которой у нас были проблемы ранее описанные но при этом всё хорошо растём дальше монитори узкие места это сеть диски и самое главное ление нагрузки Вы должны за этим следить кавка может ошибаться Вы должны ей в этом помогать вот к примеру график ра нагрузки по ном Когда у вас всё хорошо здесь можно увидеть что четыре НОД работают более-менее в одна немного отстаёт но это не так страшно при этоже этом фике н немножко изменился также стоит мониторить пере избрание лидеров бесспорно вы на это полять никак не можете данные вы всё равно теряете но Круто иметь количественную метрику того а Насколько часто насколько много данных вы теряете бесспорно стоит мониторить рас синхронизацию партиции Ну это наверно в любой системе где синхронная репликация на это стоит смотреть Рик у кавка для этого целое множество а из тех которые нравятся мне это число не синхронных партиции а из тех которые должны быть и и второе - это максимальный лак репликации максимальный лак репликации - это вот здесь график с точностью дано на Сколько сообщений мы отстаём вот что по этому графику видно Всё хорошо то есть лак в пределах нуля с ческими пиками Вот это нормальная ситуация так всё хорошо начинается проблема Когда у вас лак стабильно больше Ну то есть по сути ваша система не успевает вовремя перерабатывать всю поступающую в неё Ну реплицировать ВС поступающую в не информацию и наверное самый она здесь последняя наверно самая главная Метрика это время ответ на простейшие запросы сама по себе она мало что значит только если вы не хотите с помощью Кафки делать около обработку данных но она является очень важным симптом того что с Шей Кать что плохое а мы исполь здесь время отве изображено время ответа на запросы коню и вот красной лини выделена отсечка в 10 секунд мы сейчас её используем такая вот немного эмпирическим образом найденная если кавка отвечает меньше чем за 10 секунд наши около обработки обработки данных работают Хорошо если кавка отвечает больше чем за 10 секунд чем карау И вот на этом графике есть два Пика вот по ним сразу видно что синеко но лукой ноде очень плохо Надо разбираться А почему она периодически начинает так долго отвечать при этом плохо не только ей но и другим ном Ну и в качестве итога во-первых Если вы хотите обеспечить EX семантику доставки данных на кавка это конечно Это первоя возможно но вы должны смотреть на все элементы попна То есть вы должны знать понимать как это работает и внимательно контролировать и продюсеры и консьюмер и саму кавка бесспорно на самом деле когда вы имеете множество поставщиков данных крайне сложно За всеми уследить но в жизни обычно требуется не все данные должны доставляться семантико ванс вот а те которые до доставляться нужно внимательно за этим следить беспорно стоит мониторить узкие места в нашем случае при повышении нагрузки первое что у нас выявилось - это сеть диски и баланс а баланс нагрузки по нода также стоит мониторить Ну на самом деле много метрик самое главное - это повторюсь время ответа каки на запрос на запросы причём как продюсера так и коню а и Ну понятное дело стоит мониторить там всякие классические метрики типа КПУ Memory Но это по стандарту Ну на этом всё довольно коротенько получилось ваши вопросы вот желающий Мо почему сейчас обработка отступов никоим образом не связана с балансом нагрузки она связана с обеспечением семантики Просто если вы сохраняете их не в каку а вместе с данными у вас появляется шанс что этот процесс сделать почти аманым скажем так мы глубоко в кишки Кафки честно говоря не вдавалися Почему именно кавка вот когда человек посмотрит на ситуацию он примет другое решение то есть условно говоря более оптимальная А почему именно алгоритмы Не умет ситуаци разруливает с чем есть Ну я правильно понял что собственно возрастание нагрузки на чтение на ноды оно произошло После перехода на на схему вот с офсета на у нас исходно была схема То есть у нас Сначала появилась кавка исходно была схема с нием офсета вне кавка а спустя уже там может там год мы словили эти проблемы при миграции мы допустили ошибку в конфигах то есть всё было довольно просто то есть мы просто ошиблись в в конфигах из-за этого кон сюр стал повторно вычитывать те же самые данные это вырасти нам нагрузку в четыре раза пример была такая ситуация Ну если это была ошибка в конфиге и вы её могли потом подправить Почему А ну я вот сейчас сделал вывод сейчас с исправлением всех этих проблем у нас сейчас кавка работает переваривает больше трафик на чтения и с ней всё хорошо Ну ладно просто не не не очевидно по графикам что что у вас как происходило Ну ладно понял из нормально так получилось потом если что да хорошо Здравствуйте спасибо за доклад меня вот интересует такой момент вы для того чтобы обеспечить семантику exactly Once вы сделали что-то вроде транзакции в ходу да вот мне не совсем понятно как вот как в откат этой транзакции сделали А мы в случае если допустим во время произошла какая-то ошибка сейчас я пример Отмотай вот на эту штучку а на самом деле это всё довольно просто мы просто То есть это не наше решение именно это решение от лидина то есть мы используем готовый консьюмер и там такой используется откат можно провести следующим образом мы знаем какие файлы куда бы должны были перемещены если мы в итоге так и не смогли завершить условно говоря эту транзакцию мы эти файлы просто удаляем то есть уже перемещенные то есть целевые Ну впоследствии Вы же их потом заново перечитывайте Ну то есть это не совсем как бы Эван получается в итоге мы получим Эванс То есть просто получается такая штука что в рамках одного запуска кон сюра беспорно каки данные доехать но е учитывая что запускается регулярно каждый следующий запуск фиксит проблемы предыдущего и суммарно получается то есть у нас нет ни потерни дубле Спасибо Здравствуйте спасибо за доклад такой вопрос А почему вы не храните информация Вот именно проблема в том что мы не можем Амар сохранить Вот какой-то такой способ и в принципе в принципе вот данная пример схема стандартно для хранения отступа в зуки И на самом деле большинство коню используют именно её Но в этом случае если вы данные уже успели сохранить но у вас по пути в зки что-то произошло Вы получили проблему а ещё тогда второй вопрос я так правильно понимаю что вы используете кафку версии 8.2 А уже нет но на самом деле проблемы с балансом мы наблюдаем до сих пор у нас сечас стоит 012 и те же самые проблемы с балансирование партиции по нодамэ ухудшает время вы замеряли А да Мы замеряли у нас э оно деградировала на самом деле в разы Но это там условно говоря было несколько миллисекунд стали десятки миллисекунд то есть Для нас это было допустимо Спасибо так вон ещё Здравствуйте спасибо за доклад А подскажите ещё раз просто я не очень понял для себя немножко разъяснить ээ в зуки у вас не получилось нормально и персистентный гарантированно записывать оффсеты но получилось записывать их непосредственно в hdfs правильно Да мы храним их всех в hdfs Да ну а uker у вас тоже кластеризованный а Z kiper он используется Вот так он никак не связан с хранением отступов он просто это внутри каво зуки прямо он он на самом деле у нас он стоит на тех же самых нода что и брокеры просто параллельно то есть вот на них размазан ещё и инстанс кипера Понятно спасибо ещё такой вопрос как exance обеспечивается на уровне продюсеров а это на самом деле наша больная тема потому что с нашей с нашей стороны преимущественно потребителя данных мы сами коне даже в ка кое-что пишем но там малую часть трафика очень сложно уследить Как раз за каждым продюсером как он допит в идеале должно Это произо примерно так то есть продюсер знает о каждом сообщении которое он пишет в кавка и если на него не было получено подтверждение он пытается его переотправить вот это идеальная ситуация вот но мы не можем контролировать все поставщиков данных То есть это много-много других проектов А какие-нибудь ещё варианты кроме Кафки рассматривали для вот такого интерфейса общего вот скажем так в этой категории Вот какие у нас были требования нам нужно через неё было каждый день передавать тба информации при этом каким-то малым количеством железа также мы хотели иметь возможность вычитывать данные заново то есть на самом деле свки одни и те же данные вот за последни какой-то интервал вре неделе читает может множество поставщиков то есть там примерно Мы каждый топик вычитывать некоторым случае по нескольку раз к примеру олай и онлай обработки плюс этим занимаются ещё и другие отдела нам хотелось не терять данные покани там и вот на само и при этом хотелось какой довольно для этого популярный проект у которого есть сообщество которое развивается активно вот у нас как-то больше выбора особо и не было в то в то время это было год два назад здравствуйте Спасибо хороший доклад вопрос У меня следующим насколько критично для вас потеря нескольких каких-то сообщений за Ну например за день там 20 сообщений Да условно я так назову метрику и если это критично Как вы мониторить соотношение отправленных и полученных сообщений А смотрите к нам каждый день приезжает 3 млр сообщений бесспорно если мы потеряем там несколько десятков из них ничего страшного не произойдёт то есть но есть часть э часть данных которые используются в примеру для точной аналитики либо используется примеру для счётов денег Но на самом деле большая часть Она проходит мимо Кафки Просто потом для как раз аналитики поэтому А И там нам потеря даже 10 сообщения критично то есть Ну на самом деле просто может какой-то быть ус к примеру считаем клики для какого-то баннера за какой-то интервал несколько часов их там будет 20 штук у нас почалось 20 строк и среди них пять были клики это ну с на этот баннер Итого у нас на 25% просела итоговая аналитика это уже большая проблема то есть всё зависит очень От случая и мы следим за прям точной доставкой только для нескольких топиков Ясно А если не секрет как вы это делаете Вот именно сам механизм А сам механизм как мы это делаем У нас сейчас прямо вот сейчас проходит процесс автоматизации всей этой деятельности основной способ такой сходить в хранилище данных если оно есть на стране поставщика получить нужную нам метрику и сравнить с тем что что есть у нас всё то есть всё довольно просто это единственно есть такая проблема это можно сделать не всегда к примеру если поставщик вообще не хранит эти данные он сразу пишет в каку и забывает про них они ему особо не нужны то это делать нельзя никак Единственный способ это раз мониторить на уровне продюсера спасибо Вот спасибо за доклад вопрос по вот этой дот сде вместе с данными отступов это решение Я так понимаю основано на ещё каком-то решении от ID Нет это не наше решение Я просто вам рассказал решение которое используется в примеру Камю или Гоблине это это закрытое открытое что открытое закрытое это открытый исходный ход на гитхабе лежит то есть не проблема его можно да можно взять посмотреть как это работает се ировать то есть неких проблем спасибо большо Кстати да сделал небольшой Пир Если вы используете кавка ка Гоблин - это лучшее решение для бачей фе данных из кавка вот Это такой небольшой Пир по крайней мере мы проводили довольно такое тестирование множество решений лучше не нашли Всё Всем спасибо"
}