{
  "video_id": "bOqSexPzSIE",
  "channel": "HighLoadChannel",
  "title": "Бинарные (файловые) хранилища: страшная сказка с мрачным концом / Даниил Подольский (GitInSky)",
  "views": 5464,
  "duration": 2724,
  "published": "2017-07-30T00:47:27-07:00",
  "text": "доклад мы называются бинарные они же файловые хранилища но на самом деле на самом деле мы имеем дело с где тут clicker со страшной сказкой вчера народ конфи я рассказывал о том как была решена проблема не была решена о том как решается проблема хранения большого количества файлов на одном из проектов которые мы обслуживаем но вот этот тезис моего доклада в 2015 году не существует не то что хороший хотя бы приемлемой системах хранения файлов ну а теперь собственно доклад немного теории что такое файл файл это кусок кусок данных с именем что важно почему файл это не строка в базе данных файл слишком большой чтобы можно было обращаться с ним как с одним куском почему вот у вас сервис 1 - highload конференция да у вас сервис который держит одновременно 100 тысяч соединений это не так уж много есть ли по каждому из соединений мы отдаем файл в один мегабайт размером нам нужно примерно 100 гигабайт памяти для каши для буфере для буферов и под эти файлы мы не можем себе этого позволить обычно во всяком случае соответственно нам приходится делить эти куски данных на кусочки поменьше которые именуются чан коми довольно часто а иногда блоками вот и обращаться с блоками они с не с файлом целиком соответственно у нас появляется еще один уровень адресации и чуть позже станет понятно почему у нас это почему это так сильно влияет на качество бинарных хранилищ тем не менее несмотря на то что хороших бинарных хранилищ на сегодняшний день на свете нет файловый обмен это краеугольный камень современного массового обмена данными то есть что угодно что передается по сети интернет оформляется в виде файла начиная от собственной чуть m-elle страничек и заканчивая потоковым видео потому что в к на той стороне мы имеем файл с потоковым видео и на этой стороне на этой стороне и на часто нет но тем не менее это все равно файлы почему так почему мы опасно основываем наш обмен массовые обмен данными на файловом обмене потому что еще десять лет назад каналы были медленными еще десять лет назад мы не могли себе позволить джейсон интерфейсов мы не могли все время запрашивать сервер у нас слишком большая была лейкен сеном надо было все данные которые требуются для показа пользователю сначала закачать а потом уже предоставлять пользователю возможность взаимодействовать с этими данными потому что иначе она все так немножечко лагало файловые хранилища на самом деле с я термин хранилище крайне неудачный потому что надо бы их называть отдавали щами еще 20 30 лет назад это были действительно хранилища вы обрабатывали данные вы их складывали на ленту эту ленту сдавали в архив это вот это и есть хранилище вот сегодня это никому не нужно сегодня если у вас есть 450 миллионов файлов это значит что они все должны быть в горячий доступности если у вас есть 20 терабайт данных это значит что какой-нибудь байт из этих двадцати терабайт любой из них обязательно потребуется в самое ближайшее время какому-нибудь из ваших пользователей если только вы не работаете в корпоративной среде но если вы работаете в корпоративной среде слова highload корпоративным средам редко применяют значит в бизнес бизнес требованиях никогда не написано хранить файлы это так даже когда написано вот то что называется система резервного копирования нет это не система резервного к при копировании эта система аварийного восстановления никому не нужны не нужно хранить файлы всем нужно читать файлы это важно почему это важно будет ясно чуть позже почему файла все-таки приходится хранить потому что их надо отдавать для того чтобы их отдавать они должны быть у вас надо сказать что это не с не всегда так то есть очень многие проекты например не хранят html-странички а генерят of на лету потому что хранение большого количества html страничек представляет собой проблему a gene renee большого количества stimul страничек не представляет собой проблемы это хорошо масштабируемая задача так еще чуть-чуть теории файловые системы извините меня пожалуйста это общеизвестный материал но мне надо его прогнать я постараюсь побыстрее просто потому что иначе непонятно о чем я говорю значит файловые системы бывают старого типа и журнале руи мая что такое журнале руи мая файлов такое старого типа файловая система мы подписали на нее данные она более или менее немедленно отправил эти данные на диск в нужное место что такое журнале руи мая файловая система это файловая система которая пишет данные на диск в два этапа сначала мы пишем данные в специальную область диском из именуемом журналу а потом когда у нас есть свободное время перекладываем из этого журнала или когда журнал заполнится перекладываем это же из этого журнала данные туда куда где они должны лежать на файловой системе зачем это было придумано это была придумана для того чтобы ускорить старт у нас мы знаем что у нас всегда файловая система консистентной поэтому проверять файловую систему если у нас был неудачный аварийные restart например сервера нам не надо а надо проверить только небольшой журнал вот собственно это это будет важно файловые системы бывали плоские иерархические плоское это fat12 выяснить скорее всего не встречались в ней не было директорий а вот соответственно все файлики лежали в корневой директории были доступны сразу по смещению в таблице fat иерархические это файловые системы на самом деле на самом деле организовать директории на файловой системе не так уж трудно и на в плоской и например в том проекте где мы проблему сейчас решаем мы это сделали но тем не менее тем не менее все современные файловые системы с которыми вы сталкивались это иерархические файловые системы начинает интереса заканчивая какими-то z fsm вот они все хранят директории как файлы в этих директориях хранится список содержимого этих директорий соответственно для того чтобы добраться до файла 10 уровня вложенности вам нужно по очереди открыть 10 файлов вот и 11 тот который ваш вот на sata диски 100 и abs of вот вы можете с ним проделать 100 операций в секунду и 10 вы уже потратили то есть быть в de file 10 уровня вложенности если всех этих директорий нет в кэше вы быстрее чем за одну десятую секунды не откроете даже если ваша система больше ничем не занимается аминь файловые системы современные все поддерживают контроль доступа и расширенной атрибуты кроме fat fat как ни странно до сих пор используется никакого контроля доступа и никаких расширенных атрибутов не поддерживает почему я это сюда написал потому что это очень важный для меня момент страшная сказка связанная с файловыми системами началась для меня в 1996 году когда я и внимательно изучил как устроен контроль доступа в традиционном unix и вы помните да маска прав хозяин группа хозяина все остальные у меня возникла задача мне нужно было создать группу которая может читать и писать файл группу которая может читать файл и групп и все остальные не должны были с этим файлом уметь делать нечего и тут я понял что традиционная маска прав или unix а не поддерживает такой паттерн вот они еще чуть-чуть теории по six basics это фактически стандарт его поддерживая сейчас все на операционные системы которыми мы пользуемся в реальности это просто список вызовов которые файловая система должна поддерживать для нас во всем в этом важно что опыт дело в том что работа с файловой системы с файлом вкусит происходит не по имени файла appannie которому файл хендлер у которые вы можете у файловой системы запросить по имени файла после этого вы должны для всех операций пользовался этим самым сэмплером операции могут быть об извините операции могут быть простые типы рид операции могут быть простые типа right и операции сик которая собственно делает жизнь с пасек с файловой системой в в распределенный делает невозможным создание пояс по распределенной файловой системы стандарта posix почему потому что сик это случайное перемещение в случайную позицию файла то есть в реальности мы не знаем что мучить об извините мы не знаем что мы читаем и мы не знаем куда мы пишем а для того чтобы понять что мы делаем нам требуется хендлер который нам вернул операция alpen и нам требуется текущая позиция файле это та самая вторая адресация то что это то что файловая система по sex поддерживает случайную вторую адресацию они просто последовательного типа открыли давайте читать файл от начала до конца или например открыли и давайте писать его и каждый новый записываемый блок добавляется в конец файла вот то что пасек требуется чтобы это не было так не позволяет об этом позже не позволяет создать распределенную хорошую распределенную по six файловую систему да вот еще что существует на файловых системах пусть x на самом деле совсем не все не все posix поддерживают одинаковые одинаковый набор атомарных операций но в любом случае в любом случае некоторое количество операций должны быть и товарными что такое атомарная операция это тамара эта операция которая происходит или не происходит за один раз на напоминает чем-то транзакции в в базах данных но на самом деле то есть только напоминает например на файловой системе x4 с которой мы все должны быть знакомо раз мы собрались на этой конференции открытия каталога является а товар на операцию то есть создание каталога является от автоматной операции поэтому например нет в пример приводить не буду извините не имеет отношения к обсуждаемой теме последнюю про теорию разные вещи которые на самом деле для функционирования файловой системы не нужны но бывают иногда полезны сжатие она онлайн это когда мы при записи блока его сжимаем поддерживается например на инте fs шифрование тоже вот поддерживается на инте fs а вот на x4 не поддерживается не жрать и не шифрования там это организовано с помощью блочных устройств которые поддерживают и то и другое но это не файл важно то есть на самом деле действительно для нормального функционирования файловой системы не требуется ни то ни другое дедупликации я на самом деле очень важный момент для сегодняшних для сегодняшних файловых систем и вот например мы у нас есть проект на котором 450 миллионов файлов но только 200 миллионов чанков это означает что примерно половина файлов одинаковые просто называются по разному свитшоты snapshot и почему это важно потому что у вас есть файловая система размером вы например пять терабайт это означает что консистентная ее копия просто так быть создано не может разве что вы остановили все свои процессы и запустили чтение с файловой системы пять терабайт будут читаться sata диск из дешевого где-то 6 часов по моим оценкам но 5 по терабайту в час вот и нафему можете остановить свои сервисы на 5 часов нет наверное поэтому если вам нужно консистентная копия файловой системы вам нужны снапшоты на сегодняшний день снапшоты поддерживаются на уровне блочных устройств в vvm и там они поддерживаются омерзительно смотрите мы создаем lvm snapshot и наше линейное чтение превращается в случайной а потому что мы должны почитать в снег шоке почитать на базовые на базовом блочном устройстве записью все еще хуже мы должны почитать на базовым на базовом томми мы должны подписать в нокаут и мы должны снова почитатель snapshot а то есть в реальности snapshot nlm бесполезная назад fs есть хорошие снапшоты их там может быть много они-то могут их их можно передавать по сети если вы например сделали уже копию файловой системы то вы можете передать snapshot в общем smudge от не обязательный для файлового хранилища функционал но очень полезный а чуть позже выяснится что обязательно самое последнее она может быть самое важное во всей этой теории когда-то когда инсталлятор инте 4 запускался испытуемых доз инсталляция инте 4 без запущенного smart драйва и такое чтение вмс доз занимала 6 часов с запущенным со smart драйвом 40 минут почему потому что если мы не оперируем в памяти содержимое директории мы вынуждены всякий раз делать вот те самые 10 шагов кэширование записи на самом деле еще до недавнего времени считалось что это очень плохой тон что единственный случай когда вы можете включить кэширование записи на файловой системе не на файловой системе тогда на устройстве это если у вас есть рейд контроллер с батарейкой почему потому что эта батарейка позволяет не попавшего на случай навык луна не вне в случайный момент выключив шийся server позволяет эти данные все сохранить в памяти и когда сервер будет обратно включен дописать их на диске понятно что операционная система не еще такого поддерживать не может это надо делать на уровне контроллера сегодня актуальность этого хода резко упала потому что сегодня мы можем использовать очень дешевую оперативную память который называется ssd-диск сегодня кэширование записи на ssd диска это один из самых простых и эффективных способов повысить производительность локальной файловой системы но тем не менее это все было про файловые системы локальные для вашего компьютера хайло вот эта сеть это сети в том смысле что к вам приходят по сети ваши посетители на то они и посетители и это означает что вам требуется горизонтальное масштабирование соответственно сетевые протоколы протоколы доступа к файлам делятся на 2 группы извините все как-то давлю не туда да stateless это нфс это видов и еще некоторое количество протоколов что такое стоит лис это значит что каждая следующая операция самостоятельно в том смысле что ее результат не зависит от результата предыдущие с файловой системой basics стандарта как мы помним это не так у нас опера у нас результаты rida ира это зависит от зависят от результатов сика а он в свою очередь от результатов up on вот тем не менее поверх вот этих posix операционных систем существа папа six файловых систем существует stateless протокол передачи нфс например и это его главная проблема почему нфс такое потому что он стоит лес поверх стоит full state на самом деле сегодня все чаще используется стоит full протоколы в сетевом обмене ну потому опять же это очень плохо стоит full протокол для интернета очень плохо подходит потому что у нас непредсказуемые задержки но тем не менее все чаще какой-нибудь джейсона javascript ный интерфейс все чаще помнит о том чем закончилась предыдущие его общение с сервером и заказывает очередной джейсон основываясь на том чем закончилась предыдущая операция из сетевых доступа сетевых файловых систем состоит full потом протоколом надо отметить цифр она же самбо двуличная отказоустойчивость об этом я говорил вчера довольно подробно дело в том что традиционные файловые системы упирают на целостность данных потому что их создатели были за выражены я думаю о словом хранилища они думали что важно важно и самое важное в данных это хранить их защищать сегодня мы знаем что это не так вчера мы слушали народ конфи доклад человека который занимается защитой данных в дата-центрах он твердо нам сказал что они отказываются не только от хардкорных дисковых массивов но и отца в тварных дисковых массивов они используют каждые диски каждый диск отдельно и какую-то систему которая следит за расположением на этих дисках данных за дуб за репликации этих данных почему потому что если у вас есть дисковый массив из например 5 4 терабайта их дисков он садишься 20 кораблей для того чтобы после сбоя случайного например один из дисков вылетел его надо восстановить в реальности надо прочесть все 16 терабайт 16 терабайт считаются pad работу в час то есть у нас вылетел всего один диск но для того чтобы снова запустить массив у работу нам требуется 16 часов это неприемлемо в сегодняшней ситуации так что на самом деле на сегодня от отказывал устойчивость бинарного хранилища это в первую очередь бесперебойное чтение и как ни странно запись то есть ваше хранилище не должно по первому чиху превращаться в тыкву которая занята только сохранением сара спрятанных внутри данных то есть если уж данные потерялись бог с ним а не потерялись и главное чтобы чтобы шоу продолжалось ну и что еще важно сказать про сетевые бинарные хранились я та самая capture арема то есть выбирайте любые два из этих трех или у вас данные будут всегда консистентными и всегда доступны но тогда они будут лежать на одном сервере или у вас данные будут всегда консистентные и они будут распределены между несколькими серверами но окажется что доступ к нему время от времени ограничен вот или у вас данные будут всегда доступные и распределены между серверами но тогда то что вы с одного сервера из другого прочитаете одно и то же вам совершенно не гарантируется cup теорема это всего-навсего теорема ее никто не доказал а вот но по по факту по факту это действительно так не удается попытки предпринимаются постоянно вот например сейф с 2 которая упомянуть чуть позже итак под попытка решить как тебе доказать не судне ничтожность как теоремы что говорить о да вот но тем не менее вот это вот все про файловые системы про бинарные хранилища в чем проблема давайте разберемся самый простой способ который приходит в голову каждому системному администратору которому надо хранить терабайт данных и миллионы файлов это просто купить большую из ходы почему большая сходи это не выход потому что вы если у вас большая схд и один сервер который не общается или вы смогли разбить ваши данные на блоке на на куски и с вашими файлами общается с каждым куском один сервер у вас нет никаких проблем если у вас горизонтальное масштабирование если вы постоянно добавляете сервера которые должны эти файлы отдавать или не дай бог сначала обрабатывать а потом отдавать вы столкнётесь с тем что на большой исходы нельзя просто положить какую-то файловую систему когда мне в первый раз в руки попал д л д б д л б д блок девайс я подумал что о отлично у меня будет два сервера между ними будет репликация основанная на вот это самом д д л б д я не буду сейчас говорить что это такое и у меня будут сервера которые будут читать с 1с другого очень быстро выяснилось я был тогда молод наивен очень быстро выяснилось что все сервера kosher уют чтение это значит что даже если мы что-то в тихоря поменяли на блочном устройстве компьютер который сам этого не менял и соответственно знал какой кашин во лидировать ни за что об этом не узнает вот и будет продолжать читать данные совсем из тех нее не из тех мест где они на самом деле уже лежат для того чтобы преодолеть эту проблему существуют разные файловые системы которые им были обеспечивают инвалида цию сша фактически они заняты этим на всех компьютерах которые смонтировали общее хранилище значит еще конечно с этим асеев с 2 есть такая проблема тормоза при конкурентной записи помните мы говорили про атомарные операции что такое атомарная операция эта операция которая атомарном которая одним куском происходит в случае с распределенной файловой системой даже если все наши данные лежат на одной единственной большой исходы с хода и система хранения данных изменить на полу сказать сразу да так вот если они лежат даже на 1 единственный атомарная операция по набору читателей и писателей требует чтобы они все пришли к консенсусу консенсуса в сети это сетевые задержки то есть реально писать на усевшись 2 это боль cfs 2 это oracle oracle кластер file system версия 2 на самом деле оракал не такой идиот они сделали неплохую файловые системы просто они сделали ее совсем другую задачу они и сделали под под расшаривания одних и тех же файлов базы данных между несколькими своими оракул серверами у базы данных файлов базы данных oracle такой паттерн работы что она при то не прекрасно на этой самой асеев с 2 работают вот для файлового хранилища она оказалась не пригодна мы пробовали еще в 2008 году ну еще со сейф с 2 оказалось неприятно что is time жить и ринга то есть из за того что время немножко различается на всех виртуалка хоть и у нас запущены даже на одном посте у нас нормально не работает от ff2 то есть в какой-то момент обязательно случается что время на вот этой вот на этом сервере обеспечение этой системы насти пошло назад он на этом месте падает ну и так далее ну и почему так медленно я уже в общем объяснил а еще большую при большой исходы довольно сложно получить в собственное пользование то есть например в теснее никакой большое сходи вам не дадут есть такое у меня подозрение что вообще вот эта идея что большая схд это очень надежно очень хорошо очень высоко производительно связано просто с правильным расчетам потребных ресурсов когда вы же не можете просто так купить большие сходи их не продают в магазине вы должны пойти к авторизованному вендору и поговорить с ним он по кивает головой скажите twill костью и посчитает вам тысячи так на 50100 одно шасси еще у нас будет набить дисками но он посчитает правильно загружена это из хода будет процентов на 5-10 а если окажется что ваша нагрузка выросла они посоветуют вам поставить еще одну такую ну это это про жизнь хорошо ладно пусть большая из ходы не выход это вот мы выяснили почему это выяснили это потом и кровью вам я проект рассказывая берем какую-нибудь кластерную файловую систему мы попробовали несколько мы попробовали cfs мы попробовали люстру мы попробовали левого fs во-первых почему так медленно понятно почему потому что асинхронной операции по всему красть кластеру что значит 3 balancing а вот про и balancing предыдущий докладчик упомянул и у предыдущий докладчик упомянул что на ходу fs нет автоматического ребаланс синга для уже лежащих на ней данных почему их там ее там нет потому что в тот момент когда на цехе случается ли balancing мы перестаем мы теряем вообще возможность с ним работать либо lansing это такая хорошо отлаженная процедура которая поедает примерно сто процентов полосы дискового обмена то есть вот диск saturation сто процентов иногда либо lansing он же на каждый чих его делает иногда либо лаем синг длится часов 10 то есть первое что делают люди работающие с цехом это научаются прикручивать интенсивность этого самого ребаланс инга но в любом случае на том самом кластере который мы сейчас используем в том проекте где у нас много файлов и много много много данных мы вынуждены были прекратить а вы крутите balancing вверх и там у нас да действительно di saturnia сто процентов тиски выходит из строя при такой нагрузке очень быстро поэтому вот до ребалансить почему он такой почему он случается на каждый чих почему ну вот вот все вот эти почему у меня лично остались без ответа до сих пор ну и та самая проблема атомарная операции которые должны пройти по всему кластеру синхронно до тех пор пока у вас кластере две машины у вас все в порядке когда у вас власть или 40 машин вы обнаруживаете что все эти 40 машин должны вообще-то то есть мы имеем 40 квадрат до количества сетевых пакетов которые мы должны послать протокол или balancing и вот к протокол обеспечения консистентной sti пытаются с этим бороться но пока не очень успешно пока в этом смысле системы с единой точкой отказа с наим но дай немножко лидируют но тоже не очень сильно так почему нельзя просто взять и сложить все файлов базу с моей точки зрения именно так и надо поступить потому что если у нас лежат файлов базе у нас есть большой пакет наработанных средств работы с такими вещами мы умеем работать с базами в миллиард строк и на на петабайты мы умеем работать с базами на несколько миллиардов строк вот на несколько десятков петабайт это у нас все хорошо получается хотите возьмите oracle хотите возьмите какой-нибудь db2 хотите возьмите какой-нибудь новый сквер почему-то ну понятно почему потому что файл это файл файл нельзя из файлов невозможно обращаться как с одной как все товарной сущностью поэтому файловой системы распределенные существует плохо а базы данных и распределенные существует нормально меня тут заканчивается время надо как-то немножко подогнать а еще крест на всяких асеев с люстрах и прочих цехах ставит то что нам требуется резервное копирование файлов как вы себе представляете резерва нее праве резервное копирование двадцати терабайт напоминают to buy в час и главное куда как часто как обеспечить консистентной на таком количестве если у нас не единая файловая система и мы соответственно не можем снять snapshot единственный выход который я лично вижу ситуации это файловые системы с versio нирования когда вы пишете новый файл из старой никуда не пропадает можно до него добраться указав время на которой вы хотите посмотреть состояние файловые системы еще конечно должна быть какая то сборка мусора microsoft обещал нам такую файловую систему еще в 90-х но так и не выдал добыла распределенная файловая система для windows они и даже анонсировали для longhorn но потом не случилось ни longhorn они этой файловые системы значит почему резервное копирование это важно прицеп на и копирование это не отказоустойчивость это защита от ошибок оператора мне самому случалось перепутать с урсы destination в командир sing и получить волшебная история получить сервер на котором работает 16 виртуалок но файлов с образами их нету потому что я их удалил вот пришлось их внимать с помощью команды dd из самих виртуалок тогда обошлось но тем не менее мы обязаны на в наших бинарных хранилищах обеспечить versio не рование и никакой файловые системы которая бы нормально обеспечивала versio нирования кроме ctfs которая при этом не кластерная и соответственно нам не подходит нет на свете что же делать для начала изучать собственную задачу надо ли экономить вот если вы способное положить все файлы на одну сваю исходы и обработать их все на одном сверх мощном сервере сейчас же можно сервера с двумя терабайтами памяти поиметь и с каким-то там до несколькими сотнями ядер вот если у вас хватает бюджета на это все и вам нужно файловое хранилище сделайте так вот это может обойтись деру всему бизнесу дешевле basics если вам не нужна случайно и чтение или случайная запись то это большой к вам у вас плюс вы можете справиться с имеющимся наборам например pdf с упоминавшийся ранее или cfs или люстра люстра прекрасная файловая система для вычислительного кластера а вот для отдающего plaster а никуда не годится большие файлы нужны ли они есть ли вы все ваши файлы могут считаться маленькими маленький это я напомню ситуация они свойства файла если вы можете позволить себе обращаться с файлом как с единым куском данных у вас нет проблем кладите его в базу и все у вас хорошо почему на том проекте которая здесь упоминаю но не называю у нас все получилось потому что там 95 процентов файлов меньше 64 килобайта соответственно это всегда одна строка в базе и вот в этом в этой ситуации все работает прекрасно вот versio не рование нужно ли versio нирования на самом деле есть ситуации когда versio не рование не требуется но тогда не требуется и резервное копирование эти ситуации когда все ваши данные сгенерим и вашими роботами фактически ваше файловое хранилище представляет из себя кэш тут нет места для ошибки оператора соответственно ничего потерять вот насколько дал большим должно быть наше хранилище если вы способны собрать ваше если от возможности одной-единственной файловые системы все еще и хватает для того чтобы обеспечить ваши потребности отлично очень хорошо собираемся ему удалять файлы как ни странно это важно то есть есть такая байка и на самом деле это не байка что в контакт никогда ничего не удаляет то есть как только вы туда загрузили картинку или какую-нибудь музычку она лежит там всегда удаляются ссылки на эту информацию никакого рекуперация то есть теперь ее использование для место занято занятого файлами в том же вконтакте нету говорят я слушал такой доклад почему потому что как только вы попытаю вы пытаетесь и переиспользовать место у вас немедленно возникают серьезнейшие проблемы с консистентную почему асеев с 2 подошла под уроков скую базу данных потому что ник не перри используют места потому что когда вы пишите новые данные в базу они просто добавляются в конец файла и все если вы хотите переиспользовать место вы запускаете compact я не знаю так ли это в современном урок ли но это было так в 2001 году вы запускаете compact это оффлайн операция она обеспечит консистентная с тем что она монопольно владеет тем файлом который обрабатывает вот соответственно собираемся ли мы удалять файлы собираемся ли мы переиспользовать дисковое пространство вот тот же vkontakte растопит новые диски и нормально и я считаю что так и надо вот каков будет профиль нагрузки чтение и запись у многих файловых систем распределенных очень сильно проседает производительность на именно записи почему потому что обеспечение консистентной sti потому что атомарные операции потому что синхронная про операции по всему кластеру у новый из coinbase синхронная операция по кластеру всего одна обычно имплементация версии записи вот данных может не лежать они могут приехать потом но вот версию к конкретной записи все ноды вы должны думать про них одно про нее одно и тоже и это не так и даже не так для всех но иску или например кассандра этим не заморачивается у кассандра нет синхронных синхронных операций по всему кластеру тем не менее профиль нагрузки если вы только читаете попробуйте какую-нибудь кластерных файловых систем возможно у вас все получится с ней вот эти истории успеха как подходят люди говорят зачем вы все это сделали возьмите просто люстру до в вашей ситуации люстра работала а в нашей не очень да так вот для некоторых сочетание требований для некоторых задач решения есть прямо вот существующая а для некоторых сочетаний нету его действительно нету если вы поискали не нашли это означает что что его нет что же все-таки делать сама и вот вот вот то чего следует начать можно ходить и клянчить эти 200000 долларов начальству пару месяцев и когда их все таки дадут сделать хорошо вот только клянчить не 200000 а сначала сходить к вендору и посчитать с ним сколько надо клянчить а потом клянчить примерно в полтора раза больше вот все-таки сложить файлы в базу вот я пошел по этому пути мой наш 450 миллионов файлов сложили в базу но этот фокус удался у нам потому что нам не требуется никакой пусть и нам не и у нас 95 процентов файлов маленькие вот ну и можно написать свою файловую систему в конце концов разнообразные алгоритмы существуют вот мы нашу написали поверх новую сколько базы вы можете взять что-нибудь еще первую версию нашей мы написали поверх рсмд постфикс postgres но тут и нас возникли некоторые проблемы не сразу а через два года но тем не менее на самом деле это не очень сложно даже по six файловую систему написать не очень сложно берете фьюз и вперед там не так много вызовов и все можно реализовать но в реальности хорошо работающую файловую систему написать все-таки получается сложно вот на этом собственно мой доклад закончен спасибо если у вас вопросы да столько сложностей с этой абстракции файлах может быть есть какие-то альтернативы exactly нету нету весь наш современный обмен данными построен на файлах все наши браузер и все наши клиенты тренд турик травин торрент-трекеров они все ожидают файлов даже торрент трекер который качает чан коми он все равно для начала должен быть сформированы хэш и он должен устроив сформирован по файлу целиком на самом деле я думаю что в ближайшее время ситуация изменится она уже меняется уже довольно много где файлы не абстракция файлов не используются но ближайшие пять лет мы все таки будем с этим получать удовольствие здравствуйте спасибо за доклад у меня такой вопрос насчет хранения файла в базе данных есть не будет ли проблема курицы и яйца база данных а между же файл охране цветами в конце концов во первых не все современные база хранятся в файлах тот же aerospike может сесть поверх чистого девайса можно подсунуть файл на ssd файловая система файлу и это на этот файлом показать а можно по такой подсунут прямо девайс и он будет прекрасно с ним работать а во-вторых файловой системы современной практически не вносит а верхи да то есть вы берете какой-нибудь большую файловую систему делаете на ней файлы по терабайту подсовываете и все у вас хорошо и задать еще такой вопрос вообще с чем связаны все те проблемы это по историческим причинам или есть физические ограничения связанных железом есть физические ограничения связаны с железом винчестеры очень медленно и по сегодняшним меркам а у нас быстрый процессор у нас много памяти у нас очень быстро и канала у нас крайне медленно и винчестера дешевый винчестер обеспечивает нам ну 100 50 мегабайт в секунду линейного чтения и хорошо если 200 iops off создай это не так но ssd все таки до сих пор на в настоящий момент надо рассматривать не как геша никак дорогой диска как дешевая оперативку спасибо спасибо за доклад я просто разделяю вашу боль по поводу cfs в г прим чувства и у вас сколько машин кластеры была цифра 6 но у меня было восемь прежде чем я от них отказался и десна что в защиту системы хочу сказать что если доступа осуществляется в разные места то есть ноды взяли себе лоб на определенное под дерево и там в нем живут вот тогда она как-то живет как только начиная с конкурентный доступ с трех машин ну да так и есть это не был даже highload это было там ну 34 запросов в минуту по моему я ну и все у меня было больше у меня была там павлов 10 секунд их три проходила и все горки рванут роптал для в ожидании когда же им дадут и это при том что она это хорошая распределенная файловая система мне тоже нравится как она работала вот пока пока не начался конкурентный доступ вот то что вы говорили отдача на чтение при небольшом количестве записи она справляется вполне нормально я еще использовал как луговую файловую систему а вот там у меня была ситуация что каждый сервер пишет свою папку они друг другу не мешают здравствуй спасибо за доклад скажите вот то вы решили вопрос 470 миллионов файл сохранили в базу да допустим нету базы надо сохранить 405 миллионов этих файлов как-то хроническими если смысл их разбивать по папкам в реальности не какую папку на 403 да не какую папку на 450 миллионов файлов вы не прочтет есть файлы в ней не найдете никогда но по понятным причинам есть индексированные директории на той же s4 которые в принципе помогут вам но не на 450 миллионов так что да если у вас одна файловые системы вы хотите с нее быстро отдавать действительно по каким-то признакам файлики придется распихать по папкам и все будет отлично работать пока вы не захотите это сдублировать куда нибудь вот тут окажется что обход дерева не заканчивается никогда насколько глубоко получается можно засовывать потому что вы же самое говорит что от этого вот медленнее отдаваться значит на самом деле при достаточном количестве памяти накроешь чтение это совершенно неважно директории попадают в кэш в первую очередь как только вы ее в первый раз открыли она уже в по шее не оттуда вымоется последний и разве что вы туда попишите но тогда она папина тогда запись пройдет iv кэш и на диск и у вас все равно все хорошо"
}