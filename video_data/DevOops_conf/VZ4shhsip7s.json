{
  "video_id": "VZ4shhsip7s",
  "channel": "DevOops_conf",
  "title": "Андрей Колаштов, Дмитрий Столяров, Владимир Гурьянов — TSDB – взгляд изнутри",
  "views": 503,
  "duration": 4113,
  "published": "2022-02-02T01:25:33-08:00",
  "text": "добрый день сегодня у нас отличный доклад про time series базы данных на примере про метался но это будет не просто про секрет баз данных это будет еще и про рассказ про то как переписали свое решение на что-то были меня стандартные но давайте лучше ребята из fanta сами расскажут и покажут нам здесь отличную презентацию спасибо при день как вы видите доклад называется там серия с базы данных взгляд изнутри и изначально мы планировали это доклад в двух частях в одной части мы хотели рассказать как мы переезжали на cortex а в другой части рассказать как мы съезжали с кортекса и это звучит наверно немножко забавно но мы придем к этой детали немножко объясним почему именно так сори я потерял ухо так вот а значит но пока мы делали этот доклад у нас выяснилось то что у нас материала слишком много и даже первая часть получилась больше трех часов поэтому мы решили пойти на такой достаточно логичный шаг и превратить этот доклад в сериал поэтому в целом мы открываем сериал который называется ок метр и сезон первый тайм серия с базы данных взгляд изнутри и собственно в этой серии мы расскажем немножко про предысторию потому что смотрите есть компания флант есть ок метр причем здесь флант причем здесь ок метр как они связаны вот это все мы расскажем в этой серии с вами владимир гурьянов полный ответ дима привет а владимир гурьянов наш ведущий инженер и он не только ведущий инженер он еще наша палочка выручалочка он очень сильно помогает координация работы в команде с продуктом но и вот помогает нам например рассказывать немножко о том что мы делаем еще с нами андрей колосов андрей привет привет а андрей один из совладельцев компании флант и мы с ним вместе на самых кстати смешному ней вместе учились а я был преподавателем андрей в институте но последние там больше 10 лет мы работаем вместе и сейчас андрей руководит собственно проект на mac метр внутри фланта ну и с вами сегодня я технический директор фланта сооснователь fanta дмитрий стали в и так а прежде чем говорить потом сели с базы данных прежде чем говорить про наш опыт мы немножко вам дадим контекста объясним флант ок метр как это все связано и как мы здесь оказались да как мы здесь оказались давайте сейчас немножко про флант ну в первую очередь многие узнают что флант это сервисная компания компания который занимается devops поддержкой клиентов да мы обслуживаем роды наших клиентов и берем их под ключ на обслуживание поддержка наши строятся на базе купер на эту страну как как вы все может быть знаете может быть слышали сейчас у нас на поддержке более 200 кластеров куба куба мы занимаемся уже более пяти лет и безо взрыва пилите на самом деле никуда вот пока мы работаем с кубом мы очень сильно копаем сторону взрыва пилите и куб без коробки не дает никаких там инструментов до чтобы готовы что-то поставить ее сразу у нас было хлопает ergo берите появилась поэтому мы очень сильно в этом направлении копаем движемся и у нас сделано очень хорош и уже мониторинг на базе прометея и он стоит в каждом кластере и сейчас суммарно по всем кластером который нас есть у нас если сложить все метрики уникальный то у нас получится 120 миллионов уникальных метрик тотчас немножко расскажу про наш мониторинг у нас конечно же есть базовый мониторинг куба что такое можем посмотреть общее утилизацию мощностей кластера посмотреть в общем сколько утилизировано ресурсов сколько свободно например там я дерсу в сумме то 1000 я беру these равана можем посмотреть детальные провалиться в эту статистику можем посмотреть детальный в двух разрезах во первых по нотам кластера можно посмотреть такой классический вид как привыкли заходим на ноду смотрим сколько проца занята сколько свободно сколько памяти заняты и так далее или же сделать drill-down о примитивом кубан посмотреть сколько экспресс потребляет провалиться внутрь посмотреть сколько там контроллеры потребляют в провалиться контроллер посмотреть сколько под и потребляют помимо этого мы конечно же мониторим состоянии ключевых компонентов классика это control plain dns кроме того мы мониторим сеть кстати эту штуку она очень сильно помогает в работе потому что часто в облаках особенно где сетка хлюпает leithen все очень плохой мы сразу на графиках видим как меня есть слайд как меняется light in se да и если положение в эти момент что-то происходило она тормозило еще что-то в намочить для по графике и помогают с положением клиента который работает этом кластере помимо это у нас есть крутой мониторинг ingress я удачи буду для простоты называть балансировщика ну в первую очередь мы смотрим на нагрузку балансировщика вдруг разрезах опять же такой классический вид когда мы смотрим нагрузку потом вырастут по домену по факту да и можем провалиться внутрь сделала 3 down посмотреть какая нагрузка на отдельных локейшнах точно так же мы можем сделать drill-down из в кубовый vita в нем space и и провалиться внутрь посмотреть нагрузку о ingles ресурсам кроме того у нас есть графики в процентов распределение по времени ответа сервера по кодам ответа сервера по многим различным другим параметрам вообще мониторинг у нас получился такой знатный мощный богатые мы хотели бы вам показать в конце презентации показать ссылочку дать на него на что вы его сами посмотрели могли руками потрогать есть демоверсия из дома кластера посмотрите будет интересно что будет в конце слайдик со всеми ссылками из этого доклада помимо этого мы нам не хватало мощности граф анри мы упоролись настолько что мы даже сделали свой плагин графа не который позволял в лучше виде некоторые данные визуализировать влаги называется графа на статус map возможно он у вас уже стоит и поэтому у нас почти 10 миллионов установок уже есть и кстати даже один раз есть я сделал смотрел почту пролистывал и тут вижу письмо от человека емейлом который заканчивается на собачка т с лаком этот человек спрашивал написала флан ты спрашивал про графа на статус map различные вопросы было так удивительно и приятно да что в tesla использует наш плагин chic помимо этого так как мы обслуживаем куб да и всю инфраструктуру под ним мы должны из-за него отвечать да и нести какую-то ответственности мы написали специальный софт который считает писала и кластера и его компонентов вообще это сложное сложный момент да потому что из класть это все-таки такая сложная штука состоящий из кучу-кучу-кучу разных компонентов они могут отдельно работать отдельно не работать в целом это кран может как это влиять на кластер можете влиять на кластеры так далее но эти вопросы решили и написали специальный софт который гоняет смог тесты в кластере например там создает пазики стоит deployment и заказывайте бодики с дисками и так далее и смотрит на результат выполнения вообще получилось мне заказать диск и там создать под и зависимости от этого от этих данных он считает конечный солей и нарисует графики в целом это вот кубовая часть мониторинга помимо этого у нас есть еще стоит full стоит флэта обычно базы данных которые находятся вне куба это какие-то виртуальной машины или серверы на них куб никак не раската но они рядышком с кубом вот тут на слайде час нарисована сколько у нас есть инстансов различного различных технологий и исторически так сложилось что мониторим и 2017 года весь этот стоит вал с помощью от метра что же такое от метр почему ок метр да ну в первую очередь от метр это сайт система на это мониторинга мониторинговый сервис saas он очень просто устанавливается и него не нужно конфигурировать вы просто поставили агента и он тут же начал собирать мониторинговые данные вас тут же появились даже борды графики alert и все пошло помимо этого агент на сервере может находить различные процессы которым он обучен на примеру может подключиться танк джори engine купить пфп но и начать этих собирать мониторинговые данные опять же вас тут же появится даже борды и графики alert все вот под этим процессом которые он нашел кроме того точно также находит один агент подходит базы данных различный тоже не подключается собирать гласную статистику например у нас отличная статистика по по сбросу оно сделано совместно с компанией до той игре the next также являются пользователями от метра вообще комета такой быстрый быстрый старт в мир мониторинга если вы не хотите тратить очень много ресурсов вас их нет например ли вы какой-то стартап это вот так метр это для вас потому что огромный опыт собран и реализован в виде с авто которым можно пользоваться и сразу помимо этого ваттметр есть очень маленький и полезные фичи например если у вас есть только нить virtual очка в облаке и там стоит агент и вдруг а это virtual очки обрывался интернет вы через пять минут например интернет у вас появился и вы хотели бы посмотреть что же было в эти моменты с виртуалки происходило пока у нее не было интернета так вот так метр данные не потеряет он x агрегирует соберет пока интернета нет а потом когда он появится отправят в облака дальше к метр написано таким образом чтобы он не ними минимальную нагрузку оказывал на ногу с которой он собирает метрики и точно также сделан очень крутой вас эффективных высоко эффективный протокол передачи метрик чтобы не таки уходили с большим сжатием вы будете экономить на трафике даже если даже если не знаю что помимо этого в окне то можно очень много кастомных метрик загрузить бизнес метрик всего чего все чего хотите он очень хорошо кастомизируют поэтому даже если у вас есть силы на развитие собственного мониторинга то можете вместо компании с чем-то другим поставить себе акк метр и получить готовый готовый опыт и плюс еще сделать своих метрик в итоге флан . нравился ок метр ну во-первых он технически очень хорош был на там интерфейс на и нас всем устраивал плюс там был собран клёвый опыт ребят которого периле плюс сами ребята были очень крутым и в них поверили и семнадцатом году мы заключили с ними стратегическое партнерство прорабатываем и с ними четыре года до начала 2021 года и у ребят в этот момент начала меняться направлении собственного развития они решили что они решили что нужно сменить свой вектор и решили выйти из проекта мы поняли что с от метром сейчас нубы здесь завязали на него получать с половину собственного мониторинга мы не понимаем дальше как до кучи что будет с ахметовым придет новая команда как что будет с от метром вообще непонятно ну с другой стороны от метр это вообще такой интересно коммерческое направление для нас в итоге мы решились на сделку века мы приобрели ок метр вообще довольно таки довольно таки интересно что я думал что не доживу до такого момента что моя компания купить другую компанию и рассчитывал даже дед она получилась внезапно с одной стороны что мы смогли это сделать и приятно в итоге от метр стал клёвым к левой частью костюма фланта которая уже на тот момент состояла из платформа на базе куба который называется до krause утилита доставки кода который называется вверх она доставляет код как и то krause так и вообще в любой купер нить и логичным кусочков как раз таки дополняющих был мониторинг в итоге у нас было 200 кастеров кубой 120 миллионов метрик уже и плюс еще и сад метра приехала 23 миллиона метриках от почти 400 тысяч машин но теперь вот надо понять что с этим всем зоопарком нам делать и конечно же у нас есть план ну в первую очередь логичным шагом видится объединение всех наработок ног ветра есть клёвый мощный discover и хороший мониторинг состоит в приложении флан то есть крутой мониторинг клуба помимо этого хочется дополнительно еще расширить аудиторию убрав интерлок но вот например есть различные системы мониторинга в которых вы возможно работали там zabbix дата дата долг new relic на не все разные у них разные интерфейсы у них разные ну визуальная сторона да они по-разному configure c и вот каждый раз когда вы изучаете новую систему вы должны тратить время тратить силы и непонятно вообще но дальше вам окупиться или нет с одной стороны с другой стороны есть графа надо она может быть там не такая клевая и такая хорошая да но она такая родная уже для нас да и давайте другую аналогию сейчас вот например у вас дома выходите в чем домашних тапочках наверно ожидая и вы не ходите в сапогах вы не ходите там в туфлях и так далее вот вы когда графа ну видите вы скорее все чувствуете себя также как домашних тапочках в чем-то таком уже знакомом потому что она везде одинаковая и везде стандартная может быть на этом графике разные тону тапочках потертость разный на все плюс-минус вот мои тоже дальше мы хотим сделать чтобы в окне 3 был перспективный стек как внутри для наших инженеров так и снаружи для инженеров которые пользуются от метра чтобы инженеры которые пользуются от метров они не изучали еще одну и получали еще какое-то бесполезное знание например такой как паскаль например нужен если вы заставляли да люди еще свод учить паскаль двадцать первом году но зачем это нужно да поэтому хочется сделать так чтобы в окне 3 собрались перспективные технологии которые увеличивали в илью инженеров которые пользуются этим отметкам который мы могли бы перейти более высокооплачиваемую работу в дальнейшем например ну и вообще фланг против любого вида блока поэтому конкретное действие которое можно сказать или сделать по всем вот этим направлениям это убрать наш собственный язык oakville собственный язык запрос в сторону пользу громкие лида которые знают многие и точно также сделать пока метр чтобы был граф она лайк то есть максимально был похож на графа на что вы открывали от метре понимали что я в тех самых домашних тапочках и тут сразу все понятно в итоге нас вот есть план на ближайшие полгода мы уже на самом деле давно придерживаемся и пилим но вот нас dateline примерно на ближайшие полгода что мы и вот объём это объединить наработки и расширить всю нашу но расширить нашу аудиторию брать один дарлок помимо этого мы хотим но инцидент менеджмент добавить мы рассказывали про инцидент менеджмент на одном из дивы псов дима рассказывал что это такое это система которая собирает себя и агрегирует множество alert of из от различных серверов дальше их можно обрабатывать смотреть анализировать реагировать на них такая панелька дежурно вот мы хотим это встроить такой пойдете данный завернуто кроме того имея метрики логично и же загрузить в то же самое место в ту же самую систему и логика для полноты всей картины помимо логов аналогично можно загрузить trace и графы связей мониторинг изменений распределенный black box мониторинг и реал юзер мониторинг и потом еще хочется applications performance monitoring имея вот это все да в одной единой системе уже можно делать хочется сделать она были detection брутус анализ проблем до которые происходили и может быть вы видели у этого есть к пайлот такая штучка которая помогает вам программировать да вы набрали одна буква о новом набрал всю строчку остальное нейрон нейрон очка такая обучена это специально на гитхабе вот мы хотим такую штуку сделать который будет чинить чинить вам сервера сразу или подсказывать как чинить вам сервера например и так у нас есть вот такой план о котором еще с проговорили большой план есть команда ребят у которых горят глаза они жаждут в бой и конечно же есть продукт который надо пилить и без пользователей этот продукт был бы ну сложно было бы пилить потому что пользователь то что продукт но пользовательская база у нас еще есть есть она очень богатая широкая разносторонняя по стыку технологий и далее чтобы начать строить нашу систему нам нужно ответить на вопрос все ли у нас хорошо с фундаментом да все ли у нас хорошо с фундаментом и прежде чем ответить на вопрос все ли у нас хорошо с фундаментом надо ответить на вопрос а что же вообще для нас это фундамент но я думаю что вы прекрасно понимаете что для нас фундамента по сторож вот смотрите андрей сейчас говорил про там пачку high-level фич и все эти high-level фичи требуют достаточно простой штуки собственного интерес базы данных возможности сохранять и работать с метриками если наш нижний слой наш сторож медленный или неэффективный в том смысле что дорогой или ненадежный то все наших и ловил фичи будут медленно и не эффективные и надежные ну и конечно же там невозможно строить там хороший дом без хорошего фундамента вот тут тоже самое и как вы уже знаете там из рассказа андрея бак метре у нас уже есть то речь у нас уже есть большое количество про митяев и ну как бы прежде чем это вот развивать мы провели такую достаточно глубокую процедура анализа и подумали вот стоит это развивать дальше или нет и чтобы вам рассказать о том каким выводам мы пришли мы должны сначала рассказать о том как же устроен текущей сторож от метра но прежде чем рассказать как устроен текущей стороны от метра нам хотелось бы объяснить вообще как в целом устроенной tm series базы данных какие принципы лежат в основе какие паттерны и как они в общем случае работают поэтому начинаем и именно с этого рассказа смотрите если брать в самом простом случае мы собираем данные с одного единственного источника есть у нас какой-то градусник который показывает например 10 градусов и мы собираем данные данные что такое 10 градусов 10 градусов цельсия и таймс темп текущее время ну то бишь данная time stamp и суммарно в самом простом случае от 16 байт 8 байт не float 8 байт ней там стенд миллисекундах и все классно понятно что в большинстве time series это не разных 16 байт это сжатие или классический механизмы сжатия не знаю где был за 4 или как в прометея это горилла encoding или как в виктория metrics это вариация на тему горилла encoding но по сути это механизм какие-то механизмы сжатия но мы сейчас забываем про механизмы сжатие мы считаем что у нас есть просто не сжатой метрики и эта пара значение и таймс темп главная пара и раз 60 секунд мы такие значения собираем и в самом простом случае складываем файл у нас один единственный источник из него все складывается в 11 в один единственный файл когда мы хотим это визуализировать все очень просто берем выводим на графике у нас пары по сути x y рисуем точки выводим все классно временной ряд из одного файла один временной ряд из одного файла выводится очень просто но конечно же вопрос возникает в той ситуации когда у нас миллион источников ну и казалось бы миллион какое достаточно большое число но андрей опять же приводил цифры у нас сейчас по 200 кластером как оберните суммарный объем метрик районе 120 миллионов но можно легко поделить получается что на один кубовый кластер в среднем там типа 700 или 800 тысяч метрик это означает что цифра в миллион она в целом ну вот у любого пользователя к обернитесь она очень-очень близко может быть там 500 тысяч может быть 400000 даже для небольших кластеров почему ну потому что мы стали сервера делить на меньшие кусочки под и контейнеры ну много мониторинговой данных для того чтобы действительно иметь возможность настоящий обзор ability to нужно собирать достаточно много данных поэтому миллион это немного так вот как сохранять миллион временных рядов как их складывать и как с ними работать вариант 1 самый лобовой берем и сохраняем каждый временной ряд в свой отдельный файл 1 временной ряд в один файл 2 по 2 3 в 3 и так далее и всего у нас таких миллион файлов когда хотим это визуализировать все очень просто открыли файл там готовы временной ряд очень просто показать но проблема возникает момент записи ну и самая такая очевидная поверхностная история что для того чтобы сделать миллион записей нужно нужно сделать миллион записи черт побери есть mind million in put out put операций если мы собираемся данные раз 60 секунд но нам этот миллион нужно делать раз в 60 секунд миллион 60 секунд это 16 половиной тысяч iops of и в целом это вроде мы немного потому что любая современная сезоне может этот объем переварить но называется дьявол кроется в деталях и как бы первая деталь и заключается в том что когда мы пытаемся записать 16 половиной тысяч раз по 16 байт мы помним то что у нас пара в нашем примере пара 16 байт на ssd мы думаем что мы записываем 20 гигабайт и сзади но на самом деле ssd не позволяет писать 16 байт так уж устроены все современные диски они могут работать то есть для того чтобы созревшие работал эффективно там сделан сделана работа блоками внутри да и у каких-то дисков это 16 килобайт у каких-то даже больше это мы пытаемся записать 16 байт воздушный диск он типа вот эти 16 5 вот туда он берёт 16 килобайт ней блок обновляет в нем значение и записывать все 16 килобайт поэтому легким образом что называется одним движением руки у нас вот этот то что было изначально двадцатью двумя гигабайтами то о чем мы думаем что мы тащим 22 гигабайт и превращается в двадцать два терабайта в день но и тут конечно же возникает проблема штатов нет ssd очень быстро потому что райт endurance и так далее но проблема здесь даже не только в этом а потому что с этим еще можно как то как то жить можно купить нам очень дорогие соседи там еще что то еще что то но в целом подход такой очень плохо масштабируется потому что миллион записей миллионы abs of 10 миллионов записи 10 миллионов и abs of ну а нам нужно 120 миллионов метрах сохранить со всех параметров плюс 22 миллиона метрика текущем одни там 140 миллионов понятно что это не супер хорошо стрельцы тем более если мы хотим собирать чаще чем 1 60 секунд захотим 1 5 там еще цифры что называется sky rocket эту в общем подход так себя и казалось бы ну вообще его не стоит использовать но например в памяти и первой версии до выхода версии 2 использовался ровно такой подход ok если делать не так что можно еще сделать вариант другой положить все данные в один файл да я на самом деле под файлам в кавычках до некоторую в одном место на диске да но будем считать что это файл для просто простоты соответственно у нас миллион метрик и вместо миллиона файлов у нас один файл и когда мы собираем точки со всех метрик мы их складываем как бы одним куском подряд в один в один файл проходит секунд собираем 2 1 2 кусочек и так далее и у нас все стало классно запись просто летает на никакой проблемы нету потому что мы пишем по факту секунд шел все классно все удобно 16 байт миллион раз всего 15 или 16 мегабайт 60 секунд любой даже нжмд уж мы если не говорить там про ssd да даже самый самый простой диск позволяет писать там не менее 80 или 100 мегабайт в секунду до здесь 15 мегабайт 60 секунд ерунда полная но конечно же возникает другая проблема как это читать потому что если мы попытаемся построить график вывести один временной ряд то что нам для этого нужно сделать нам нужно этот временной ряд выводим и как ты его найти файле найти точки относящиеся к этому временному ряду если решать эту задачу в лоб то это скан всех 23 гигабайтов но даже если решать эту задачу не в лоб то есть у нас есть какой-то индексы мы знаем композицию каждой точке каким-то образом да мы сейчас не обсуждаем каким-то это в любом случае в сутках тысяча четыреста сорок минут до если мы собираем данный раз 60 секунд нам нужно в 1440 мест файле сходить и по доставать эти точки да это тоже неудобно но опять же 1440 мест это еще не страшно но давайте представим себе что мы хотим визуализировать загрузку процессора на сервере и у нас стоя der и по каждому ядру мы храним ну например 5 временных рядов загрузка юзер time system time weight что там служебная перерыва я прерывать ну короче пятак показатели мы храним да это получается стоя der на 5 показателей 500 серий на 1440 мест это уже 500 1000 abs авто есть для того чтобы показать загрузку цепью одного серого нужно в 500 тысяч мест понятно что это не рабочий вариант ok собственно проблема да как я думаю что вы уже прекрасно поняли в там серия с базе данных мы чтения хотим делать всегда горизонт полина то есть мы всегда хотим достать один временной ряд ну или несколько временных рядов но целиком как бы горизонтально опишем мы всегда вертикально то есть у нас всегда приходит как бы текущее значение нам нужно их дописать во много много много мест и вот эта штука она лежит в основе вообще всей проблемы time series баз данных это как бы главная такая вещь лежащие в основе на самом деле для нее есть очень хорошие и очень простое решение и делает следующим образом есть у нас этот миллион источников мы пишем в один файл подряд и в этом файле невозможно искать но дополнительным и пишем в память разгруппировать данные по по временным редондо жена случается как бы две записи 1 на диск а другая в память и память ну она же у нас называется свой дамаг со сменой и теоретически она random access на самом деле память ни разу не рэндом ну то есть там есть очень много нюансов связанных с кашами в процессоре из вообще с тем как устроена память но она гораздо более random access чем ssd и если высадим и не можем делать миллион случайной записи постоянно маленькими маленькими маленькими кусочками то в память мы уже миллион можем делать легко как можем и 10 миллионов и 100 миллионов если мы говорим в минуту то есть это как бы для оперативной памяти не проблема соответственно пишем на диск пишем в память когда в памяти у нас накопился какой-то кусочек мы этот уже кусочек спрашивал сбрасываем на диске эта операция простая потому что это линейная но это большой блок это sequent шел sequins white он выполняется без проблем ну и после того как мы записали на диск мы можем удалить журнал можем удалить данные в памяти потому что они у нас уже больше не нужны ну и собственно журнал нам нужен зачем затем что если наша программа упадет до чтобы она могла восстановиться и журнала восстановить значение в памяти по неполному блоку хорошо ну и после этого мы соответственно продолжаем писать также журнал память и у нас появляются дополнительные блоки современ нас со временем у нас становится этих блоков очень много возникает еще одна проблема давайте вернемся к примеру с цепью а у нас был пример что 500 метрик нам нужно поднять 500 временных рядов нам нужно поднять чтобы показать данные по серверу потому что у него стоя дир по каждому ядро мы храним 5 показателей в этом примере у нас 720 часовых файлов соответственно получается что нам нужно 500 метрик 500 iops of достать не из одного фунта чтобы достать из одного файла нужно 500 iops of ну что по 720 и нам нужно 150 тысяч наверно 150000 достаточно много ну и на самом деле кейс с визуализацией там с достоянием 500 500 временных рядов он еще не такой страшный потому что а что если мы хотим посмотреть цепью со 100 серверов да тут уже одних метре 5000 а еще на 720 а что если не за месяц за год но в общем я думаю что вы понимаете что этот ну как бы есть есть ограничение как с этими ограничениями можно обходиться операция мерч накопили несколько сегментов меньшего размера объединили их сегмент большего размера при этом если все правильно сделать эта операция является очень простой ну и с точки зрения там вычислительной сложности она линейная ну то есть если данная во всех сегментах отсортированы это объединение сортированных данных это операция с линейной сложностью и ну то есть все что нужно это просто секунд шел прочитать файл с диска и записать другой файл тоже закончил после этого эти четыре можно удалить допрошу нас появился блок большего размера тем самым что мы сделали мы уменьшили количество и abs в которые нам требуется для того чтобы достать больший период понятно что это можно повторять несколько раз ну то есть часовые сливать там 4 часа вы и часовые в не знаю 16 часовые количество слоев таких файлов уровней биржа размер на каждом уровне до часто там или шесть часов 2 или 4 в разных time series системах сделан по разному до него все вообще есть там много слоев некоторых есть просто всего два там только память и persistent storage но общий принцип он соблюдается везде окей с проблемой разобрались читаем горизонтально пишем вертикально и поняли что за счет этого подхода с оперативной памятью это больше не проблема но я специально упустил для простоты еще один момент у нас ведь миллион временных рядов нам надо их как-то идентифицировать ну и когда мы говорим про идентификацию вот как бы в текущем мерида когда у нас там прометея и все остальное как uber нить из ее все остальное с чем мы работаем to open метрик 100 то первая мысль которая приходит в голову это лейбл set но я думаю все знают что такое лейбл сад набор пар ключ-значение например если мы храним данные по сыпью с узла кюриосити с hidra номер четыре то вот этот этот временной ряд можно идентифицировать таким лейбл сетом ну та же самая пример для пода тоже сыпью с пода endever и знаем space а марс там судьба такого-то так вот значит у нас получается миллион временных рядов и для каждого временного ряда у нас получается лейбл сад который мы можем уникально идентифицировать временной ряд дальше мы много будем говорить про лейбл сеты владимир много будет говорить про лэйбл сеты и для того чтобы проще было показывать мы везде будем использовать фигурные скобочки и слова л.с. не удивляйтесь еще момент слой был satomi самим по себе работать не очень удобно ну просто технически да и во многих но я бы сказал обо всех tm series база данных которых я видел на самом деле лейбл сад и напиться в некоторое внутреннее адэшника они там разные совершенно природы в некоторых ситуациях там даже позиции файле но не суть а эти штуки уже мотаются во временные ряды то есть у нас есть mapping между лейбл sata мечником а временные ряды мы идентифицируем поедешь ником давайте соберем все это вместе журнал на ssd оперативная память миллион временных рядов у каждого из временных рядов есть ли был сад которым он идентифицируется в оперативной памяти у нас есть специальная метка когда мы пишем 1 лейбл сад мы идем в эту папку и там ничего нет она пустая соответственно мы выделяем этому временному ряду идиш ник и запоминаем что этот лабаз от имеет такой то яичник и записываем информацию о том что мы выделили этот айдишник этому лэйбл сату в журнал плюс журнал мы записываем самую точку только раньше в упрощенном примере я говорил о том что мы записываем 16 байт значение и таймс темп на самом деле мы записываем айдишник значение перстень я думал что это логично и супер понятно но и все так же мы пишем в память уже сгруппировано и значение для всех остальных источников мы повторяем то же самое добавляем необходимый labels от tuoi диме пинге в память добавляем в журнал и митинги и сами точки ну и добавляем собственно сгруппированной а данная память и это был только как бы первый проход нам первые 61 61 и если мы собираем данный раз в 60 секунд а это были первые 60 секунд при последующем проходе скорее всего мы не будем добавлять новые маппинге но больше часть потому что скорее всего большая часть временных рядов сохранится большая часть из миллиона то может быть часть исчезнет часть новых добавится но большая часть останется поэтому журнал мы добавляем только новые точки ними пинге ну и в память мы добавляем новая touch . разумеется спустя время мы сбрасываем блок потом другой и так далее так далее так далее у нас копится блоки на то есть вся схема работает также вот я бы сказал примерно так устроена любая time series база данных когда мы рисуем график происходит очень простая штука к нам приходит запрос в котором указанный был сад мы ищем эти лэйбл ct во всех файлов на диске и в оперативной памяти находим айдишники и по этим айтишником находим уже временные ряды кстати о хищнике в некоторых системах они глобально уникальное в некоторых системах один и тот же лейбл сад в разных файлов может иметь разные техники это детали реализации но принцип примерно такой же везде но разумеется нужно эти временные ряды из разных мест собрать в один временной ряд который уже нарисовать но тут я думаю что никакой сложности нет понятно как это делать там есть timestamp и соответственно не просто пытаемся темпом сортируются с чем мы разобрались что мы поняли первое мы поняли то что обязательно нужен в time series базе переворот на 90 градусов второе мы поняли то что нам нужно как-то идентифицировать временные ряды нам нужно как-то не цитировать метрики и для этого используются например лейбл сатану и чаще всего лейбл считая ну и мы поняли что вот примерно вот так организовывается tm series баз данных при этом это журнал это активный блок и есть еще завершенные блоки ну и мы также проговорили что завершенные блоки обычно объединяются в более крупные завершенные блоки в целом такой подход называется лог страх черт мерч 3 единственное что это не совсем в прямую лог страх смерти это некоторая вариация на тему blog structure мерзкий но принцип ровно тот же и если мы посмотрим где еще используется паттерн хранение данных хранение обработки данных лог строк смерш trade of time себя с базы данных он используется и в influx биби и виктория metrics и впрямь это единственное что в infix биби и в виктория metrics написано прям мы там серия с баз данных которая построена на принципе logs and try to frame ты едва не написано но вот все что я вам до этого рассказывал это именно ну то есть я в упрощенной форме рассказывал имена как устроен стоящих по сторож прометея но опять это приемлемо применимо ко всем ко всем системам то есть прометей несмотря на то что не говорит что он лог страх черный h3 по факту он является вариацией на тему lock straps and merge 3 а база данных всеми нами любимый house всеми нами опять же ки любимая кассандра big ты был который лежит но на самом деле в основе всего и hbs и многое-многое другое тоже на базе экстракт черт мерзкий то есть это не только потом который применяется не только в там сидя с базы данных но и гораздо шире ну ок метр мы подобрались как метру на самом деле очень интересно что в окне 3 ну и наверно удивительно до что в окне 3 тоже используется в текущем старичок метод уже используется вариация на лак страх черт мир 3 и теперь разобравшись с тем как устроена там силе с базы данных вот со всеми этими штуками поняв основы давайте посмотрим что у ак метра внутри устроим это и какую вариацию сама не реализовали вот как метра началась еще в 2012 году в то время никто еще практически ничего не знала то есть д.б. кроме теллуса вообще не существовало и у большинства пользователей в качестве основной системы мониторинга использовался zabbix сейчас от метра состоит из кафки кассандра elastic сеча memcache и большого количества кода на гол от метры как и любая другая система мониторинга начинается с метрик для сбора метрик используется ок агент о cogent это бинарный файл написанный на языке гол который устанавливается на сервера на виртуальные машины в купер над с классе вам кластер может запускаться в докер контейнеры после его установки и запуска с он сразу же начинает собирать метрики собирает он сын джеймса спички фпс операционной системы с баз данных в общем с всего до чего он может дотянуться и далее он оставляет эти метрики в коллектора коллектора уже коллектора тоже часть ок старриджа и располагается он в нашем облаке прежде чем начать принимать метрики от агента от метров производит аутентификацию сначала он проверяет локальный кэш если там не одна ходится необходимых данных для аутентификации он обращается в memcache если это мне находится данных то тогда он уже идет в кассандр и кассандра во многом является неким источником правдой и не только для для аутентификации после того как коллектор проверил аутентификацию агент начинает отправлять точки а коллектор начинает их обрабатывать ну во-первых коллектора сохраняет мэппинг ой бл сет войди в кассандру также чтобы не создавать огромные нагрузки на кассандра он сохраняет это в локальный кэш и на всякий случай в кластер bang и шеи во-вторых лэйбл сет целиком сохраняется в ластик сеть для да не дальнейшего полнотекстового поиска ну и после этого точки отправляются в кафку интересный факт на самом деле ок метр не взаимодействует с отдельными точками а использует концепцию bunchy что такое bunch представьте у вас дома есть шкаф и в этот шкаф вы можете положить в один ящик футболки в другой ящик штаны в третий ящик пиджаки можете поступить по-другому вы можете в ящик сразу складывать подготовленные комплект одежды и в первом случае чтобы одеться вам необходимо открыть один ящик второй ящик третий ящик а достать все необходимые элементы одежды и начать одеваться а во втором случае вы можете открыть один конкретный ящик выбрать в нем необходимый комплект одежды и сразу же начать одеваться вот второй подход лег в идею bunchy метрики группируются по каким-то заранее известным правилам и хранятся группами ну например у нас есть buncha в котором хранятся все метрики относящиеся к secu есть другой bunch в котором хранятся все метрики относящиеся к м идея с бойцами прикольная она позволяет во первых ускорить процесс записи потому что теперь мы пишем немаленькими пачками а группируем отдельные точки в bunchy и пишем пачку данных сразу же и очевидно это происходит быстрее с другой стороны когда мы строим графики очень редко мы хотим посмотреть как график по какой-то конкретный 1 метрики чаще всего мы смотрим графики по группе метрик ну например вряд ли вы будете смотреть загрузку ядер сепию по каждому ядро отдельно вам потребовалось ну вы захотите посмотреть всю картину целиком и это как раз-таки и идея с бойцами мы располагаем всю информацию а нагрузки на гидра в одном bunchy и чтобы построить график в дальнейшем по загрузке достаточно вытащить этот баньши и мы сразу же можем начать строить график но как часто бывает в любых хороших идеях есть нюансы и как и в случае с комплектами одежды чтобы эта идея работала нам нужно знать все случаи для которых нам нужны комплект одежды потому что вам могут позвонить 8 вечера и предложить резко поехать куда-нибудь в южную стану и у вас может не оказаться просто нужного комплекта в окне ты тоже самое поскольку сбором данных на данный момент занимается о cogent мы знаем все метрики которые он собирает мы знаем правила по которым мы хотим их группировать идея с большими от работает отлично как только мы захотим а мы захотим отправлять в него данные из внешних источников ну например у нас есть почти 120 миллионов метрик в памяти усах и мы можем использовать ок ок метры как long-term старт в этом случае мы не знаем какие метрики будут приходить и соответственно мы не можем поплясать правила группировки их в bunch и получается что в общем случае дизбанд большими классная но она не решает задачу чтения и записи в дальнейшем я не буду использовать термин bunch для простоты будем считать что ок метр работает с отдельными точками и того получается что в коллектор поступает сыр и метрики коллег так сохраняет mapping одевай balls эту кассандру и отправляют в кафку точки кажется мы где то это сегодня уже видели действительно то же самое тот же самый подход используется в тест db в журналах данные находятся в кафки и следующий компонент находится с другой стороны кафки он давайте будем его называть формирователь блоков данный компонент вычитывает точки из кафки формирует в памяти четырехчасовые блоки и сохраняет их в к санду данные из кафки формирую в памяти блоки и сохраняем их в долгосрочное хранилище это же тоже сегодня было все правильно и эта концепция блоков которые используется в тсд b и все выглядит очень просто и очень легко но есть небольшое количество подводных камней давайте взглянем чуточку подробнее на то как работает формирователь блоков как мы уже выяснили он читает кафку и формирует 4-часовой блоки в памяти но сейчас в от метр поступает более 20 миллионов метрик в секунду в секунду и хранить все эти метрики за четыре часа в одном блоке просто невозможно далее с ним ну будет не как оперировать поэтому формирователи блоков используется сортирования и он одновременно формирует большое количество блоков если же мы будем from начинать формировать блоки в в один момент то и заканчивать формировать мы будем их тоже одновременно что в свою очередь приведет к тому что все эти блоки одновременно будут записываться в кассандра и очевидно что кассандра не выдержит такой нагрузки поэтому в окне 3 сделана немножко по-другому и блоки начинают записываться него в один момент а начало блоков разнесено немножко во времени и собственно это позволяет нам заканчивать запись блоков в разный момент что приводит к тому что в к санту они также сохраняются не иди на момент на и это позволяет нам сбалансировать нагрузку на к сандова и кажется что проблема решена но при этом появляется другая проблема от откуда нам читать данные которые еще не сохранены в кассандру потому что если попробовать запросить данные за какой то определенный промежуток времени окажется что в одном блок нету ни одного блока который содержит полного набора данных поэтому в окне 3 есть еще один формирователь блоков 2 типа принципы его работы очень сильно похож на принцип работы формирователя блоков первого типа он читает точки из кафки и в памяти формируют блоки определенного размера за одним существенным исключением что все блоки начинаются в один момент во время старта он вычитывает блоки и начинает заполнять начинается заполнение когда блок полностью сформирован в кафки же постоянно прибавляются данные то он продолжает их вычитывать и новые данные начинают вытеснять самые старые данные по сути это реализация fest in first out и при таком способе работы с блоками мы всегда мы знаем какой временной промежуток находится в этом формирователи блоков и можем корректно из него читать данные на данный момент в окне 3 используется формирователи блоков а по сути это некий кэш 2 промежутков часовые и 4 часа вы часовые используются для того чтобы во-первых отображать данные для графиков а во-вторых по нашему опыту большая часть триггеров для проверки условия используют не более часового интервала с другой стороны формирователь блоков держит все метрики у себя в памяти и это требует достаточно много ресурсов а с третьей стороны проверка триггеров это самые частые запросы в сторону поэтому сделано деление на часовой который прочим масштабировать и по сути дешевле и 4-часовой который позволяет нам перекрыть вот то время пока блоки формируется и еще не успели сохранится в кассандру и уже исторические данные мы можем читать из кассандры и получается примерно следующее упрощенная упрощённая схема работы от метра у нас есть кассандра в которой содержатся маппинге улыбался твой где есть elastic search в котором содержатся weightless это для полнотекстового поиска есть коллектор плюс кафка которая реализует концепцию журналов есть формирователь блоков плюс кассандра которые отвечают за запись есть часовые 4 часовые каши плюс кассандры которые используются для чтения данных из тораджа и получается что в целом в окне 3 реализовано те же концепции что и втс db за одним отличием что в нем достаточно большое количество разнообразных элементов и отсюда возникает следующий логичный вопрос а насколько в целом это надежно но прежде чем ответить на этот вопрос давайте вообще разберемся что что что есть надежно по отношению к кометам ну во первых от метр так же как и любая система мониторинга не должна терять данные и это ну такой совсем очевидный факт во вторых нам нужно обеспечить доступность часовых данных потому что именно на этом интервале проходит производится проверка триггеров и отправки alert of a система которая не может своевременно уведомлять о каких-то проблемах однозначно не может считаться надежным после того как мы получаем уведомление проблемах нам нужно понять вообще почему сработал тот или иной триггера и для этого нам требуется графики и надежная система должна нам позволять посмотреть графики как минимум за 4 часа но иногда четырех часов бывает недостаточно поэтому нам нужны и исторические данные ну и как вы понимаете все это должно работать быстро потому что если у вас график за 30 минут строится 10-15 минут системой просто нельзя пользоваться и однозначно ее нельзя назвать надежным чтобы обеспечить такую надежность на наш взгляд необходимы чтобы систему удовлетворяла следующим требованием во первых не она должна позволять много и часто выкатывать обновление в этом случае мы можем спокойно выкатывать обновление а если же мы не можем этого делать то каждое новое обновление подобно хождение по льду и любое неосторожное движение может привести к тому что будут серьезные последствия которые будут заметны всем окружающим в том числе и нашим пользователям если мы говорим про высокую доступность то система должна уметь самостоятельно восстанавливать свою работу поскольку как только в любом процессе появляется человек система начинает работать существенно медленнее ну и должна быть обеспечена возможность быстрого масштабирования поскольку в зависимости от разные нагрузки чтобы удовлетворять требованиям быстро во время ответа нам необходимо обеспечить возможность масштабироваться ну и последняя но на мой взгляд самая важная система может падать то есть отдельные компоненты в системе могут падать и это нормально главное чтобы эти падения не приводили к не аффект или пользователей мыло фронте в целом считаем что но бывает построить систему которая никогда не падает это тупиковый путь и намного более правильно строить так сказать клауд на этих приложения которые готовы к тем самым падением поскольку причин падений может быть огромное количество у нас может перегреться сервера и выключить выключится ошибка в операционной системе dash все что угодно может идти уборщица задеть патч-корд и мы потеряем сетевую связанность между какими-то компонентами по сути своей падение отдельных компонентов системе это очень естественный процесс он очень похож на то как люди чихают если мы чихаем то в целом ничего не должно происходить страшно ну вот что произойдет страшного если человек просто чихнул но до в эпоху к вида можно кого-то напугать но в крайнем случае мы можем кого-то испачкать соплями но из-за этого просить людей никогда не чихать кажется очень странным поэтому правильно проектировать те системы которые спокойно относятся к чихания как же обстоят дела с от метра и что происходит когда он чихает вот когда чихает от метр это выглядит достаточно феерично с большим количеством спецэффектам а взрывной волной задевает многих кто находится рядом почему так происходит давайте посмотрим на конкретных примеров и поскольку мы уже выяснили что чихать в целом это нормально попробуем просто выключать отдельные компоненты и смотреть как как метра реагирует на это чьих 1 формирователь блоков для начала давайте просто перезапустим формирователь блоков и посмотрим что произойдет на первый взгляд не должно произойти ничего страшного поскольку данный компонент не имеет никакого прямого взаимодействия с пользователями тем более у нас есть еще и 4 часовые каши которые дают нам запас по времени в четыре часа чтобы восстановить работу этого компонента единственное что нам нужно обеспечить чтобы вот он успевал стартануть за эти четыре часа что нужно сделать чтобы запустился формирователь блоков ему нужно вычитать 4 часа данных из кафки и очевидно что он делает это быстрее чем 4 чем за четыре часа поскольку если бы он за четыре часа не успевал обработать 4 часа метрик то в целом бы эта система не работала но давайте взглянем подробнее на то как происходит процесс запуска формирователя блоков он читает точки и сказки читают одну точку читает 2 читает третью тут астрологи объявляет день падения формирователя блоков и наш формирователь блоков выключается на его месте запускается следующий он определяет последнюю обработанную точку и смещается относительно нее на 4 часа в прошлое после этого он начинает читать читает читает читает и прекращает свою работу на его месте запускается следующий и он делает ровно то же самое определяет последнюю точку отступает от нее четыре часа в прошлое и начинает обрабатывать точки данный алгоритм приводит к тому что если формирователь блоков перезагружается один раз то ему нужно вычитать 4 часа точек если он перезапускается два раза подряд то ему нужно вычитать уже восемь часов дальше 1216 но и так далее пока точки в кафки не закончится и если мы перезапускаем его больше одного раза то это приводит к тому что данных в кассандре еще нету а данных в кэше уже нету потому что время его запуска начинают превышать те самые четыре часа и получается что при перезапуске блоков во первых мы не удовлетворяем требованию надежности доступности исторических данных а с другой стороны мы не можем быстро и часто выкатывать обновления потому что во первых он сам по себе очень долго запускается а плюс к этому очень нежелательно несколько раз подряд его перезапускать и мы не можем быстро выпускать хотфиксы если с нашим обновлением что-то пошло не так нам приходится делать полный rollback исправлять и спустя какое-то продолжительное время делать повторную попытку ваката выкатывания изменений окей с этим разобрались еще один важный вопрос можно ли это исправить до на самом деле можно исправить и это делается достаточно просто например также как это реализовано на данный момент в про металась периодически можно делать запись на диск по сути снапшоты текущего состояния блоков памяти с одной стороны это позволит нам ускорить процесс запуска с другой стороны это избавит нас от необходимости отматывать каждый при каждом перезапуске 4 часа прошло в кафки чьих 2 cache с этим компонентом все еще проще если мы перезапускаем каши то очевидно что пока они не станут пользователь не будут видеть метрики при этом у нас есть часовые и 4 часовые тише и здесь мы сразу же перестаем удовлетворять требованию доступности данных за короткий и средний промежуток времени и важный вопрос как быстро мы можем восстановиться после их перезапуска на данный момент часовые каши стартуют парят порядка десяти минут а вот четырехчасовую уже порядка 40 минут очевидно что в это время пользователи не видят метрики но хуже того что пока перед перезапускается эти компоненты мы не можем обрабатывать alert и и получается что вы сидите пьете кофе у вас нет ни одного алерта а на самом деле 10 минут как у вас уже догорает сервер на и на самом деле 10 минут это достаточно длительный срок в современном мире где клауд на этих и приложение должны стартовать там несколько секунд ну минуту максимум и в кашах на самом деле есть еще одна проблема поскольку это формирователи блоков они держат весят объем данных за определенный промежуток времени это занимает достаточно много оперативной памяти на данный момент 4-часовой каши занимает порядка 50 двух гигабайт если мы захотим ну допустим увеличить количество отправляемых метрик эти раз то это уже будет там под 300 гигабайт мы столкнемся просто с физическими ограничениями которые нам не позволят масштабироваться можно ли решить эту проблему до тоже можно например мы можем использовать тот же подход который используется на данный момент в город такси это шарди рование чтобы каждый кэш содержал не полный набор данных а только какую-то его определенную часть а если мы сюда еще добавим сохранение snapshot of the это позволит ему еще и быстрее запускаться чьих третий последний mapping айди way ball set и за mapping за хранение маппинга fat ли отвечает несколько компонентов это коллектор кластер моим кэш и кассандра давайте попробуем перезапустить два из них во первых это моим кажется во вторых коллектор при перезапуске коллектора очевидно что агенты начнут к нему перри подключаться и во-первых они начнут проверять аутентификацию после того как аутентификация проверено они начнут отправляйте play ball set и но локальный кэш пустой и кэш memcache а пустой поэтому коллектору придется ходить в кассандру для того чтобы убедиться что такой mapping одеваю был сет уже есть и если вы помните то в окне 3 сейчас собирается порядка 24 миллионов уникальных метрик то есть по сути это 24 миллиона запросов в кассандра очевидно что она не может справиться с таким на груза с такой нагрузкой сразу же и часть запросов просто не выполняется это приводит к тому что агенты отваливаются потом пытаются переподключиться еще раз еще раз еще раз и это все продолжается пока не прогреется каши каши прогревается порядка 30 минут и это время уходит на то чтобы вот обработать эти 24 миллиона запросов а на самом деле их больше потому что часть агентов успела отправить какое-то количество лэйбл сетов потом она переподключается и заново заново заново это в свою очередь приводит к тому что у нас не поступают актуальные метрики пользователи не видят данные мы не можем обрабатывать alert а потому что но у нас просто нету данных на которых мы можем проверить alert и что-то обработать им что-то какие-то уведомления отправлять пользователям можно ли решить эту проблему до тоже можно ну например мы можем отправлять в кафку не метрики в видео иди ой бл set a метрики с айбол с этом сразу же и в этом случае нам не нужно будет так сильно издеваться над кассандры и как мы видим uac метры есть ряд проблем но при этом он все равно классный он все равно классный в нем реализованы те же принципы которые используются в современных системах мониторинга использующих тсд б у него есть большое количество оригинальных уникальных fitch ну например те же bunchy но с другой стороны у него есть большое количество подвижных частей которая заставляет нас сидеть на карантине и это приводит к тому что мы не можем начать реализацию всех тех фич о которых вначале говорил андрей прежде чем мы сможем начать их реализовывать нам нужно решить текущие проблемы и тут есть главный вопрос а нужно ли решать эти проблемы в 2021 году или может быть стоит посмотреть на уже готовые 100 раджи в которых эти проблемы уже решены интересный вопрос сейчас в авто рассказывала я подумал что опять как это все проблемы программирования сводится к неймингу и их в редакции кушаете в очередной раз посмотрите мы сегодня поговорили о чем о том как мы здесь оказались какая у нас ситуация вообще какую проблему мы решаем мы поговорили о том как в общем устроена грубо говоря любая time series база данных понятно что есть очень много деталей реализации понятно что есть всегда нюансы особенностей так далее но принципы с минус одни и те же и мы поговорили а в архитектуре от метра до о том как вот текущей сторож который был в оригинале в окне 3 устроен а его сильных сторонах и главное наверное сильная сторона это очень хорошее время ответа но за счет использования bunchy так называемых очень хорошее время ответа и ну такая небольшая надежное хранение данных потому что по факту но нас кейсов то что мы какие-то данные не доехали не бывает но вот бывает повар сказал о чем мы не поговорили и о чем хочется поговорить дальше во первых о том как мы переезжаем на cortex во вторых и что называется главное зачем мы переезжаем на кортах и почему именно на cortex и как я уже говорил мы планируем потом переезжайте то есть мы сразу сейчас уже знаешь мы будем съезжать с cortex а зачем мы переезжаем на cortex если мы планируем дальше с него съезжать ну и как я уже говорил мы этим докладом открываем сериал про ну time series баз данных ок метр и так далее но и чтобы не пропустить новые серии подписывайтесь на youtube канал фланта как прежде чем закончить доклад хочется сказать гигантское гигантское спасибо прежде всего команде от метра которая с горящими глазами и cortex внедряет и толст cortex будем это так называть огромное спасибо антон акимова который тридцать-сорок сороковую презентацию нашего с ним месте ну и в этот раз ребятами делает и огромное спасибо фланта за то что мы вообще можем всем этим заниматься мы обещали слайд с ключевыми ссылками демонстрация фунтов ского мониторинг окубо это одна ссылочек демонстрация от метра это другая ссылочка но и несколько релевантных докладов доклад гипса кажется 2 о 2 года назад про то как мы делами ценят менеджмент но и доклад so rude comfo pro мониторинг и кубер нить из огромное спасибо с вами были владимир гурьянов андрей колосов дмитрий скляров компания флант ну и андрей с владимиром это по сути команда от метра спасибо спасибо"
}