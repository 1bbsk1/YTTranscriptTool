{
  "video_id": "n7Te9WIz1ho",
  "channel": "DevOops_conf",
  "title": "Сергей Фёдоров — Ускоряем Интернет запросы, и спим спокойно",
  "views": 1541,
  "duration": 3090,
  "published": "2020-02-07T07:55:17-08:00",
  "text": "о о а как уже сказали меня зовут сергей я прилетел из калифорнии работа в компании netflix и сегодня будем говорить про то как делать интернет быстрее и как делать это так чтобы это не влияло на вашу личную жизнь несмотря на представляем то не будет связано с остатком мы говорим про выгорит немножко про другие вещи в интернете но все равно интересны и важны потом можно померить в ходе тоже можно померить но сначала почему скорость интернета важно есть очень много данных от разных компаний которая показывает что скорость интернет запросов она напрямую связана с вашим бизнесом например сфере шопинга amazon и другие компании говорили что каждая дополнительная задержка когда пользователи ждут она приводит к уменьшению продаж все больше и больше у нас становится мобильных устройств мобильных сайтов мобильных приложений есть данные которые показывают что если ваша страница загружается дольше чем три секунды вы теряете примерно половину пользователей более того с недавних пор google включает скорость вашей страницы в калгари панарин king чем быстрее ваша страница тем больше у вас будет пользователей тем больше google будет направлять трафик а к вам но это не только связано с поиском или шопинг есть и другие применения например финансовой организации где задержка последнюю она абсолютно критично например несколько лет назад компания хабер не над works она закончила прокладку кабелей между нью-йорком и лондоном который стол 400 миллионов долларов только ради того чтобы уменьшить задержку на шесть миллисекунд из примерно 66 миллионов долларов за каждую миллисекунду времени неплохо задержка этот один из факторов которые влияют на скорость который видит пользователь и я иду я я здесь представляю над fix мы делаем streaming через интернет то же самое делают многие многие другие компании мы знаем что есть другие факторы которые влияют на качество user experience например мы изучаем связь между качеством видео между задержкой сколько пользователь ждут чтобы начать стримить между количеством либо форекс когда когда вы видите спиннер когда у вас переливается качество но это не только то что нам важно нам также важно чтобы у нас быстро загружалась приложение быстрый браузинг быстрый был поиск мы видим что тем большим возникает и работает тем тем больше пользователей смотрят наш контент есть данные которые показывают что для типе чисто для среднего сайта после примерно 5 мегабит в секунду скорость интернет мегабит в секунду перестает напрямую влиять на а скоро загрузки обычной страницы однако между задержкой соединения и скоростью замка загрузкой сайта зависимость продолжает быть линейной это как что о чем я буду говорить сегодня сегодня в остальном я буду фокусироваться на уменьшение именно задержки британцы в интернет запросах я была говорить о том как это сделать почему это может работать немножечко поговорим о низкую ранних вещах пресс при сетевые протоколы я расскажу как мы это решаю на 30 с нашей инфраструктурой и большая часть моего доклада будет сфокусирован на том как это сделать с практической точки зрения как сделать это так что вы мы все равно смогли заниматься разработкой они поддержкой сложных распределенных систем я надеюсь что после доклада вы узнаете немножечко узнаете или вспомните о том как работать сетевой протокол и как они влияют на задержку интернет запросов вы знаете о том я на примере практического подхода как natrix подходит к решению сложных задач где постоянно все ломается в есть очень много переменных как мы это делаем успешно с достаточно мамой маленьким количеством людей и как мы проектируем такие системы как мы их оперируем как его их мониторим как мы убеждаемся в том что все у нас работает хорошо прежде чем я начну давайте сделаем быстренький опрос кто вообще смотрел или смотрит кино фильмы или сериалы давайте поднимем руки интересно кто является активным подписчикам я не будь те кто опустил руку и него я я не я не буду спрашивать где вы смотрите я лучше расскажу вам про то что у нас внутри нас версия как он работает многие думают что на секс это просто веб-сайт streaming ничего сложного на самом деле там немножко больше у нас на офисе работает с по сути на любом устройстве где есть экран который может получать видео трафик у нас тысячи различных устройств которые поддерживают наше приложение у нас есть четыре разные команды которые делают по сути разные версии приложения для android для ios для тире платформ для веб-сайта и мы очень много сил тратится на то чтобы персонализировать его experience мы очень много запускаем обе тестов по ралли параллельно мы делаем так чтобы каждому пользователю наслаждался и мог найти лучший гонконг кантон для себя вся эта я персонализация она рассчитывается в нашем облаке у нас есть богатый набор микро сервисов которая перед подсчитывать пирс анализируют данные для юной также у нас есть control plain который направляет трафик есть нашу инфраструктуру с телеметрии спектре the young 1 gom и так далее и так далее прицелом это на самом деле выглядит примерно вот так это расстояние того как у нас все это соединяется как ты работает с левой стороны вы видите как бы edge entry point дальше все это идет разными красивое своих поддерживать разные команды они достаточно независимы друг от друга действуют однако это еще не все немногие люди знают что у нас есть ещё один очень важный фактор нашей инфраструктуры это наш сериал в кантон должен at work которые на самом деле доставляет все это видео с контентом который вы смотрите помимо этого он еще доставляется и картинки у нас там везде на сервис и выглядит эта сера примерно вот так это красивый красный сервер который мы дизайне им которую для нас специально делают которую мы отсылаем по всему миру и этот ящик он по сути наполнен ssd и жесткими дисками у нас есть оптимизированы и везде системы с яндекс перед ней и мы тянем этот серый для того чтобы он как можно больше трафика мог доставлять нашим пользователям и выглядит это примерно вот так это пример нашего дипломата в интернет excel это большая стена где есть это мои ноги много этих серверов они подключены к раутеру которые наша натура команда поддерживает и 3 минут одна такая стенка она может доставлять примерно один процент от всего мира сего трафика в интернете таких мест у нас много у нас есть примерно 70 80 мест где у нас стоят вот такие большие стенки которые мы поддерживаем сами это по сути в интернет и где интернет интернет провайдеры приходит и и соединяются друг с другом но это еще не все мы такие сервера также отсылаем интернет провайдером напрямую чтобы они помогли подключите к себе в сеть и сэкономить много денег на свой трафик и улучшить experience для наших пользователей и в нашей инфраструктуре все эти сервера в наших интернетах чинджеров они соединяются тем что называется backbone network это по сути физически оптоволоконные кабеля которые мы которые мы покупаем которые мы полностью менеджером и они соединяют наши интернет и очень злаки сеанс друг с другом и с нашим amazon плавно чтобы мы могли обновлять нашу библиотеку эта инфраструктура сегодня доставляет примерно одну восьмую часть интернет-трафика в мире в пиковые часы примерно переть трафика северной америке где мы уже достаточно долго и успешно существуем но я думаю что самые впечатляющие факт это то что все это делается очень небольшой командой меньше 150 человек включая инженеров operations at логическую бизнес людей и так далее меньше 150 человек ответственна за восьмую часть интернета и что там делаю я я изначально выпускник вмк между русского государства университета имени лобачевского до intel а я немножко поработал в интервью microsoft и но последние семь лет я работал нетфликсе я был один из первых инженеров который работал над платформой и и инфраструктурой нашего сидела за это время я написал мониторинг систему достаточно много тузов для анализа данных force.com которые уже был упомянут но последние несколько лет я работал над проектом который фокусируется на тем чтобы использовать нашу инфраструктуру для ускорения динамических запросов в облака как бы то что нельзя за каши ровать статически массе dent и об этом я сегодня буду говорить о том как мы это сделали итак сегодня у нас есть три региона зоне у нас есть это по всему миру и в целом общая задержка будет зависеть от того насколько клиент далеко от нашего от нашего облака чем дальше клиент находится тем больше у него будет задержка и в целом это достаточно неэффективно идти через весь мир также у нас есть множество наших серверов в нашем сидел вопрос можно ли как-то использовать этот сидел чтобы эту ускорить при том что мы не можем или почитать мы не можем закодировать это потому что каждое пей запросу он уникален это один из факторов что нас персонализированный интерфейс а то есть если в том что если мы сделаем прокси на этом седин сервере и начнем через него прогонять трафик будет ли это быстрее работает если какие-то преимущества такого подхода давайте съел подумаем кто думает что если мы так сделаем хотим ее даже идем в клауд мы все равно сможем сделать быстрее кто думает что это будет быстрее кто в этом сомневается кто думает что не будет значительного ускорения 50 давайте немножко вспомним то как работают сетевые протоколы я постараюсь вас убедить что это может работать сегодня большей части 5 рафик у нас идет через протокол и степи или чаще ftps который зависит от протокола sip нижнего уровня тисе пи и тел с и 1 штука делает о том подключается к серверу чтобы подключиться ему сделать нужно сделать рукопожатие или раньше как английски и это занимает взять защищенного тело соединить это занимает в целом как минимум три раза чтобы установить соединение плюс один ряд что вы передать данные поэтому если у нас клиент находится на расстоянии 100 миллисекунд просто скорость света через провода перейти у нас занес 14 миллисекунд просто чтобы получить 1 бит данных если же мы наши сертификаты для тела с положим на седин сервер и у нас игнорировать ближе им в как вы видели на карте мы это расстояние можем сделать знать мы этот эту рукопожатие можем значительно сократить в этом случае первые три операции у нас идут только до силен и вместо 300 миллисекунд мы тратим 120 и дальше им отряд им 100 миллисекунд на то что вы уже доклада нам по устанавливаться в них дойти и скачать данные мы почти два раза сократили временно то чтобы получить данные вот сервер но это еще не все потому что после того как соединение уже установлено 10 протокол работая таким образом чтобы он увеличил увеличивает то что называется can женщин window количество информации который может по этому сиденье передаваться но такая штука что если мы потеряли пакет-то открыто окно схлопывается два раза и чтобы она восстановилась это опять же нужно она зависит от полного расстояние от клиента до сервера в данном случае так как у нас connection задней не идет только до седин сервера у нас это выставление будет идти быстрее более того нет он как дороги там тоже могут быть пробки и интернете могут особенно в пиковые часы люди смотрят видео люди скачивают файлы люди делают бэкапы и все это соревнуется за опасную способностью с нашим небольшим по объему но очень зависим от задержки графиком в нашем случае мы можем контролировать сеть между нашим семьям и облаком потому что съесть свой backbone мы можем сконфигурировать настройку так что небольшие пакеты которые зависимы от задержки не бери активируются а более большие потоки информации они пойдут чуть чуть чуть чуть позже это то что мы не можем сделать в открытом интернете потому что никто не контролирует полностью инфраструктуру думай о протоколах над 10 петель с сейчас много говорят про протокола степи 2 the quick про другие вещи в нашем случае у нас есть клиенты у нас есть устройство которым 10 лет который мы не можем обновить некоторые клиенты не хотят обновлять и они не смогут пользоваться последними достижениями если же мы между нашим семьям между нашим прокси и клаудом сделаем отдельные соединений мы можем контролировать обе части мы можем использовать писание настройки которые оптимизированы под то что мы хотим сделать и неэффективная часть на старых протоколах у нас пойдет только между клиентом и сидена той же короткой части более того мы можем делать мультиплекс запросов с нескольких запросов с клиента на один и тот же запрос на наши уже установленное соединение между семьям и облаком и так это было немножко теория я не буду вас больше мучить надеюсь немножечко вспомнили чему нас учили университете да бывает полезно и так может ли это помочь да и же день в том что вместо того чтобы идти напрямую мы делаем прокси через интернет давайте еще раз построим то это классическая инфраструктура динамические запросы идут в облака статика населен мы вместо этого переносим наши прокси на наш сериал и весь трафик идем через него будет ли это быстрее кого я смог переубедить или кто уже думает что да давайте поднимем руки если кто-то что кто думает что нет отлично у нас есть конце один человек 2 ну почти консенсус что мы под будем делать ну теория говорит что можно строить давайте делать план собирает команду бюджет через шесть месяцев наверно точно получите сделать как бы четкий срок анонсируем релиз и к счастью так не делается мы после того как у нас есть какая-то идея мы должны доказать должны измерить почему это сработает поэтому нам нужно понять будет ли это быстрее будет ли это надежнее насколько это будет сложно сделать и сколько это будет нам стоит с точки зрения инженерных инфраструктур их задач сегодня я пишу как мы делаем анализ к скорости на остальные вопросы мы похожим образом отвечаем поэтому что нам нужно для анализа скорости мы хотим получить оценку такой инфраструктура от наших пользователей со всех наших девайсов со всех наших пользователей и мы хотим не тратить очень много времени на этот прототип 2 желательно ничего не сломать пока мы все это делаем иначе у нас будут не любить и так один подход чтобы это оценить мы можем делать пассивные мониторинг наших запросов можем просто мерить сколько сейчас у нас запроса занимает у нас получится полное покрытие строить полный этот покрытый пользователей но мы не получим очень стабильный сигнал потому что запросы могут большие или маленькие они могут долго занимает времени на викенде они могут долго занимает времени на клиенте плюс мы же не можем перенаправить их через прокси или иначе нам нужно как бы прокси стабильным делать есть риск что-то сломать альтернатива использует лабораторные тесты делать специальная сервера в которых мы делаем специальные тесты меряем что мы хотим полностью контролируем но сделать сервер в африке с телевизором действительно давности немножко сложно нам хотелось чтобы вы сделали вы получили оба преимущество мы это сделали сами оказалось что достаточно просто как это выглядит это мы написали небольшой кусочек кода не бачу агент которые мы включаем как часть нашего приложения вы его называем пробуй и он позволяет нам делать сетевые тесты с наших устройств которые полностью контролируемые как это работает вскоре после как приложение загрузилась и начальной активность закончилась пользователь думает что америка что ему делать мы гоняем несколько тестов желательно до того как начался начался streaming чтобы мы не повлияли на 1 то как быстро у пользователя все работает в продукте в этот момент я делает запрос на наш сервер и просят как конфигурацию теста которые ему нужно запустить кооперация тесты это по сути несколько to get off несколько адресов откат из которых нужно скачать информацию и померить сколько времени это занимает из другие параметры которые говорят как сколько раз отмерить какой последовательности и тогда и так далее мы можем занять несколько таких тестов параллельно что мы делаем у нас есть разные конфигурации и мы случайным образом нашим бэг-энде решаем какой тест мы дадим а дальше с помощью агрегации мы уже выбираем результаты при данных тестов santos с клиента выглядит примерно таким образом у нас есть несколько адресов мы параллельно делаем запрос то каждый из них образом что у нас не было проблем с тем какой-то с 1 был какой второй и так далее мы верим сколько это заняло времени и ну и в целом делает несколько раз у нас есть несколько пультов в первом курсе мы верим сколько у нас заняло установить соединение и скачать какие-то данные на втором после мы смотрим сколько у нас за него скачать данный пол по уже установлены на соединению и дальше можем делать что-то еще например подождать 30 секунд еще раз попробую скачать посмотреть что будет и мы в этот момент меряем все что у нас получится замерим сколько у нас dns занял уверению сколько эти цепи чтобы ставить соединение сколько тел с за какое время вы получили первый бит и сколько в целом времени у нас заняло чтобы скачать и также смотрим сломалась на что то или нет здесь важный момент что мы по спасти независимо от производительности продукции устройства и почти не зависимо от того что происходит у нас в облаке те данные которые мы скачиваем это просто случайный какое-то случайно сгенерированные данные и после того как мы все это померили мы загружаем эти метрики для анализа так сегодня мы достаточно активно стали пользоваться такой инфраструктуры она казалась полезно не только для нашего из нашей изначальной идеи у нас 14 разных тестов или что мы своими рецептами больше чем 6000 и результаты в секунду со всех наших девайсов со всех наших пользователей просто примерно яценко для нас если мы покупали бы такой сервис используя корпоративные ценные и так далее так далее то бы нам стоило миллионы долларов и мы не получили бы то покрытие которое у нас есть очень-очень важный момент у нас есть система чтобы измерять теперь нам нужно понять что все-таки мы будем мерить что сделать прототип и этот прототип нам нужно запустить на нашем сильно нашего сидена структура нам нужно понять как соединять клиентов с каждым прокси и сравнивать мы будем то насколько быстро наши пробы идет через прокси или идёт напрямую для прототипа мы пошли самым романтичным путем мы выбрали гол для того чтобы это делать у них очень хорошие network библиотека мы его dipline как static baner и чтобы не было никаких зависимостей и в основном используем стандартные компоненты мы немножечко модифицировали чтобы сделать connection pooling но в целом у нас мы отказались от любых необычные функциональности что вы могли сделать как бы чтобы был простой концепт мы это сделать и чтобы сделать чтобы до привлечь клиентов мы сегодня уже используем между клиентом и облаком чтобы балансировать между регионами мы используем географическую базу данных dns что по сути клиент пойдет к графическому самому близкому региону для того чтобы направлять клиентов через наши прокси в интернет экстент где вот эта большая стенка у нас стоит мы стали использовать tcp и не coast это очень простое превратить иное решение мы используем один ip адрес на всех на север в который у нас там есть мы это можем сделать потому что мы контролируем раутера мы можем контролировать все полость из которые мы используем и да просто с одним айпи сеть от клиента найдет путь к самому близкому серверу с точки зрения остановок по сети с точки зрения хопс к сожалению у интернет-провайдеров мы не можем такое сделать потому что мы не контролируем их роутер и просите так сделать достаточно долго но у нас уже есть логе который направляет клиентов к интернет-провайдерам для видео стриминга и мы просто использовали ту же самую логику чтобы быстренько оценить и так нужно понять какой из этих трех путей у нас будет быстрее в целом и какая будет нос надежность поэтому здесь мы используем нашу для этого мы используем нашу probing систему которая писал каждой из этих путей становится отдельным таргетом и мы смотрим на время которое у нас получилось и наконец то сейчас дам ответ на этот вопрос теперь это то что мы на самом деле смотрим оказалось что все кто ответил были неправы потому что как часто бывает такой ситуации кроме одного двух человек который возможно смотрели слайда я не уверен она не будем об этом таких устройствах чаще не бывает простого ответа оказалось что несмотря на всю теорию есть случаи когда у нас через фокси серна будет быстрее или будет медленнее на этом графике по вертикали мы видим различные регионы и чем левее у нас есть точки чем ярче . тем больше клиентов попадают в такой производительности чем левее тем быстрее в процентах ускорения по сравнению с амазона с тем чтобы напрямую в lukoszaite чем чем правее тем быстрее чем левее кем у нас будет медленнее работать наши практи по сравнению с прямым путем естественно нас такая ситуация не устраивает мы не хотим делать что то что клиенту будет хуже поэтому мы уже получили важные результаты мы поняли что просто прокси все делать через него недостаточно мы получили ожидаемой произвольность наших настоящих клиентов со всех устройств которые у нас есть и мы ни разу не трогали ничего в продакшн мы ничего не сломали что очень важно это значит что нам нужно опять что-то пробовать что-то менять и что-то измерять после этого задача становится такая что вместо того чтобы все-все упускать через прокси мы выбираем самый быстрый путь для каждого клиента нас есть по сути три пути пятью напрямую отправить его через eterna toxins или отправить через наш сервер в интернет провайдере из соломы использовать я буду использовать термик steering я не нашел хорошего перевода для этого но это означает что мы делаем это решение мы 2 клиенты решаем куда его направлять и так как бы узкой мы это делаем для запросов наше облако мы не можем это делать надо коня душном дабы канг нужно собственно попасть сама начала и мы также не хотим слишком много логики делать на клиенте потому что это сложно это долго это нужно много людей просить что то делать но оказалось что можно с такую интеграцию сделать с помощью день с 11 система которая превращает наши на наши пути к ресурсу из из-за строки в api адрес и в этот момент у нас есть наш dino сервис мы можем по сути легенд и решить для этого коня то мы даем ему айпи адрес сервера в амазоне или сервера в нашем сесна 6gen инфраструктуре есть небольшая сложность когда мы это делаем сожалению авторитарной где нас провайдеры не видит api адрес клиента он видит только пи адрес рекурсивно вырезал вера который которым клиент пользуется это может быть их зонах bsp или паблик визовые например google dns поэтому опять же нужно нам чуть чуть больше логике делать вместо того чтобы принимать решения для одного клиента мы принимаем решение для группы клиентов у нас есть наши результаты наши измерения для каждого клиента какой путь до него быстрее мы их агрегирует по рекурсивным револьвером которые не используют и решаем куда их направить или этого решения нам нужно по сути сделать рядов возможно для кого-то это будет тщательно для кого-то будет быстрее и здесь вопрос какой какие метрики у нас важнее это тоже с чем мы должны как бы поэкспериментировать попробовать и в итоге получить результат получить модель которая работает таким образом что наш авторитарный резольвер содержит модель того как какой рекурсивный resorter куда направлять и все клиенты направятся по этому пути также вас будет ли это работать или для большого количества клиентов у нас все равно не получится их быстро ускорить будет клиенты которые очень плохо работают к счастью мы можем тоже это померить мы можем собрать данные посчитать это запустить и опять же использовать нашу систему для измерений по сути один из наших тарик еда становится наш steering то что мы предпочитали на основе исторических данных мы через это пропускаем уже тестовый трафик сравниваем с тем что у нас работает сейчас есть смотрим на ту же самую картинку наконец-то видим то что лично мне сильно нравится все все клиенты у нас с правой стороны либо у них такой же такая же предупредительность как сейчас либо ускорение причем достаточно значительную в два или больше раз мы так опять же мы смогли протестировать определенный подход вы поняли что подход на основе динас может сработать мы смогли подобрать правильные аргументы для нашей модели и мы смогли бы протестировать не трогая продакшен ничего не ломая что опять же очень очень важно и это получил сделать достаточно быстро всего но на самом деле это была достаточно простая часть потому что дальше начинается продукцию нас есть прототип да он работает но его еще нужно запустить для всего трафика почему это сложно потому что у нас больше чем 150 миллионов пользователей тысячи устройств у нас есть сотни микро сервисов и все постоянно что-то меняют клиенты меняет свой продукции игра меняет свой продукт это тысячи миллионы запроса в секунду критичного трафика который мы собираемся поменять как он идет если мы сломаем у нас клиенты будут страдать а направляем все это через сотни и тысячи серверов через интернет где тоже постоянно все меняется поэтому помимо этого все это делает команда из трех человек мы не хотим держать целую армию operations чтобы эту систему чтобы с ней справляться поэтому собственно следующая часть будет про то как нам спокойно с пальчика но это все это сделать не сойти с ума в целом наш вот заключается в том что мы пытаемся уменьшить масштаб поломок которые мы ожидаем и мы думаем что что-то сломается обязательно мы не пытаемся сделать решение которое абсолютно не ломается мы просто думаем о том как с этим справляться на момент проектирования и мы предпочитаем делать это с постепенной деградации если что-то у нас работает не так это должно чиниться с сами само собой пускай это будет чуть чуть хуже чуть чуть медленнее но главное чтобы отработала казалось что это когда ты об этом думаешь самого начала это не так сложно сделать потому что мы можем попросить клиент добавить достаточно небольшой кусочек кода посчитать количество ошибок которая связана с сетевыми запросами которые идут через нашу инфраструктуру и если что-то сломано просто сделать full дак и fall back сделать напрямую в облака как это работало сейчас у нас есть много лет как мы с этим работает это считается стабильно и ты сильный разряд заряд нам руки на то как что мы можем делать насколько быстро мы можем двигаться мы можем делать изменения не сильно боятся о том что мы привьем чите чей-то сеанс наслаждение от секса этом собственно мы по сути должен делать замечательные вещи но не то чтобы мы такие как бы ковбои все безответственно и давайте все менять нет у нас естественно есть достаточно четкая дисциплина попова того как мы делаем изменения у нас есть наша throbbing система который мы также можем использовать для оценки стабильности каких-то продуктовых изменений вас есть новая фича мы ее сначала тестируем на пробах не рискую продакшн и дальше мы делаем либо evitest либо canary с опять же нам на и большой части нашего трафика и потом постепенно ее тепло им немножко больше про canary гадов у нас есть тысячи различных мест где нужно есть нашей инфраструктуры конри могут выглядеть достаточно интересна каждая из этих сайтов он там есть разное количество серверов они в разных местах мы хотим получить достаточно обобщенную информацию и мы просто выбираем парой серверов в наших сайтах которые похожий трафик получает который у некоторых одинаковая конфигурация и запускаем нашу новую фичу как бы наш build который мы предлагаем предлагаем выпустить на наш канал дальше мы запускаем анализ у нас есть система которая собирает метрики сравнивает насколько лучше или хуже наша коннелли работает по сравнение с контролом у нас примерно 100 150 метрик которые собираются с разных частей нашей системы если что-то идет не так у нас есть дашборд на котором мы можем быстренько посмотреть что где не так при диагностировать и решить на самом ли деле нужно это чинить или это более-менее ожидаемая проблема даже если к на это у нас успешно мы не хотим менять все сразу это очень большая система мы можем что-то пропустить поэтому мы запускаем все это постепенно волнами на этом графике вы видите примерная картинку как у нас выглядит наш дипломант ведь а сверху тоненькая полосочка это наша canary и потом потихонечку мы увеличиваем то сколько серверов получает новый experience и опять же мы делаем так что на каждом из наших сайтов мы не обновляем все сервера одновременно потому что то как у нас сделал себе on мы можем потерять несколько серверов но мы не можем потерять несколько сайтов без негативных последствий на качество сервиса ну естественно все количество метрик которые мы должны собирать и мы должны по сути мы собираем все что у нас возможно мы собираем статистику с клиентов сколько они запросов сделали сколько сессий у них было сколько не раз нашими full баком пользовались то же самое собираемся эджа в claude смотрим как бы куда у нас все это пошло и естественно собираем метрики с нашего седан статистику ад-дина статистику от нашего прокси и другие метрики которые важны для того чтобы понять работает и все правильно все это собирается в единый pipeline и мы решаем какие метрики отправлять для рилтайм она она анализу нас есть системы телеметрии которые в реальном времени все нам показывают или о для более детальной диагностики засовывать это все волосы search или в объекты да для того чтобы у нас уже делать диагностику и чуть более детально про то как мы все это мониторе опять же это очень большая часть того что мы делаем потому что если мы меняем что-то что в критическом пути огромное количество команд которые что-то меняют даже если что-то сломалось связанная с ними это же мы что-то сделали абсолютно новое поэтому нам нужно будет объяснить что мы на самом деле не виноваты и для это значит что нам нужно очень серьезно задумываться о том как какие метрики мы собираем как мы их анализируем и как быстро мы можем критик результаты и здесь метрика очень много нам нужно делать рядов мы хотим все сейчас я сразу но это дорого стоит но мы можем выбрать более обобщающие метрика которые мы собираем и сохраняем и велел таксисты системах и как бы дальше если нам нужны более детальный метрики мы делаем thread'ов что доступ к этим данным чуть чуть более медленный будет в целом для обнаружения и триш мы используем и real-time система когда она называется атласа на open source она по сути хранить данные в памяти есть интеграции с allure ink системами и она очень быстро позволяет делать разные запросы по и строить различные графики для локализации к диагностике мы идем чуть чуть на более детальный уровень но мы и храним данные власти ксир чьи используем кибанова для того чтобы понять что происходит что не так и если что-то требует уже более серьезного более сложного анализа мы для этого используем big data и моделирование сбу с помощью более сложных подходов певцом статистику у нас к бы десятки и десятки есть даже гордов каждой каждой системе казалось бы очень много и очень сложно с такой системой работать но они организовано таким образом что мы начинаем с пункта 1 и потом знаем как в какую сторону идти чтобы все нам правильно идентифицировать что же такое это нас нам деле занимает достаточно мало времени для того чтобы понять что у нас системе не так делать изначальный trash у нас уходит 1 2 минуты что понять мы это или нет где сломалась что сломалось в коренной каком пути что-то не так на что нужно смотреть и потом от десятков минут до часов уходит на то что вы проблема диагностировать и уже начинать действовать но то в целом недостаточно даже если у нас все очень быстро мы не хотим чтобы это происходило часто особенно если это часто происходит когда тебе звонят и говорят что что-то сломано это популярные проблемы она называется are things фотик наверное можно это перевести как усталость от alert of классический пример эта книжка про мальчика который кричал пиво про волков мне здесь пришло более новая идея это замечательные рамки которые стоят метрополитене я думаю все мы видели стражи порядка насколько не бдительно слушают что там происходит мы не хотим чтобы у нас были такие системы поэтому мы минимизируем то что у нас будет беспокоить у нас есть всего на такую систему которой у нас есть у нас всего два over the две конфигурации которой она будут звонить но они смотрят на той работает ли у нас система для большинства пользователей мы смотрим на то сколько клиентов воспользуйся нашим форбак если они не смогли получить ускорение и у нас есть наши пробы который анализирует именно статистику по сети и смотрят что именно там сломалась у нас естественно гитик более обобщенные более детальные вещи но не на мне звонят нам приходит какой то имел в день мы можем нормальное время с этим разобраться почему нам не нужно спешить потому что нас есть fall back даже еще что-то случилось она не будет сильно кого-то беспокоить здесь вы видите что в среднем у нас меньше 1 over the в неделю хотя изменение в системе огромное количество но в целом ещё один важный момент что у нас есть наши probing система которая позволяет нам переделать а еще уровень позволяет нам автоматически спектрами проблемы справляться каким образом такой же ситуации когда это мы можем смотреть не только сколько времени у нас заняло что-то через разные пути скачать а получилось ли у нас это сделать и если не получилось мы можем знать где и что она сломалась и можем лишь мы ли мы что-то предпринять чтобы это починить например здесь на этом примере мы видим что если у нас клиент пойдет напрямую в облака или через прокси всп у него получить данные не получится есть он пойдет через наш backbone он получит информацию мы знаем что сломано и мы знаем как мы можем это починить нам нужно перенаправить трафик через наш интернет учинить похожим образом если у нас наоборот сломан путь через backbone мы можем это починить таким образом что перенаправить трафик напрямую другой стороны если у нас ломается все пути это скорее всего либо проблемы в интернет провайдере или в домашней сети мы не можем ничего сделать вас нету никаких вещей как мы уже просто ждать когда либо пользователи в интернет провайдер инфраструктуру починит и мы это можем автоматизировать можем это включить нашу систему стирлинга которое собирает данные и в процессе выбора быстрого пути она еще смотрит на то какая надежность есть и где-то что-то начинает ломаться достаточно небольшой задержкой вскоре наш промоушен трафик сам себя починит опять же у нас есть еще и fall back только что даже момент пока это все чиниться у нас клиенты не считаются с ломами это в целом основные принципы мы минимизируем масштаб поломок мы делаем так чтобы у нас все если ломалась то ломалась не полностью себя и много метрик работаем над нашими тузами чтобы быстро и эффективно встретился какими-то проблемами справляться даже с очень небольшой командой писал основные моменты как мы работаем но это на самом деле не совсем хорошая визуализация потому что она не показывает сколько времени мы на что-то тратим чтобы что-то померить или написать прототип это не так много времени нашем случае уже через несколько месяцев нас будет был прототип мы получали данные мы мерили их меньше чем через год у нас уже был первый пробег шин трафик после этого началась очень нудно и очень сложная работа потихоньку все продукте zero вать как бы передвигать остальной трафик учиться учиться и на самом деле это не waterfall это не то чтобы она мы полностью шли через эти стадия у нас было много небольших вич много небольших проектов проектов которые все проходили через эти циклы и в зависимости от того какую информацию получали мы на это реагировали соответственно на момент что для нас оказалось критичным это не верить тому что говорит нам опыт что нам говорит интуиция я не зря спрашивал такой вопрос о том будет ли это работать быстрее потому что наши же интуиции тоже сначала думал мы сначала думали что мат часть работает все будет быстрее на самом деле это не так и у нас есть много других примеров где в таким же образом у нас всего случилось более того очень важно как можно быстрее получить доступ к продаж in traffic упуская даже небольшому количеству потому что сколько бы мы не мерили сколько вы не делали синтетических тестов обязательно во что-то допустим потому что на самом деле весь трафик который у нас есть все случаи которые нас могут быть в продукте все что может применяться мы не моем мы не можем предсказать и не верь тому что говорят другие в том числе я пытайтесь следует следовать принципам не следуйте слепо советом потому что то что мы не ring the red те результаты которые мы получаем они достаточно сильно могут отличаться от того что говорит фейсбук что говорит о cамое столько uri клубе это не значит что мы не правы ли они не правы это значит что наши системы наши клиенты наши устройства отличаются от устройства других провайдеров точно таким же образом ваши сервисы ваши устройства будут отличаться от наших поэтому мерите сил станице сами все тузы которые для этого доступны они уже есть также пробег система если вы хотите использовать она на у концерт сайт у нас есть если огромное количество вендоров которую вам дадут уже полностью упакованную систему и со сбором и с анализом информации главное это делать и делать самим и делать решение на основе данных которые вы получаете и не стремитесь за самыми последними модными трендами намного лучше сделать система которая простая которое работает сделать это быстро и научиться с того как она работает чем потратить огромное количество времени на разработку каких-то компонентов которые на самом деле вам не нужны кому-то это мои возможно было нужно не написали блокпост но у них были причины почему они это сделали нужно сначала понять если у вас эти причины эти фичи делать и как вы учитесь как вы собираете данные со своей системы не забывайте следить за тем что ещё вы можете это сделать я привел бы аналогию со стартапами если вы подумаете как они работают они не пытаются предсказать что хотят пользователи они делают прототип начинают его тестировать начинают смотреть как реагируют пользователи и смотрят а той работает ли их начальное предположение есть ли какие-то другие предпосылки сделать что-то нужное что-то полезное в данном случае он мы начали с анализа ускорения задержки но мы также поняли что мы эти прокси можем использовать эффективно для балансировки запросов чтобы уменьшить наши расходы на amazon и инфраструктуры для того чтобы моделировать и строить модели стабильности нашей инфраструктуры чтобы настраивать конфигурации наших сетевых протоколов и не только для интернет запросов таким образом сегодня я показал вам как мы решаем проблему что ускорение запросов для нас это работает для нашей инфраструктуры мы собираем исторические данные у разных путях возможных способов нам направить запросы через интернет мы это все агрегирует использует используем где нас систему для интеграции с клиентами и собираем каковы наши данные застрял наш мониторинг потом чтобы это у нас не приводило к перегруженный у нашего на шаг operations но это то что я нас для над секса что важно из моего рассказа из моего доклада получить вам это принципы который может вы можете следовать возможно вам вы можете построить систему как у нас возможно у вас нет такой инфраструктуры или ваших инфраструктур отличается важно то что вам скорее всего это делать все равно нужно вот стрима есть выбор в даже если у вас нет усы восемь йен если у вас нет своего backbone у вас есть выбор между fallout 1 дермы между тем где поставить его же сервера между тем каким сидена пользоваться каким ген с этим пользоваться и все это может влиять на ваших клиентов и нужно это белья не понимаете нужно это мерить всеми доступными способами мерить на своих клиентах если вы делать какие-то изменения начинают с простых решений и заботиться о том что как вы делаете изменения привет предсказывать и ожидать что что-то будет работать не так думать о поломках думать о том что работать не будет идеально таким образом вы сможете избежать перегрузки с проблемами позе которые случились продакшен потому что вы уже получили начальные данные о том что можно сделать с помощью небольших шагов вперед и как вы работаете на своей системы не забывайте постоянно спрашивает себя зачем я это делаю приводит лет и к нужному результату нужно ли что-то поменять возможно моя интуиция меня подвела и с новыми данными нужно как бы принять какое-то другое решение сегодня я говорил о том что делаем в на forex мне интересно послушать о том что делаете вы ответить на ваши вопросы возможно услышать какой-то федак что-то интересное с вашей стороны спасибо большое это это было замечательно пасибо большое прям буду очень-очень круто наверное все таки масса вопросов там просто сейчас у тебя в дискуссионной зоне просто затопчут все таки наверное я должен спросить тот который задали все почему free psd как почему вот так вот все просто в шоке я думаю ответ будет стиле моего доклада что мы стороне мы верим и для наших проблем free везде работает лучше недавно был фондом конференция доклад а джонатан луни и который ведёт команду который занимает секунды разработки очень советую с ним познакомиться там на эти вопросы будет отвечать намного лучше чем я вот ответ есть да и 2 наверно интересный вопрос который может быть всем будет интересен это вот ты говорил что несмотря на все ваши прототипы измерение так далее иногда все-таки backbone работает медленнее как так может быть в мире лежим все же продумали что что случилось это не обязательно что бегу он работает медленнее но клиенту нужно добрых был на еще добраться нужно от клиента до нашей интернат инфраструктура еще как то дайте у нас есть достаточно определенная интернет инфраструктура как мы хотим делаем как - бежит и настраиваем как мы делаем пол полости наши для трафик менеджмента они ввиду того как мы сделали седан настроена на доставку видео они могут быть не оптимизированы для задержки ну а даже одна из наших задач и тоже не пытаться менять полностью всю инфраструктуру нашим продукт нашего продукта нашей большой системы а попытаться найти прагматичный путь получить как бы лучше признательность с тем достаточно небольшими изменениями без вовлечения большого количества усилий инженеров ну вот вам ответ на нам два самых таких популярных вопрос а все остальное действительно в дискуссионной зоне еще одно объявление после того как вы получите все ответы дискуссионными зоне от сергея приходите на зону кофе-брейка тони получил вчера книжки и сегодня я там еще 100 штук подпишу да спасибо большое дискуссионную зону сейчас вас всех проводят это было прекрасно"
}