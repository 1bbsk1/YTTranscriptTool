{
  "video_id": "oVNpbJximrI",
  "channel": "DevOops_conf",
  "title": "Николай Ихалайнен — Как рулить пароходом MySQL",
  "views": 632,
  "duration": 3590,
  "published": "2020-02-07T07:49:38-08:00",
  "text": "у доброе утро очень приятно видеть столько дисциплинированных людей которые способны встать рано и пойти на первый доклад 2 дня я хочу рассказать о том существует майской в среде обычный open source побочный в конкретно кибернетика и какие простенькие проблемы у вас возникают когда вы пытаетесь его развернуть или даже использовать в продакшен в стародавние времена для того чтобы начать работать с базой данных например смазку или вы получали логин пароль какой-то пи адрес и вот уже этого достаточно чтобы приложение могло подключиться и начать работать даже если сейчас вы находитесь в облаке вы можете работать по тому же самому принципу вы можете заказать большой installs за много денег настроить там вашу базу данных и сказать вашим приложением которое постоянно возникает и умирают в облаке сказать где будет база и после этого замечательно работать у этого подхода есть огромный минус что мы за hard ходили свою инфраструктуру в современном мире где инфраструктуру хотелось бы программировать хотелось бы иметь такой программный код или хотя бы а конфигурационные файлы которые будут нам позволять иметь 100 процентно воспроизводимую инфраструктуру со всеми составными частями в том числе с базы данных для того чтобы бороться с вот этим симптомам одного большого иконостаса в моей сколь существует уже десятки лет репликация казалось бы кроме баз данных есть и другие приложение состоит там почему бы нам не воспользоваться тем что в базы данных сохраняет все на диске клонируем все изменение с диска получаем копию на другом узле к сожалению так не работает если мы попытаемся поменять что-то в базе данных эти изменения они обычно небольшие есть такое понятие как райта амплификация усиление записи когда мы пишем одну строчку сколько в строчке может быть изменений может быть 100 байт может быть даже меньше такой вот когда мы пишем 100 байт в реальности на диск от базы данных может приходить несколько сотен байт или даже килобайта то есть усиление записи в сто раз вполне нормальная вещь в базах данных если мы в тупую будем пытаться копировать все изменения которые выходят на старой у нас получается очень большие потери производительности и потери и эффективность репликация в моей школе работает по другому принципу все изменения которые мы делаем в виде запросов или в виде отдельных строчек который поменял транзакция мы помещаем в специальный лук виде события репликации эти события репликации когда они уходят на другой сервер они могут там исполнится и мы получим в каждый момент времени точный снимок нашей база данных таким образом сад арам у нас есть так но у нас нет надежности и у нас нет адекватной производительности в облаках мы можем стартануть практически любое количество ресурсов на которые у нас хватит денег для приложения которое умеет работать с большим количеством не очень мощных instead of это очень большой плюс особенно не очень мощные иисус и хороши тем что они обычно стоит дешевле дополнительные возможности если мы уже поделили наше приложение на такие кусочки которые относительно независимы друг от друга работают мы можем производить плавно и обновление нашего приложения когда вы последний раз видели базу данных с аптайма 1 год вот за весь этот год не было применено во-первых ни одного обновления безопасности и первое же залетный дятел которое оставило уязвимость приложение он может дать доступ к огромной базе данных в которой данные сохраняют большое количество приложений если у нас огромный incense а вторая проблема которая автоматически возникает что очень страшно людям применять исправления ошибок проблема беспокоит уже есть исправление несколько лет для нее а все равно и администраторы баз данных и devops и и собственники приложений они боятся обновить эту базу если у нас есть много реплик мы можем на одной реплики сделать минорную версию повыше и при этом у нас продолжит работать и все запросы когда реплики и сама реплика продолжить работать если мы заметим что реплика стала в 10 раз медленнее чем до этого были другие реплики значит мы временно на этот апдейт пока под забьем но через какое то время когда мы выясним почему у нас скорость пропала мы можем вернуться и произвести правильное обновление такие решения они уже давно существуют в облачной среде если взять amazon google microsoft они все предоставляют возможность запускать mais quel просто по щелчку мышки в интерфейсе или по запуску команды и они предоставляют доступ к к своей модификации майской несмотря на то что майское лет теперь для облачных провайдеров так как они не продают исходный код они не обязаны предоставлять улучшение поэтому у всех облачных провайдеров есть свои улучшение майской или которую меня они не делятся и проблема даже не в том что они такие жадные можно было бы обойтись без и худший проблема в том что разобраться что именно их улучшение не привело к тому что сервис не работает и техподдержка отказывается вам помогать адекватно помогать не дает естественно доступ к самим вот этим сервером не дает исходных текстов чтобы вы сделали review что происходит это сложно вернитесь в связке с моей скулы сейчас уже позволяет иметь те же самые возможности но полностью концу рс возвращайся к репликации the replication которую мы все хорошо знаем для mais quel а она асинхронная когда мы хотим сделать изменения мы изменения направляем все на один и тот же сервер называемый мастер все эти изменения пакуются в событии репликации и через какое-то время они будут применены на слове в чем проблема во первых когда у нас какие-то длинные большие запросы которые работают например 2 часа мы эти изменения увидим совершенно точно через 4 часа на свой век потому что во первых сначала мастер будет два часа работать потом своих будет два часа работать это первая проблема а во-вторых может случиться что слоек еще медленнее и тогда мы увидим изменения не через два часа а через сутки и все это время те запросы которые будут после вот этого двухчасового запроса или во время этого двухчасового запроса они не будут видны однократно мы затормозили slave и мы не видим всех новых изменений это очень напрягает приложение которое хотели бы видеть свои изменение базис желательно в секунду или миллисекунды синхронная репликация решает эту проблему следующим образом мы говорим хорошо нам очень важно знать что все изменения которые у нас были на наши запросы вот давай-ка база подождёт нашего запроса что мы не увидим результаты запроса до тех пор пока все изменение будут на всех узлах побочный эффект от этой интересной ситуации в том что мы уже не ограничены одним сервером и мы сможем в этом случае делать изменение сразу на большом количестве серверов для того чтобы войти в нужную скорость чтобы мы действительно могли быстро и запросы исполнять хотя бы за миллисекунды было придумано решение что все изменения которые делают запрос мы не виде запросы отправляем на slave а в виде строчек если запрос искал нужную строчку среди миллионов строчек в результате поменялось только три строчки то значит на три строчки и будем менять на свой это будет гораздо быстрее еще побочный эффект синхронной репликации то что системные администраторы baby ей и разработчики приложений вынуждены иметь одинаковые данные на всех серверах ведь если у нас разойдется хотя бы одна строчка между серверами это будет значит что у нас что то не так что мы не сможем применить все изменения одновременно на всех серверах и не понятно что с этим делать особенно если у нас нет никакого дополнительного версия нирования естественная плата с хром ность так как у нас все узлы будут находиться в разных местах сети мы вынуждены платить сетевыми задержками на каком-то этапе на каком этапе например у меня в транзакции есть 10000 insert of и это большая акция ну хорошо мне не обязательно ждать после каждого inserto это сетевой задержки когда транзакция точно отработает когда мы точно знаем что она не вызывает ошибок уже прямо сейчас и пользователи или приложение сделали commit значит на этом комете мы будем ждать сетевую задержках пока остальные узлы сертифицируют то что они тоже применили эту транзакцию меня часто с вы хорошо вот асинхронной репликации у меня было два узла и у меня замечательно работало репликация почему когда каждый раз предлагаются синхронные варианты реплика самой скорик почему требуют обязательно 3у звук решение очень простое если у нас делится сетевой проблемой кластер пополам у нас должна остаться такая часть костра которая покажет что у нас есть точно даны и у нас есть точно знание что мы самые последние кто получали данные потому что нас больше те узлы которые осознали что они в одиночестве что их стало совсем мало эти узлы понимают что они не могут ответь потому что если они вернут данные это могут быть устаревшие данные если они позволяют поменять строчки то возможно где то в другом месте тоже будут позволять поменять и у нас не будет единого такого центра который будет позволять знать что здесь у нас правильная база doc что же такое мой скорику вернитесь чаще всего люди представляют себе одиночный instance такой же как instance в нашей большой базе и таким образом мы лишаемся основных преимуществ базы данных в среде кации что мы можем добавлять количество узлов и ускорять хотя бы чтение приложение которое очень много читают их очень много тех которые читают больше чем пишут и такие приложения когда на них возрастает нагрузка мы можем добавлять такие же сервера до тех пор пока мы не упремся в скорой записи а когда мы упремся скорость записи значит нам просто надо поделить наш кластеры аппликация на несколько использовать шарди рования для того чтобы у про репликация ведь раньше для того чтобы сделать хотя бы репликацию нам нужен был специально обученный человек который знает как она устроена как сделать совершенно одинаковое состояние на разных серверах и после этого сказать что вот с этого момента мы начинаем передавать сообщение репликации для того чтобы это сделать в кубер найтись и мы должны запрограммировать как-то этот процесс чтобы этот процесс был автоматически чтобы мы также как в облаках могли нажать на какую-то кнопочку или выполнить какой то скрипт и поучить то что у нас в реплике сталлоне два узла три эти рутинные задачи должен оптимизировать какой-то код этот код оптимизируется специальными дополнениями кубер не tissue который называется оператор операторы работают с объектами внутрь кибернетика я не знаю в курсе вы или нет но если посмотреть на кубер нити с точки зрения программиста которая именно делает сам кубер найти обернитесь это просто аресты 5 к базе данных это базы данных она объектная и есть объекты разных типов и когда происходит изменение добавления удаления объектов выполняется действие действие они могут быть как от облачных провайдеров так от операторов про который я говорю так вот как же нам расширить какую-то функциональность нам нужен какой-то кастомный объект этот кастомный объект называется кастом ресурс для обернитесь перед установка оператора мы определяем как нас выглядит кастомный объект и после этого мы можем уже делать реакцию оператора на изменение свойств этого объекта запуск узла моей сколь и начинается с того что пользователи надо понять какие диски использовать сейчас же есть ssd hdd есть n ворам есть какие-то магнитные диски и для каждого этого вида стороны должна быть различия поэтому в кубер найтись есть встроенный стройный объекты типа сторож класс который мы должны определить прежде чем мы поймём как нам стартануть нашу базу этот 100 раз класс который мы сделали мы прописываем вместе с тем какая версия майской ли нам нужно какие нам нужны бэкапы какого размера у нас данные какого размера нам нужно оперативной памяти так далее все эти фишечки прописываются в кастом ресурсе кластера и и пайку верните со дернет оператор и оператор начнет действовать каким образом оператор может начать действовать он может точно также модифицировать уже стандартные объект который находится в убирайтесь если мы хотим запустить что-то в кубер найтись и мы либо делаем depo мент или если идет речь о перси снят на каких-то данных мы делаем стоит full set оператор точно также делают стоит full set стив фоссет это такой объект который хранит сколько нам нужно узлов и где они должны находиться если у нас добавился узел а всего у нас было два узла то мы делаем но 3 если у нас умер на 2 to stay focused создаст но 2 на том же физическом сервер каберне tissot для того чтобы он имел доступ к тем же самым диском и локальным ресурсам таким образом стоит full set управляет количеством тех контейнеров которые мы будем запускать что же мы должны хоть минимальной единицы исполнения в кубер найтись и это под вот это набор контейнеров плюс еще какие то дополнительные свойства чтобы контейнер стартануть базу данных нам нужны какие-то диски которые будут сохранять данные для того чтобы диски появились мы должны сделать заявку ассистент фолен клей в этой заявке прописывается как раз тот сторож класс которую мы определяли недавно и прописывается размер какого объема нам нужны эти самые диски после того как заявка создана у нас есть возможность под эту заявку сделать уже настоящий том это делается с помощью провайдеров которые выдаются самим кострам например облачными провайдерами выдается том в амазонии бы с томах на момент готово к запуску кастера мы готовы создавать контейнера под может иметь несколько контейнеров это нужно для того чтобы был доступ из других контейнеров к файловой системе основного контейнера есть такая штука как сайт car я не знаю помните ли вы еще нет мотоцикла с коляской ну вот представьте себе урал где двигатель находится руль и прочие тормоза азбуку еще находится коляска в которую можно что-то положить кто-то это может сидеть вот как раз сайт корыто название коляски мотоциклетный так вот что может быть в этих дополнительных контейнерах во-первых физические бэкап и физические бэкапы получает доступ к файловой системе и с помощью этого доступа они имеют возможность скопировать все данные во вторых мониторингу очень важно быть как можно ближе к тому что мониторить берем mais quel экспортер для про метился это же помещаем его в контейнер так же бывает еще парочку контейнеров связанных с тем что контейнером мой сколь и они достаточно глупы и они умеют только запустить мой сколь с чистыми дисками для репликации нам надо начать с определенного стоит мы должны начать с быка по уже существующего узла как раз тем чтобы развернуть такой бэкапы и сделать его на другом узле тоже могут быть отдельные сайт car облаках очень важно не потерять данные когда мы запускаем удаляем инстанции добавляем добавляем диски очень неприятно было бы оказаться без данных которые собирались до этого иногда даже десятки лет бэкапы нужные бэкапы это очень сложные процессы для того чтобы управлять bacopa my в контейнерах в операторах мой скулит есть специальные тоже ресурсы на которые они будут реагировать для того чтобы с можно исполнить код чтобы исполнить код нам нужен скрипт скрипт должен запуститься в каком-то контейнере где этот контейнер будет жить в каком-то поле хорошо почему оператор не запускает под потому что для того чтобы делать однократные задачи какие то есть уже вещь называемый a job в обернитесь и которая позволяет не только запускать нужно отслеживать статус уже завершившихся работу с как раз же реальные bacopa да и происходит копирование данных в какой-то удаленные у лука где лучше всего хранить большие файлы относительно большие для этого есть хранилища с форматом из 3 из 3 это и 5 реализация ис-3 есть у всех облачных провайдеров кроме того есть собственный проект меняю которое позволяет иметь с 3 в вашем кластере кубер найтись без использования пропили тарных каких-то программ тут есть сложность база большая когда мы делаем backup мы еще возможно над бэкапом какие-то преобразования делаю где хранить копию базы до того как мы ее от прогулок купер найти тоже все хранится что связано с большим количеством файлов в persistent волю мах поэтому бы копыта же делают пирсе стив у него для того чтобы делать бэкапы периодично потому что не будем же мы заставлять какого-то специально выделенного человека нажимать на кнопку каждые сутки или даже каждые два часа для того чтобы у нас был book есть расписание где в формате крон жаба обычного крон таба мы можем задать когда будет backup если мы задаем когда будет backup в формате сделай бэкап в три часа дня каждый понедельник то где произойдут эти три часа дня три часа дня произойдут по той зоне мастера кубер не this когда м из каких важных объектов состоит работа с нашим москвой в классе рк обернитесь мы можем использовать несколько команд для того чтобы решать все проблемы которые у нас возникают самая простая команда это команды git и название какого-то объекта например поводы например ресурсы с названием миску или или майской бэкап и мы будем видеть список и какой-то статус эта же команда может преобразовать из просто списка можно получить в формате yaml то что нам нужно мы получим как конфигурацию так и текущий статус и более такая простая команда dice краб она позволит нам получить не только конфигурацию какую-то уже словами они виде яму но и логику bernie this если у него что то не получилось и наконец команда которая выделяется которая не имеет как видите название ресурса да если у нас было раньше как сетей get под вот это название объекта а тут куб citilux эта команда позволяет получить доступ к stydio у ты и есть идеи от каждого контейнера мы можем увидеть что нам скрипт бекапа написал какая ошибка или например май сколь и не стартует операторы для mais quel есть open source есть три основных из них есть один оператор для асинхронной репликации и 2 оператора для асинхронной репликации есть еще оператор для марии д.п. я его не включаю потому что он не уксус как только вы уже отвод на source тогда будет интересно и на них посмотреть оператор приз labs можно и легко инсталлировать буквально в две строчки если уже у вас настроен хиль если вы не знаете что такое фильм это как опыт и или юн для linux мы можем какие-то пакеты устанавливать буквально в пару команд сначала добавляем репозитории а потом исполняем запускаем пакет на репликация мастер slave которая управляется гид хоп оркестра таро оркестра tor это такой замечательный проект который позволяет нам управлять кто сейчас мастер переносить роли мастера слоев между серверами видеть текущую топологию и делать переключение аварийной если мастер умер он работает и без кубер найтись поэтому если вы используете репликацию mais quel и до сих пор не используете гидропарке стратах обращайте на него внимание скорее всего вам на 99 процентов он нужен после того как средняя строчка хиллман стала появляется много много сообщений в консоли в том числе предложение запустить мастер sleeve костер с количеством реплик 1 и задав пароль для root а для маску или это не рутовый пароль для linux это пароль именно для управления объектами базы для того чтобы создавать удалять дропать база данных таблички и создавать других пользователь майской запустили давайте есть морской может исполняться только в виде какого-то пода прошло уже много времени у нас до сих пор только один сервер mais quel а мы сказали реплика один где же наша реплика mais quel это просто одиночной сервер майскую это неинтересно посмотрим как раз список ресурсов которые нас связано с и москаль кластером тоже видим что количество реплик 1 и больше не собирается ничего делать с помощью команды get с форматом я мол получаем весь конфиг этого кластера и меняем конфиг костры меняем там количество реплика 1 до 3 теперь у нас уже есть настоящий мастер slave кластер но совершенно непонятно а кто из них мастер кто славы как с ними жить и работать мы можем обратиться к помощью оркестра таро берем пробрасываем пор таки стр отара к себе на локальную машину на ваш ноутбук и скачиваем простой относительно простой ваш скрипт который работает с опять же ареста и имя этого оркестра таро этот скрипт регистратор клиент позволяет увидеть список костров так как регистратор это часть оператора он исполняется как как раз сайт корр контейнер внутри кода оператора то сколько у нас может быть оркестр отаров на нашем кластере мой сквозь один только может быть и поэтому команда минусы casters выдаст нас все костра mais quel которая сделана на пресс вообще в этому к старику вернитесь с помощью команды to pull a g предоставив и dns имя мастера на самом деле там не только мастер там любой может быть и порт 3306 это обычный порт морской мы получаем список из всех серверов где у нас вначале будет идти мастера а потом через плюсики будут идти с вы вы оркестра tor позволяет иметь не только вот эту плоскую структуру когда у нас есть мастер и только свой вы а также легко позволяет работать с деревьями майской верификации когда у нас есть один мастер и у него свои вы и у какого-то слова есть еще какие то свои слова тут хорошо видно что мы используем перка на сервер потому что версии у орков mais quel а они идут как 57 26 у перка на сервера так как только на сервер это тот же самый майской плюс некоторые патчи для производительности для того чтобы instrumentation было лучше то вот это циферка после эта версия самого уже дракона сервера относительно базовой версии майскую также у нас первый сервер он первые три драйт значит он как мастер может исполниться использоваться не только для select of но и для inserto вылетов формат репликации используется робость значит что мы не генерируем вместо inserto строчку insert into в бинарный лог мы передаем между мастером слой вами только изменившейся строчки и используется глобальный идентификатор транзакции хорошо мы выяснили кто находится где я давайте возьмем одноразовый код и там будет один контейнер в котором мы запустим ваш и запустим уже майской команду мы вместо майскую команды можем использовать программу на каком-то языке программирования это сейчас просто пример приложения подключились к плыву получили то что слив и имеет статус переменную readonly что это значит это значит что слои вы не будут исполнять запросами модификацию база если у пользователя базы данных не будет прав root москаль а то есть получается что вот в этой репликации асинхронный все равно может найтись такой человек который будет иметь право сломать репликацию удалить строчку и когда на эту строчку придет изменение the replication не будет понимать что делать потому что если мы не можем изменить строчку и и нет она мастер она была что мы можем в этом случае сделать мы можем либо синхронизировать эти с изменение строчек либо мы можем удалить этот узел и создать его заново из копий на самом деле вот эти имена имя кластера mais quel номер узла . моя сколь . namespace обернитесь они не должны использоваться приложениями мы получаем более адекватно именам например мы имя кластера майской или тире мастер и когда приложение надо поменять что-то в базе данных она использует connect вот этой штуки если мы будем использовать просто имя кластера терема и сколько мы получим случайный sleeve переключение на этот случай нас life она получается за счет того что кубер нити сумеет раскидывать и connect и новые между несколькими узлами 1 сервис хорошо настал такой момент когда мы по каким-то причинам хотим поменять мастеру например текущей мастер слишком медленно работает мы хотим временно увести роль мастера на другой сервер мы можем это сделать с помощью команды регистратор клиент грейс пул мастер they cover сделать так что все текущие запросы на модификацию базы завершатся на текущий мастер и и потом произойдет переключение на другие на другой сервер если мы опять посмотрим то получше видно что до переключения произошло все слои вы которые были у этого старого мастера не перед подключились ко лбу мастера и при этом старый мастер почему-то квалификацию не подключился это сделано специально если вы хотите сразу после подключения провести какие-то манипуляции после того как вы этим популяции завершили вы можете совершенно спокойно стартануть и репликации оно продолжится с того что в момент проблема заключается в том что оператор он тоже отслеживает где у нас находится масть у него в свойствах ресурса который можно увидеть через диск райп ресурса написано что мастер у нас узел 0 основы узел 1 и 2 вот после такого ручного вмешательства в топологии репликации у нас оператор он все равно будет думать что у нас мастер остался на узле на ну так а вот мы запускали простенький сервер который совершенно непригоден для нормального использования в продакшен потому что у него какие-то настройки по умолчанию какие-то диски какие мы не знаем как нам все это задать мы можем воспользоваться примерами которые выдают на гитхабе создатели оператора при словца это же их запустить к сожалению и даже эти примеры они не позволяют работать почему не позволяют там есть нужные опции но эти опции закомментированы и над каждой этой опции еще есть подробное описание что это опция делает немножко такой не самый удачный подхода к документации с другой стороны очень удобно когда вы только начинаете так вот сколько процессора и памяти потребует каждый узел мои сколько обычно мы что хотим мы хотим знать что мы имеем возможность потребовать вообще все ресурсы которые есть на сервер и физическом и чтобы эти ресурсы увеличить мы должны где-то написать 100 процентов процессора сто процентов память вот в спецификации кластера мы можем написать конспект и даже ограничения какие ограничения как понять и вообще должны быть ограничения каждый узел у нашего костра каберне this и мы можем посмотреть что у него есть например у него может быть 4 processor на хидра это по-моему стандартные instance в голову и 12 гигабайт оперативной памяти хорошо кубер не тестом показывает что нам доступно три 1920-м где м это мили также как у нас есть миллиметры тысячные доли метра вот так же у нас есть тысячные доли одного гидро и у нас если мы зададим 4000 м или четыре в качестве требования нашего процессора у нас наш узел моей сколь никогда не стартанет потому что нет ни одного узла в кластере у которого было бы достаточное количество ресурсов поэтому надо задавать меньшее количество учитывать то что кроме самого контейнера майское ли у нас находится еще сайт кара и какое-то место оставлять в процессор ных ресурсах и в памяти для всех остальных как понять а сколько нам вообще нужно ресурсов ведь может быть так что база то и не требует таких больших физических ресурсов простой момент если у нас приложение посылает запросы запроса отрабатывать за одну миллисекунду и у нас нет никакой дисковой активности сетевых ожиданий то у нас этого одну миллисекунду практически полностью используют мой сколь процессор одно процессор на еду значит 1000 запросов в секунду съедят тысячи для памяти все немножко сложнее для того чтобы база работала быстро она должна использовать память чтобы она использовал память нам нужно чтобы вся база помещалась или хотя бы самые важные данные из базы помещались в оперативную память и для миллионов строк так как обычно в таблицы содержит сотни байт в строках это будут сотни мегабайт или чаще всего гигабайт не знает о том что мы ему предоставили конкретное железо нет самонастройки майской или под нашей железные изменения значит мы должны прописать конфигурацию mais quel в каком-то месте вот в частности в морской конт секс и я мог кластера мы можем прописать очень важно что в случае репликации нам не обязательно гарантировать то что данные записались на конкретном узле на диск нам не обязательно сбрасывать все буфера на диск после каждой транзакции можем делать это каждую секунду или каждое какое-то определенное количество транзакций как для логов восстановления с самого движка база данных и на тебе и так и для слова free приказ современные базы их они получают а большое количество запросов на диск причем эти запросы чаще всего имеет случайный порядок ssd с этим справляется лучше но и создай дороже и до сих пор можно встретить дефолт в облаках то что будет в качестве запроса на пирсе сент волю вдаваться и видеть с помощью описания сторож классов мы можем сказать что вот гугловый провайдер передай нам конечно сделай нам сторож класс с именем ssd и в кассе параметров будет переезд и этот 100 раз класс потом можно использовать в volume speck и также мы задаем количество места на диске пожалуйста не пытайтесь сделать экономию если у вас есть сорокапроцентный запас по месту скорее всего в ближайшие несколько месяцев вас будет все хорошо с базы если у вас заканчивается место на диске то мой skype просто повесит все текущие запросы и чаще всего это приводит к полной остановке всего приложения несмотря на то что у вас есть сотни узлов которые работают с вашим приложение хорошо мы можем легко создать удалить узел базы данных а что если произойдет так что будет удалено всю эфемерность облаков и инсталляции мошка или в обернитесь и она заставляет нас не только думайте об этапах но и делать их каждый раз нам надо сразу эти ресурс москаль бэкап и сначала я рекомендую вам сделать пробный запуск если вы сделали неправильно и что-то в конфигурации the backup он будет быстро создавать много много провальных кодов поэтому сначала проверьте если все хорошо работает то запускайте по расписанию преслав сделал то что не было до этого в мире майской он позволяет запустить кластера репликации работать с ними даже если у нас нет какого-то специально обычного человека которая очень хорошо понимает мой склеили а если этот человек есть то этому чеку не надо тратить кучу времени на ерунду и может он может заняться теми вещами которые действительно требуется от настоящего тебе вспоминаем что синхрон то есть сильные проблемы с разницей данных на разных узлах проблемы с лагом и поэтому хотелось бы решение синхронно репликации команда варкал для того чтобы сделать синхронную репликацию взяла все подсистемы которые существовали в обычной репликации и добавила протокол конца собак счас и сетевую часть все это называл протоколом extabit.com никич xq самая часть которая отвечает за синхронную репликацию называется майской группы критичная это поговаривают для того чтобы легко можно было найти по ключевым словам эту вещь в документации москаль в чем особенность в том что мы теперь выделяем изменение которое сделало каждая транзакция это raised у нас для каждого изменения которое сделало транзакция есть имя база имя таблички и какое-то изменение когда у нас есть прайма реки первичный ключ там и для этого первичного ключа и вот этих вот предыдущих данных считаем хэш и с помощью этих хешей мы можем легко понять пересекается ли у нас транзакции когда транзакции меняет одну и ту же строчку они не могут быть исполнено и параллельно если транзакция а поменяла строчек и и которых нет в транзакции бы а значит эти транзакции могут сработать совершенно параллельно и таким образом если у нас было три потока которые исполняли запросы когда приложение работало есть 8 потоков которые могут на свою ваха исполнять запросы у нас исполнение запросов на слове может быть даже быстрее чем она на мастера также ракеты позволили найти конфликтующие транзакции не только из тех которые у нас мы уже записали в бинарный лог но и те которые мы исполняем прямо сейчас когда мы знаем что транзакция которая сейчас исполняется на сервере но еще не закончил commit изменяя те же самые строчки что транзакция которая исполняется на сервере б мы можем в любой момент времени сказать что хорошо на сервер и бета как она чуть чуть позже старта новом и транзакцию исполнить не будем ее просто откате мы вернем ошибку клиент у нас не будет конфликта изменение репликации когда на одном сервере поменяли на втором на одном одни данные на другом другие мы не знаем какие данные выбрать для того чтобы работать с большим количеством узлов которые одновременно могут менять значение и для синхронной репликации у нас есть выбор мы можем сделать двухфазный коммент это технология которая давно известна в мире базу данных пример это свадьба согласна ли вы одним соглашается сначала вопрос одному он соглашается вопрос второму если он бы согласились значит хорошо вот эти вот согласны ли вы которые бегающие по сети они будут добавлять накладные расходы когда у нас количество транзакций одновременных очень велико которое исполняет сразу на всех узлов это не работает значит нам нужен специальный протокол который будет позволять серверу которая отправила изменения носить другой сервер знать что это транзакцию же исполнил это называется сертификацией транзакций когда мы не подтверждение на каждую транзакцию просим а когда мы по каким-то числом которые передаются в нашем протоколе сертификации понимаем что да эта транзакция было исполнено значит мы у себя транзакцию завершаем commit возвращает окей клиенту еще работать съесть у нас есть очень хорошая отказоустойчивых сизо этого потому что каждый узел он идеально одинаковой и у нас получается архитектурного сколько доступным база данных как запустить опять же если опять же простенький самом в результате три узла пароль даже не надо сдавать секрете и он автоматически добавится оператором случайных секрет на какой сервер мы попадем мы можем использовать балансировку типы соединений буксировка и соединения это не очень удачная вещь приложение любит открыть connect и долго с этим коллег там жить если мы попали в такую ситуацию значит у нас на одном сервере может быть много запросов на другому если у нас есть какой-то прокси который знает протокол mais quel узнаешь туда вот это соединение она сейчас idol значит вот это соединение которое мы есть мы можем отправить на любой другой сервер о протокол маской вот как раз по протоколу майская работает мой сколь и роутер это такой fox настройки также как из пресс labs он практически задаются единственное что здесь оператора от oracle требует конфиг мы конфликт map это такой способ в вернитесь и сохранить файлик конфигурации на который потом будет использоваться в контейнере к сожалению оператора только он еще достаточно сыроват несмотря на то что у них есть фиксы проблем например сейчас нельзя задать что мы не 10 процентов от одного процессор на в ядрах хотим а больше и там больше оперативной памяти бэкапы также можно сохранить в истре единственная проблема что если при slabs использовал для бэкапов экстра backup это физические бэкапы которые по сути копируют файлы и копируют лог транзакции для того чтобы эти файлы были правильными то тут используется при образовании с помощью селектор базы данных вот текст и это не параллельно и это очень медленно хотим bacopa каждый час делаем строчку следу получаем быка если мы хотим использовать маску и роутер вместо и балансировки то мы добавляем сайт корр контейнер к нашему приложению приложение запущено на том же самом физической музыка обернитесь и что и войска в роутер сетевая задержка 0 идеальная вещь для того чтобы балансировать потому что мы за эту балансировка не будем платить время рекомендую вам попробовать этот оператор сейчас он к сожалению не на продакшен уровне но это все равно достаточно интересный проект а что если синхронная репликация не появилась пару лет назад как случилось с оракулом и до сих пор у выпадает в ошибке а что если она уже была на протяжении многих-многих лет до этого реализаций асинхронной репликации галера с уже существует много лет она используется в дракона x-tribe кластеры в марии деби и галера кластере и у перк он и есть для своего перк он x-tribe хостер open source оператор которая позволяет делать все то же самое что делает in baby кластер только это еще и работает стабильно у нас точно так же происходит легкое клонирование между узлами когда мы добавляем новый узел он автоматически скачается существующего узла все изменения которые были если у нас узел пропадает на какое-то короткое время то когда он появляется он на котят только те изменения которые произошли за это время и полностью весь там-то работин backup не будет тащить я наверное же пропущу каленые момент и потому что не хватает времени чтобы рассказать это очень интересные темы достаточно долго можно говорить есть важная вещь в синхронных аппликациях что у нас есть очередь на исполнение транзакции что произойдет если это очередь переполнится краны репликации ничего страшного но отстал sleeve на несколько часов не проблемы в синхронная репликация мы платим оперативной памяти взять все транзакции которые ожидают сертификации поэтому тот узел который тормозит тот узел который не способен исполнится достаточно быстро он будет отправлять сообщение flow control эти сообщения flow control который получают другие узлы заставляют другие узлы замедлять исполнение запросов если у нас какой-то узел тормозить на запись будут тормозить на запись все остальные узлы значит на скорость работы в при синхронной репликации зависит от скорости записи на самом медленном узле и поэтому нет смысла делать один мощный сервер а для реплики делать менее мощные запуск даже без фильма просто несколько яму файлов помещаем выбирайтесь и после этого у нас появляется кластер состоящий из трех узлов также как с кучей с пресс лапса мы можем залезть в каждой из ямов и подтюнить параметры пользуюсь комментариями что здесь интересно что у нас кроме самого кластера есть еще прокси это прокси иску и похожая вещь на майскую или роутера только с большим набором возможностей и тоже достаточно стабильно работающий подключаемся мы к прокси из приложения и после этого у нас автоматически происходит распределение нагрузки между всеми серверы общие не смог там было несколько кодов все эти поду будут содержать одни и те же настройки настройки между всеми прокси иску или одного кластера синхронизируется автоматически настройки происходят по протоколу мой сколь мы меняем опаленные таблички в чем проблема с пользователями чтобы пользователь работали они должны быть как на стране прокси сколь так и на стороне майской есть специальная команда проксей сколь админ которым можем запустить и оно синхронизирует всех пользователей из майской в боксе sky мы можем с помощью текстов задавать какие запросы пойдут на какие то группы серверов или на какой-то конкретный сервер для того чтобы показать вот это в чем вообще смысл обернитесь и облаков что у нас надежность станет выше мы можем взять и полностью удалить все данные с одного из узлов отели что было до этого в proxy сколь работает севы узел один после того как мы удалили то мы сразу же увидим ошибку и следующий же запрос уже будет подключен к другому серверу другому узлу мой сколь который все еще жив через какое-то время у нас каберне this закончит все свои дела по восстановлению закончится копирование резервные с другого узла и у нас полностью будет снова живой costa в отличие от up обернитесь и мы не можем балансировать увеличивать расширять кластер на основе загрузки процессоров в базе данных всего нескольких запросов которые получают много строчек достаточно чтобы загрузить все процессорные ядра но при этом быстрые запросы которые нам нужны они будут продолжать быстро работать это не проблема для базы когда все процессоры загружены поэтому масштабирование вверх и вниз надо делать руками с пониманием что вы делаете и операторы они позволяют просто менять says вот как раз учи пикси мы меняем с 3 на 5 опять же почему проще менять нечетное количество 3 5 и так далее просто потому что мы этот кластер можем поделить пополам и получить неравные части не потерять в процессе изменения мы можем это не работает мы захотели застрелится до 5 узлов но не сработало смотрим диск райп на не за пустившийся под и видим что он не нашел такого физической ноды кобрин эти сена которой все будем хорошо просто мы добавляем количество зубов то же самое вниз меняем тот фиг у нас остаются артефакта в виде дисков и в виде дисков at proxy sky автор сам не удаляет потому что удалять данные без ведома пользователя это плохо поэтому когда вы точно знаете удаляйте персис титул miss a pixie опера он уже работает сам пикси тоже уже работают много лет пользуйтесь если у вас нет такого что много много записей который может быть заторможено каким-то одним из узлов то это самый идеальный способ запускать мои сколь в каких-то кластерах я может быть не самых больших нагрузках но это очень легкий способ потом работать если есть время на вопрос если нет то извините я на них отвечаю то у нас наверно время буквально 12 вопросов все остальное можно будет в дискуссионная зоне обсудить тут в течение доклада писали некоторые вопросы и так как времени много я выберу те которые мне ближе поэтому извините все те кто своего просто не услышит вот такой вопрос если примеры работы подобных схем синхронной синхронный в продакшен какие нагрузки и размеры баз данных если размер баз данных около одного терабайта какова скорость поднятия одного слова размеры для асинхронной репликации обычно не больше чем 3 4 терабайта это самой большой инсталляции который я видел для больших инсталляции требуется очень быстрые диски потому что клонирование узла это очень важная операция для таких кластеров и время копирования данных всех данных с одного узла на другое должно быть минимально значит нам нужно нвм я и детей гигабитные connect тут был просто роутинг ну ты потом про него рассказывал но давай я еще раз задам может ты как-то дополнить вот за сервисом в кубе скрывается как мастер так если их при обращении приложению к сервису как работает маршрутизация запросов на чтение и запись у нас есть запросы которые точно требуют мастера если это какая-то транзакция в этой транзакции могут идти изменения значит мы должны отправить на мастер если это одиночный select мы парик эксплуатация мы тот select на какой-то слоев выбранные случайные либо выбранные по весу если это запрос select фолдит например то мы его отправляем совершенно точно на запись и вот крайне вопрос мэра на успеваем потому что маленький какие dfs посоветуете для persistent насти баз данных смотрите смысл репликации в том чтобы не заниматься умножением нагрузки на запись и запросов на чтение посетить смысл репликации в том что нам не нужна диф из мы можем использовать dfs но тогда мы должны использовать такие базы данных которые умеют работать с шарит сторону mais quel разработчики считают что работать с шарит 100 раджим неэффективным поэтому нет смысла хранить данные mais quel на распределенные по его системе лучше хранить на максимально быстрое локальный я думаю все давайте поблагодарим николай еще раз"
}