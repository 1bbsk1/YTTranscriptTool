{
  "video_id": "E6O_TU_twf8",
  "channel": "DevOops_conf",
  "title": "Евгений Дехтярёв — Счастливая жизнь с Kubernetes в продакшне",
  "views": 2203,
  "duration": 2994,
  "published": "2019-07-01T07:08:22-07:00",
  "text": "меня зовут дегтерев евгений я работаю инфраструктурным инженером в компании 2гис приехал сегодня рассказать вам о том сколько нам более радости доставляет купюрницы продакшене давайте вообще проведем опрос кто кубинец использует есть а кто в продакшене его использует хорошо понятно давайте сначала я познакомился с вами познакомимся с нашей командой наша команда инфраструктурных инженерах она создает инфраструктурные решения которыми мы дальше отдаем нашим разработчикам в частности мы поддерживаем кадр nights класть кластер кластер а несколько кластеров баз данных у нас это такие как подгрузка сандра лакируем и как и все через elastic search метрики собираем прометеем иногда ходим на питоне и на к все это инфраструктурное безобразие мы храним в git лобби как а ты диплом с одной кнопки сегодня более конкретно мы поговорим о обернитесь и и его окружение поговорим о том зачем нам вообще нужен был купер нет с как он решает наши задачи без чего он не может решать свои задачи входящий трафик по доставку в губернии мониторинг а логирования давайте начнем с нашего пути как у нас начинался кубинец зачем он нам нужен был известный себя хорошо работает зачем что-то менять как говорится но на самом деле у нас была так у нас были проблемы у нас в компании существовали проблемы со скоростью доставки наших новых приложений на бой то есть продукты придумывать новое приложение разработчики его реализуют а потом упирается в команду системного администрирования которое надо создать virtual очки в одном дата-центре во втором всд и дженги в продакшене балансировку отказоустойчивость достроить базы данных поднять и вот это вот все безобразия занимала еще там неделю две да того как наши пользователи видели наше новое приложение поэтому эту проблему нужно было решать нужно было делать так чтобы как только вы разработали новое приложение через несколько часов его уже могли увидеть наши пользователи если это необходимо вместе с проблемой собственно которую мы начали решать мы решили что для нас нужно также решить решить еще наша задача это унификация инфраструктуру инфраструктуры которая была на тот момент просто безобразном состоянии это ubuntu от 8 до 14 и кто их будет обновлять непонятно например и доставку приложений которую каждый каждая команда пилила по-своему кто-то пользовался антибан шефом кто-то писал ваш скрипт и кто-то руками накидывал на бой а потом когда факап случался все спрашивали как бы восстановить общую как у многих наверное я не буду об этом долго рассказывать я на самом деле рассказал об этом недавно на другом докладе ссылочку на нее виду в самом конце если очень хотите посмотреть наш путь в деталях то посмотрите расскажу кратко скорость доставки мышь или пойти путем кай контейнеризации приложений то есть мы их упаковали в контейнер и закидывали в оркестр атор который уже сам там крутился на бою и приложение поддерживал в жизнеспособном состоянии собственно инфраструктурой доставка приложения она решилась с тем же самым оркестра там то что мы не пилили под каждый не знаю под каждую команду под каждый отдел свой оркестр отара сделали один на всю компанию и предоставляем его всем отделом тем кто хочет тем кто хочет его использовать и не появится быстро у нас инфраструктура все конечно же решилась потому что этот оркестр поддерживая мы там больше нет такого безобразия как было раньше ну собственно ки статора так удобней мы к нему шли шли где-то года полтора и пришли что такое вообще к berlitz в двух словах него есть какое-то количество нот общих какая-то master control plain в которой вы закидываете ваше описание контейнеров как они должны работать вам все дальше поддерживает для пользователя это удобно ему нужно написать всего лишь контейнер собрать и написать окружении как этот контейнер должен работать и дальше отправляет в купюрница и не задумывается ни о чем это удобные пользователи и нам потому что мы как бы один раз рассказали как надо делать и они делают это очень здорово наш путь путь наш и путь на других команд купер лицу ведет вот таким образом сначала мы диплом testing не месяц-два играемся с ним ломаем восстанавливаем смотрим как он себя ведет обновление работают и в общем песочница такая где мы работали дальше мы видим что убивает в принципе он живой отказоустойчивые нашим потребностям удовлетворяет и разворачивается стейджинг в стринги уже запускаются какие-то внутренние приложения для внутренних пользователей на них начинает идти какой-то небольшой трафик мы видим какие-то проблемы которые дома на небольшом трафике можем подпереть там конец маршрутизация сломалась сети сломалась еще что-нибудь в общем стринги мы откат обкатываем это тут главное вот эти все этапы station к testing сделать их конечными то есть вы должны до какой-то вменяемый какое-то вменяемое время на них потратить а потом идти дальше в сторону продакшена потому что иногда начинает делать тест только до тех пор пока его не отточат до идеального состояния дальше не идут но это не очень правильно для бизнеса в нашем случае у нас получилось что мы нас 4 дата центра мы подняли cabernet отдельно в каждом из дата-центров они живут не зависимо друг о друге они ничего не знают то есть все пользователи deep лица в четыре отдельных купюрница что ж у нас ну вообще куда влейте творится вот за два года у нас использованию купюрница туда заехала уже больше 500 приложений мне кажется это хороший point потому что если бы эти 500 приложений они также появились бы в компании за эти два года мы заезжали на виртуалке вы представляете себе 4 дата центра минимум 2 виртуалке на каждое приложение для отказоустойчивости плюс базы данных плюс балансировщик ну я бы сейчас эти virtual очки создавала не доклад рассказывал что же это за приложение собственно у нас очень много пишет то есть опишите это доступ к данным они отлично туда заезжают это вот для примера это например наша избранное наша справочная информация это все записки которые живых оберните авто скейлится при необходимости и обрабатывают пользовательский трафик нам хорошо то есть если что-то падает какое-то какая-та инфраструктура они тут же поднимаются рядышком у нас отлично заехал гид лапку да нет мы еще начали писать описание гид лобан улику вернуться до тех пор пока он как бы появился где-то на самом гид лобби мы его написали как монолитный контейнер большой и это как бы работает хорошо теперь у нас нет более при обновлении гид лобо мы просто меняем номер версии контейнер приподнимается и оно все работает это легче чем антибан накатывать 2 виртуалке еще не знать как то от катишься потом в кабинете отлично живут джо бы то и кроншо бы и какие-то задачи миграции и тут здорово то что вы сказали поверните запускаем не вот это пожалуйста в 1140 по воскресеньям и вы не задумываясь дальше будет жива вот эта виртуалка в 1140 воскресенье или не будет живо вниз просто сам поднимет где надо эту джуббу и будет работать из практических примеров вот например мобильная версия нашего сайта тоже работает в каберне ти когда вы заходите а туда заходят порядка там 7 8 10 миллионов пользователей в месяц они сразу попадают к нам в uber ниц и у них все хорошо еще мы недавно закинули туда нашу карту это горбатая тоже опишешь к и база данных виде кассандры это все же в кабинете и ну то есть если надо увеличить количество инстансов карты мы спокойно увеличиваем их и просто изменение написав одну команду скейл и не создаем дополнительные виртуалке что то еще но все это дело у нас летит порядка пятнадцати двадцати тысяч описав и в принципе предела еще нету то есть мы роутеру наши еще не на пределе а приложение также можно просто дров докидываем купюрницы он будет обрабатывать больше ну как бы эти 500 приложений это все здорово они у нас есть мы создали они как-то работают но на самом деле если бы у нас были только купюрницы только 500 приложений мы бы наверное ничего не делали кроме как смотрели как вообще эти приложения работают чтобы нам помогать в этом обязательно необходима система мониторинга губерний сам очень хорошо работает памяти об этом пишут везде и самку первые цифрами 3 и это удобная система которая автоматически снимает метрики снова контейнеров составе купюрница метрики выдаются из нескольких компонентов это например в кадре за который выдает метрики по контейнерам это потребление циpкa потребления памяти какая-то сетевого сетевая посетила пропускная способность штука классная кубе стоит metrics которая рассказывает вам о том как работает ваш кластер сколько у вас подав сколько запущенных сколько сломанных сейчас сколько у вас нот и вот их лэйблы вся остальная информация и мы для себя еще ставим ноги экспортер который собирает информацию с конкретный с нашей инфраструктуры на которой крутится cabernet чтобы соотносите и что если advisor нам скажут что какой-то контейнер нас потребляется по но дик спортивном скажет что насколько вообще проблемы с циpкa на конкретной ноги а со стороны это у нас очень важная штука для повернуться это сервис дисковые он умеет ходить кадры записочки спрашивать что у тебя вообще там происходит и по каким откуда я могу забирать данные api шичко ему это говорит вам сам забирает данные вам ничего делать не надо у вас появился новый новое приложение vk оберните у вас уже есть метрики по нему это очень здорово чтобы все время не смотреть в метрике олег менеджер обязательно штука к прометею чтобы вы не все время сами смотрели приходили к ним только когда у вас случаются какие-то проблемы но для визуализации этого все дела нужно grafana которая с прометеем отлично дружит то же самое может термиты это те же самые олег ты настроить в гавани это тоже это намного быстрее чем в менеджере например ну вроде связку мы поняли вот у нас есть к бернейс у нас есть прометей прямо у каждого к вернется игра фанатам смотрит на него все это мы можем если у вас один купюрница у вас проблем нету у вас все хорошо вы смотрите мониторить и работаете с ним а что делать если у вас 5 копий армейцев деплоить для каждого графа ну или создавать дашборд вы замучаетесь листать там в графа не этот дашборд чтобы посмотреть все по-своему приложению мы придумали для себя вот примерно вот такую схему у нас есть энное количество кластеров к бернейса у каждого из них есть свой маленький прометей все это дело федерации собирается дальше в основные параметры и основные прометей знают о каждом каберне с класть ее ходят к нему томас 15 30 секунд за всеми метриками это вполне работает и тут еще такая особенность нам нужны обязательно два помета и основной и дублирующий потому что память и говорит о том что если вы хотите мои отказоустойчивость пожалуйста собирайте эти все данные в нескольких экземплярах я вам не предоставляю какого-то единого хранилища и она работает ваша граф она просто сходит к одному из двух параметров и смотрит уже со всеми метриками а еще это помогает решать проблему со старой инфраструктурой чтобы вам не тянуть вот эти вот метрики со старой инфраструктуры в памятнику бернейса вы просто их тяните уже на основные параметры и в одном месте видите как у вас работает с той инфраструктура и как например приехавшие приложение в купюрница работает уже на новой инфраструктуре еще есть такое состояние что у продуктов когда им надо смотреть вообще изменения тренда в своих метрик там за год за 20 и как бы если в основном про мид и хранить все метрики несколько лет это довольно таки дорого там просто дисков ни у кого не хватит тем более создашь них-то мы для себя решили поднять еще один нами три почему бы нет чем больше комментариев тем интереснее и подключили к нему хранилище в виде цифр она бесконечная как эти ребята кто на молода и мы туда просто складываем те метрики которые нам нужно хранить долго при необходимости собственных grafana можете обратиться либо к одному про метаю либо к другому вроде как вот у нас есть кубинец у нас есть мониторинг виде прометея уже что-то такое не просто купить у нас получается и рассмотрим ситуацию нас приходит в нашем приложении вк оберните она сломалась приходит alert мы такие приходим система мониторинга и смотрим на лев все сломалось а чужие вообще произошло то что произошло нам помогает понять собственно система журналирования которая хранит просто логия что у вас случилась принципиальная схема от кубер лица как поступают ваши ноги и дальше ваш вашей системы логирования из контейнера они пишутся просто-напросто в 100 дауд выстрадал для разработчика а для нас инженеров это просто файл на конкретные машинки из этого файла вычитывает агент логирование данные построчно и отправляет их дальше backend где хранятся ваши логия ну у нас например на той ластик в этой схеме нас на самом деле интересует вот это звено это наш агент логирование который читает данные обрабатывает отправляет дальше на каждой машине со стороны подвергается предлагаются решение в виде флинта то есть он как всегда говорит что у меня есть вот такая вот такой порядок доставки логов а дальше вы можете вычитывать их в принципе как как вы хотите но я вот вам предлагаю флинт можете использовать его мы попробовали и флинт мы попробовали и прямо лог старше закидывать на конкретную машинку это все дело в принципе работает работает довольно неплохо но как только у вас появляется какое-то более-менее нормальный поток логов там хотя бы ты стать 400 500 сообщений и у вас есть немного логики на флинн телёнок старше это все дело потребляет одно ядро циpкa и один гигабайт памяти на себя потому что в одном руби в другом уже руби по моему и как бы представьте если у вас станут overhead просто какое-то страшное кажется это стоя бьется пульс 100 гигабайт памяти да это еще один кластер кубинец который вы просто на просто не подняли где-то или это вот то что вы могли бы выделить на ваше приложение которые нуждаются в памяти или в циpкa мы для себя решили почему бы нет мы сделаем своего агента логирования нога уже любим ногой писать надо было практику нарабатывать мы написали своего агента логирование нога который я не стал потреблять 05 сапу это только на загруженных машинах где там поток логов типа 56 тысяч сообщений в секунду а где поток логов там до тысячи сообщений в секунду он потребляет 01 циpкa и это очень здорово памяти он у нас никогда не больше 100 мегабайт мы даже не знаем нужно она ему или может вообще отключить вы собственно если сравнивать с предлагаемыми шение могу вернуться и с тем что сделали мы мы просто напросто сэкономили а сэкономил считай заработал и еще больше приложений могут нам заехать мы можем просто-напросто предоставить всем больше ресурсов и они будут работать лучше но это еще не все есть такая штука как с вами три кита как агент на go написаны на на у нас теперь мы можем его нормально кастомизировать в коде и делать из него все что мы захотим и мышь или ну как бы нам надо знать как работают наши продукты не просто в общем виде типа вот у нас продукт работает там отвечать всегда за 200 миллисекунд а потом мы появились а продукта стал отвечать за 500 миллисекунд а может быть ну как-то это не так и поэтому мы решили что мы должны снимать наши метрики по работе продуктов это во-первых относительная доступность продукта которая говорит о том сколько у тебя было 500 их за период времени например то есть в день ты 500l там три процента или четыре или пять или вообще не 500 и время ответа по конкретным методам то есть вот если вы общее видите что вас продукт после релиза стал отвечать за 500 миллисекунд вместо 200 то вы можете зайти в частности в конкретные методы и посмотреть а может быть у нас только один метод стал отвечать дольше и видеть что этот релиз поломал вот это конкретное место и это очень сильно упрощает разбор проблемы вы просто идете вот то в то место где поломались и чинить и ну собственно с агентом вроде мы разобрались о чём нас дальше то дальше вся логе у нас попадают в агент логирования в логин backend простите по командам у нас ну и как у многих я думаю выступает а ластик и тут такая вот ситуация у нас 100 нотку дыр лица и есть один ластик бедняга против один против ста что мы что будет если со всех сторон у нас отправятся лаги и ластик но он взорвется просто напросто потому что ребята я за вас текау себя даже в документации говорят что если вы хотите отправлять ко мне с большого количества ресурсов данные вы должны агрегировать эти ресурсы где-то и только с одного места потом отправлять в меня иначе мой сетевой стек ну так все не переварит это как собственно поступили мы для решения этой проблемы мышь или агрегировать от агрегировать наши лагги в каком-то buffy буфер мы изначально выбрали в виде rubbed and you ну вроде как у вас сообщение rabbit in kyoto очередь сообщение все логично вы отправляете сообщение очередь сообщений эта штука работает хорошо удобного момента когда у вас происходит при выполнении rubbed and you например за раббит include там же еще есть pipeline которая доставляет логия пластик когда происходит затуп этого дальнейшего 100 к рыбе танки начинают копиться сообщения и если при состоянии когда у него в очереди 0 сообщение он обрабатывает нам спокойно 23 из 30 тысяч сообщений в секунду то когда у него например очередь сообщение выросло до миллиона и потом наши дальнейшем backend он просто sales он начал дальше нормально работать то скорость высчитывания из этого миллиона составляет там 67 тысяч сообщений в секунду и это вроде как нормальное поведение для робята они просто горят не допускайте вот таких затупов пожалуйста то есть получается так что когда мы в день у нас в течение дня на прием выросла очередь до миллиона у нас стандартный поток в день сообщения там 15000 например в секунду в этого кролика оон вычитывает только шесть оттуда ну как бы вас от очередь растет растет растет и вычитывается только ночью когда у вас поток сообщение происходит но уменьшается и в течение дня просто-напросто если случается какая-то проблема наши разработчики не могут узнать а в чем же вообще у вас проблема то случилось потому что логов нету а и там надо идти в каберне из cubesat релакс вот это все никто не хочет делать все хотят смотреть в кибом мы для себя ну собственно избавившись от ребята мы перешли в сторону редисом я на самом деле скажу что родится у нас уже был у нас было два буфера это рабби thank you для купюрница и радист для legacy систем и вот как бы сравниваете два буфера мы увидели что если на орбите есть такие проблемы сочи идет в одессе их нету если там очередь растет она забивает всю память то как только дальнейшей pipeline а это собственно voxtel и власти он дальше начинает работать ну продолжает работать в нормальном состоянии очередь и сгодится вычитывается буквально там 6 гигабайт за 10 минут то есть как только у вас а 6 гигабайт это порядка там не знают 3 4 5 5 5 миллионов сообщений в радиусе то есть когда поймали начинает работать на нормальном состоянии за 10 минут эти ваши логе можно сказать доезжают до ластика и ваши разработчики спокойно видят что вот мои ноги если есть проблемы они на них смотрят ну смотрите у нас уже вырисовывается какая-то какое-то окружение купируется то есть у нас был просто купюрницы в нем есть 500 приложений которые как-то работают теперь мы знаем как они работают исходя из системы мониторинга и можем реально посмотреть какие-то факап и или логия этих этих приложений но этих 500 приложений не было бы там если бы у нас не было каких-то брендов к этим приложением давайте вообще посмотрим отвлечемся чуть-чуть посмотрим какие приложения могут работать в к верните нормально это могут быть какие-то джесс приложения то есть это но-но-но . которая при подключении к ней клиенту клиента кнопочки например на скачивает к себе леску и уже на клиенте запускает дальше сам самого клиента идет работа то есть у друга раздача раздача джесс файлов туда отлично заезжают в митинге у нас много форм тендеров фонд индира делают лендинги и просто пакуют их в докер-контейнер а там уже закидывает к бернейса сколько угодно он просто-напросто увеличится количество экземпляров можно увеличивать до бесконечности если вдруг на ваш лендинг пришло много запросов в кабинет это отличное место для раздачи небольшого количества статике это как бы если ваша статика вмещается там например в гигабайтные контейнер вы можете собрать его закинуть купюрницы раздавать эту статику оттуда поджога я уже говорил джо бы крон джо бы это все работает там хорошо еще у нас зашел такой кейс для купюрница это тепло и туда контейнер с машин лингам то есть там там получается как саму модель для машин леоненко у нас подготавливают ребята у себя на инфраструктуре там где-нибудь где гapрu например есть они ее готовят обрабатывает на каком-то объеме данных а потом отдают нам уже готовы контейнер который просто может потреблять конкретный циpкa и обрабатывать данные то есть мы сзади плыви туда кантине кидаем в него данные он исходя из своей модели как-то эти данные обрабатывают его дает нам на финале что должно что должно быть из этих данных и машины ning туда отлично зашел потому что это очень ресурсоемкая штука по циpкa и в кабинете есть авто скиллинг и очень удобно то что вы например днем можете держать там 10 экземпляров машины чтобы он обрабатывал хоть немного но оперативных данных ночью когда у вас нет нагрузки вы просто говорить а вот с 21 пожалуйста авторский сын до 100 экземпляров и все у вас ночью ресурсу не простаивают купюрница не простаивает а другие пользователи этого не замечают потому что вас просто нагрузки нету на самом класть и но мы же ее собственно говорили о папе жки что у нас много описок и что почему я них не сказал а не сказал я потому что опишите у нас любят база данных это может быть под газ это может быть мускуле это кассандра те же самые rabbit and you то есть любые бэг-энда где хранятся ваши данные ну для себя в своей компании очень много используем полюса поэтому мы как бы из брендов выбрали по сбросу и тут вышло такое дело то что если у вас есть к бернейс где у вас легко поднимаются приложение вы на каждое приложение будете поднимать свой поиск власти ну как бы вы ничего не выиграете мы подумали над этим и решили что у нас все-таки микро сервиса они же маленькие поэтому и базы на них будут маленькие и мы подняли один большой кластер полоса на всяком на-на-на большинство команд он удовлетворяет потребности наверное 80 90 процентов от тех приложений что у нас есть и запишет купер нити и тут получается так что как только разработчики пишут описку они просто-напросто не просят нас поднятием дополнительные базы настроить на них мониторинг отказывай устойчивость еще что то они просто носят доступ к общему кластер получают его и работают с ним как обычно поэтому общая база данных для купюрница это как мне кажется маст хэв ну как бы в нашем случае это под газ в вашем случае это любая другая база данных которая вам у вас используется а нам нужно это просто напросто ппу у бы стоит вашу доставку приложение до боя и сделает сократит ваши opel сон вашу операционную работу по поднятию отдельных экземпляров ну мониторинг журналирование база данных вроде обговорили тут еще такой интересный вопрос а как у нас уже профи кто попадает в cabernet вот у нас 15-20 тысяч с что как они попадают что с ними происходит дальше кто это обрабатывает принципиальная схема must стандартная есть клиент он попадает на in gas контроллер это так называемый это так называемый роутер которые содержат все правила обработки конкретных состав дальше с по этим файлам мы попадаем нужные сервисы и конкретные приложения кубинец как всегда не говорит используете вот это он предлагает вам на выбор а вот столько сами выбираете пожалуйста ребята вот вам тут расширение кругозора вот это все это обычный индекс истек проект трафик и kong то есть можно в принципе когда вы только внедрять и всякую верните вы можете на этом моменте сесть и и не знаю закопаться каждый инструмент попробовать посмотреть его плюсы минусы и так у вас пройдут полгода жизни скверные цену вы просто напросто взорветесь на вот этом моменте чего делать как мне кажется нельзя мне кажется что нужно выбрать какое-то более-менее то решение которое вы знаете если вы работали с истребить есть если работали с engine сам берите его на первый момент на первое время вам этого хватит а в будущем если вы хотите перейти на новое решение ну это кажется сделать не так сложно и дорого потому что они все есть и вы просто поднимаете его как еще один микро сервис рядом я расскажу немного о нашем пути как мы вообще шли вот к нашим как как наши ходящий трафик заходил к обернуться как мы с ним работали у нас все так же как у всех есть клиенты клиенты приходят в с балансировщик и которые по bgp клиентов ближайшему дата-центр раскидывают из весов у нас трафик попадает в кадр меняется и попадает у нас он на такой проект как громко ingles контроллер от проекта ds ds это аналог heroku кто не психику работал есть люди хорошо в общем это г б д с а то heroku созданную себя собранного коли ребята вообще этот проект делали как свой регистратор контейнеров в них первая версия была на кой россии и кучей систем для юнитов которые управляли вот этими вот всеми контейнерами по своими по по имеющимся но дом там все в принципе работало хорошо на разваливалась часто потом ребята сделали д из второй версии и он уже работал на основе купюрница они не стали ничего делать своего никеша дублеров никаких контроллеров то есть они не лезли в механику купюрница они сделали грубо говоря о фишку надо пешкой купюрница и позволяли купюрница работать самостоятельно а не просто через себя проектировали описание приложений в cabernet и он уже сам их создавал это все было здорово мы на самом деле начинали с дэвиса 1 потом пошли до из 2 а потом только в купер ниспошли а почему мы туда пошли то что нас вынудил вынудил нас товарищ microsoft который увидел что проект до из второй версии людям нравится разработка хорошая работает это все здорово полив купюрница эдип ловится через доску бернейс оказал одну казалось что это проще чем че ставку бернетт потому что кубе он подразумевает под собой какое-то количество знаний которая вам нужно нужно знать перед тем как заезжать туда поэтому ребята из microsoft а просто купили проекта через три месяца закрыли его сказали вы выбито классные давайте к нам работать ваш проект на закроем у них как бы и получилось вот так что мы просто-напросто остались с нашим купер лицам и с проектом который уже и о его лица вот через несколько месяцев нам надо что то делать с этим собственно что делать до из нам пришлось отказаться от него но нам его руки очень понравился поэтому мы взяли обвязали его еще больше кошечкой и поехали на нем то есть мы оставили его населена поддержки в двух словах что это вообще такое и то go приложения которое под собой запускает яндекс и configure the dangers исходя из информации в api серые о купюрница девайсах которые там находятся но собственно там дальше история простая то есть там сконфигурирован jets там ваши домены ваши не ваши описания как вам надо для вашего сервиса работать по какой по конкретным доменом а общее описание и общее описание судя по всему подходит тоже там чуть ли не для 80 до 90 процентов приложений этого хватает почти всем им не надо выдумывать свои ingress контроллеры ингрос правило дальше вот из роутера попадает трафик сервисы и воды в ваше приложение мы предоставляем чуть-чуть кастомизации пользователям на уровне описание cabernet сервисов и это не так чтобы они писали прям лакей шины в яндексе это на самом деле простейшие описание в несколько строчек вот например у нас на 2-е вроде восемнадцатый год уже конец да и без сертификатов как-то стрёмно выпускать приложение на бой а выдавать сертификаты 500 с лишним приложениям это там нескольким сотням разработчиков тоже как то не хочется поэтому мы взяли просто волковы сертификаты положили в кубер ниц и сказали что если ребята у себя в коде напишут вот 2 магические строчки с имеем домена из привязка из моего домена к имени сертификата которые захардкожены то они уже по умолчанию использует свои сертификаты таким образом просто-напросто мы не отдаем сертификаты на лево и на право но все наши приложения уже работают безопасно также если к приложению любят приходить какие-то постеры или боты или что то еще он может на нашем внешнем пути он может ограничиться ограничить свои рпс а тоже одной строчкой буквально черные и белые списки ну штука хорошая да то есть если к вам пришёл к попсе которая совсем уже отъявленного спасти твоего вписали в черный список и поехали дальше и для себя мы их как-то не очень приятно в общем для себя мы еще сделали такую штуку как request айди то есть когда у вас несколько десятков микро сервисов представляет из себя одно приложение вам нужно знать как у вас проходит запрос пользователя от одного микро сервиса к другому и к сожалению нас еще нету системы трассировки типа йоги или zip кино поэтому вот на первом этапе мы сделали квеста иди которые просто напросто прокидывает в хоррорах и каждое приложение влоги рук выводят этот request айди таким образом в киба не просто жмешь что до нее весь путь вот этого request a иди и он тебе показывают вот я заходил сюда то сюда то сюда то и сюда то это обычный режим совские квест айди который впитывается там одна одной командой в джинсе вот смотрите у нас уже купюрница как бы появился такой не просто поверните наших 500 приложений 500 приложений уже целая кота обвязка и она почему-то не заканчивается а еще это на самом деле не все у вас есть куча приложений которые надо как-то доставлять обновлять изменять в кубер нити в общем тут интересно конкретно доставка фку бернейс принципиальная схема всем известно себя сиди написали код систему контроля версий тесты прогнали создали образ и закинули в кабир нет инструментов для этого большое огромное количество это дженкинс трэвис гид лапкой the circles я и там еще и я даже не читал их всех мы для себя остановились на git лобби потому что он просто умеет запускать джо бы а в жопах вы можете делать все что угодно ну и кот у вас хранится в git лобби тут же под рукой зачем вам что-то больше зачем вы думаете что-то лишнее если вы все простое здесь мы просто взяли гид лап и прогоняем все эти этапы там но мы же говорим о кабинете поэтому нам интересно как нам доставлять наши приложения vk бернейс где-то года когда два года назад когда у нас только появлялся к бернейс мы доставляли приложение вообще туда как мы для начала взяли просто готовые яму файлы и закинули туда работает здорово у нас один тестовой кластер повернется он создает приложение все хорошо потом у нас появилось два кластер приложения это одни и те же их надо просто празднуем кластером раскидывать и мы попробовали фильм который нам зачем-то пакеты собирал что-то с ними еще делал там хранился кита конфигурации мы не поняли зачем это нам надо и написали свое как говорится которое умеет просто доставлять готовым ямал манифесты в кубинец то есть эта штука называется кагосимы скандального концерте она написана на питоне таким образом любой из нашей команды ну как бы сисадмины инфраструктурные инженеры devops они осознают питон поэтому каждый может подпилить это приложение под себя или закинуть нам пури квест на изменения ресурсы которые мы туда доставляем они написаны ямам и шобла низе руется джинджи ну и кто работал сансе было наверное все знают и я мало джинджер и уже десять раз и пользовались 10 миллионов раз простите и еще такая штука мы не заставляем никого ничего устанавливать ну кроме токио которые так стоит наверно у всех у всех стоит докер нормально в общем эту штуку доставляем до кирк поставляем в докер имиджа и вы просто делаете докер пул и поехали и запускаете уже из докер образа доставку своего приложения мы его называем нашей маленькой turbo тележкой вроде доставляет быстро и хорошо разберем маленький пример как это вообще работает потому что ну как бы инструмент новые не все о нём знают вот у нас есть минимальный минимальная конфигурация которая необходима чтобы доставить ваше приложение в купер лиц это файл конфигурации конфиг ямал где задаются ключ-значение для джоша 2 шаблонов это собственно описание deployment а и сервиса в deployment и ваше приложение описывается в сервисе описывается маршрутизация до вашего приложения конфигурационный файл вы это обычная мо он состоит из нескольких секций секция comon например описывает стандартные параметры имя приложения порт на котором она работает количество реплик в котором наложит должна работать и вот здесь написано имя образа которая запускается для этого приложения и секции стейджинг тут же может быть testing production in production 2 3 5 10 сколько угодно или там testing тоже может быть 10 секций для каждого тестировщика своя и там описываются просто шаблоны которые в этой секции deep ловятся и при необходимости и определяются переменные которые вот для вот этой секции необходимо переопределить например для тестами мы захотим использовать engine 114 версии мы просто и определили его для нужной секции и используем в файле deployment а это обычный коберна писака обычный кубинец deployment где написано имя приложения реплики и все остальные параметры вот как бы вы написали чистый ямал просто его обернули несколько скобочек от g&g и считаете это переменными в сервисе то же самое то есть и тут такой момент вот смотрите у нас есть me to do to name a pop ним и здесь тоже metadata на и мы попадаем и таким образом изменив один параметр в файле конфигурации вы можете за деплоить два одинаковых приложения просто рядышком под разными именами и работать с ними вот двумя разными командами разработки при желании и это вам стоит буквально две строчки это описание еще одного окружении файле конфигурации вот вместо stay джинга тест 2 например и в эту секцию написать об имеем шаблоны вы можете вообще в ком он вынести и работать с ними по общему принципу как это все делается вот так быстро скоро скажем чтобы это вам сзади площадь вам на запустить докер-контейнер в него прокинуть ваш каталог где у вас лежит файл конфигурации файлы шаблонов и покинуть туда конфигурацию подключения купер нет класть его которого снова калия записано то есть но это для варианта когда вы плакали запускаете вы можете параметры подключения куперу записать просто-напросто в конфиге ямал в тот же самый или инвалид переменными передать дальше мы просим докер запустить в контейнер из нашего образа 2гис k8n да и выполняем команду кагосимы скандал пожалуйста за диплом не секцию стейджинг а для подключения используя клубе конфиг и кагосимы с хан был нам отвечает говорит о я нашел этим плиты я из них сгенерирую сгенерирую финальные я могу манифеста вот он собственно deployment и сервисном сделал потом дальше он говорит нам что у тебя не было такого дипломата и сервиса я создам его создает его и если у него все успешно возвращает 0 код ошибки и цветочки вам по цветочкам вы всегда видите что вас все нормально все завершилось успешные даже можно не считать тугой текста которую он там написал проверив что у нас появилось оберните мы можем вот сразу после диплом заходим и смотрим есть у нас deployment да есть есть у нас сервис down появился коды создались и в поле мы видим что конкретно нам создал сам нужный контейнер с нужной версии рен джинсы и вот в принципе мы уже подняли какое-то окружение рядом с кубинцем без которого наверное наличие тех 500 приложений которые есть у нас сейчас нам было бы сложно их поддерживать обновлять деплоить работать с ними но это просто-напросто у нас отнимала бы все свободное время и мы по не вносили ничего нового в нашу систему то есть в принципе считаю я считаю так что купил ведь сам по себе ваших проблем никаких не решит вам если вы хотите себе купюрница вам нужно готовиться поднимать к нему еще дополнительные компоненты которые просто был помогать вам намного меньше времени тратить при работе с кубер никсон вы всегда будете знать как у вас там приложение работают мониторинг у вас будет за ними следить они вы самостоятельно логе у вас всегда будут при необходимости быстро разобраться что у вас там случилось если олег прилетел и главное эту экосистему всю сделать очень удобный потому что чем удобнее система тем проще пользоваться не только вам ну как бы разработчикам которые будут основными пользователями вашей системы им нужно будет за деплоить свое приложение посмотреть как она работает когда они проверили какой-то и лиса не должны посмотрите как этот релиз отразился вообще на производительности приложения и если это будет неудобно то как бы новых приложений у вас от этого больше не будет в вас разработчики будут больше тратить время на поддержку старых приложений чем на разработку новых поэтому удобная система это кажется именно то что нужно для купюрница это что позволяет туда закидывать все больше и больше приложений спасибо вам большое что пришли на мой доклад пока обходу есть ссылочки на гитхабе на гитхабе лежит и самопрезентация и ссылка на к 8 сэндоу например его и мои контакты если у вас есть возникнут вопросы еще в будущем и там же есть доклад о том видео доклада о том как мы обшили когда инфраструктуре если вам это интересно спасибо есть какие то вопросы браст спасибо за доклад вопрос такой вы сказали что вы писали своего агента но для работы с и ластиком у них же есть там набор file-bit.net беду них свои агенты они не подошли или вы просто не пробовали или как мы пробовали файл бит но еще 2 инфраструктуре и у нас с ним был негативный опыт скажем так то есть получалось так что файл бетона засылал блок стаж напрямую по порту уже минуя буфера если у вас лук стоишь лежит по где вас купит в каталоге а файл бит там такси с буферами работает он с кроликом не умел сходи сам по умеет но там какой-то свой формат и получалось так что когда файл бит у нас за салона прям за совал напрямую в лог стаж и стаж иногда просто зависал мы не знаем из-за чего это произошло то есть вот идет рейсинге мы настолько глубок глубоко мы наверное не капнули нам было просто дешевле отказаться от файл битов пользу других инструментов на легаси там полгода назад ну как это было года два с половиной три назад на то есть еще до купюрница даже поэтому мы его особо не усматриваем спасибо здравствуйте спасибо за доклад а расскажите пожалуйста а как вы деплоить вот такая систему вокруг uber нету со непосредственно то есть prometheus пластик это отличный вопрос мы все диплом по старинке анселом то есть сам куда мне тепло adsense блам памяти это отдельные машинки которые одеваются антибан прометею верните де пояса кагосимы с хэлом например то есть это же отдельными к сервис мы его просто мы по телику бернейс он заработал бы сзади плывет туда прометея он работает в основном прометей которые мы за тепло и леонсе было мы связали его с новым кубер не сам поиск новым прометеем тот же ластик эта вся система пока что отдельно на virtual как она deep ловится тоже анселом this тащить ее в купер нет стоит или нет это такой вопрос салливан и потому что как бы держать все в одной корзине вы потому и логов не увидите купюрница не увидите но опыт деплоя и ластиков кубинец например есть и он на самом деле очень положительный потому что это намного проще за тепло ее тела стиков купюрницы и обновлять его чем ван сибли выдает все тоски прогонять если где-то на 1 свалятся или ansi был обновиться или что то еще у вас там короче мест где поддаваться намного больше чем когда вы сзади плыли в кабинете для обновления просто поменяли одну цифрам и мышь вершин и все вот чё там роутеры а у тира это тоже кубер на это приложение она тоже через к 8 скандал deep ловится база данных это анти был у нас есть база данных кассандра она например в кубе найти надевается к 8 и скандал рабби темп мы закинули тоже в кубинец и он неожиданно работает хорошо то есть мы подняли этой машины связали в кластер написали небольшую обвязку как с ней удобно работать на питоне и все и работает"
}