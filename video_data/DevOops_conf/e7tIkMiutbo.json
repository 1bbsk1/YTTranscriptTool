{
  "video_id": "e7tIkMiutbo",
  "channel": "DevOops_conf",
  "title": "Алина Власова — Как сделать стабильно, когда тысячи разработчиков могут всё сломать",
  "views": 255,
  "duration": 2857,
  "published": "2022-02-02T01:26:30-08:00",
  "text": "добрый вечер всем меня зовут виктория я senior security архитект makers of цены в microsoft норвегия и сегодня со мной в гостях алина который работает как software development manager в лаборатории касперского привет аллен как дела во первых да извините я у меня сразу же вопрос чем же занимается software development group manager но я руковожу группой в нашей компании которые поддерживает process continues integrations как раз вот мы на репозиторий у нас в лаборатории касперского чем и будет мой доклад мы поддерживаем этот процесс автоматизируем различные инструменты для его работы удобства пользователей вот примерно этим я и занимаюсь но не я моя команда я идею руковожу и команда она большая у тебя или если не секрет сколько людей моя личная команда там вне 7 человек если говорить обо всей команде там нашим она репо то это там порядка 40 50 человек не знаю точную цифру и у меня торговать тоже вопрос возник почему ман ман репозиторий ну я думаю что ответ видно на этот вопрос можно будет на и услышать моей презентации так он как раз содержится это подробно рассказываю почему мы выбрали именно такую модель почему будем работать так я думаю что стоит дождаться в презентации там можно услышать ответ на этот вопрос то есть никаких спойлеров мы должны не так не так долго ждать осталось чтобы слышать на этот ответ на этот вопрос поэтому почему бы просто этого не сделать вполне согласна ну что же давай тогда приступим свои презентации я очень хочу посмотреть и узнать почему же вы выбрали моно репозиторий и как вы сделали все стабильно круто тогда я перейду к своей презентации еще раз всем привет меня зовут алина власова как меня уже представила виктория я работы лаборатории касперского сегодня я расскажу о том как сделать стабильно когда тысячи разработчиков могут все сломать или как организован процесс continues on ты грешен в лаборатории касперского о чем будет мой рассказ вот как раз будет ответ на вопрос почему именно моно репо почему тысячи разработчиков могут все сломать где же заканчивается спокойствие этих разработчиков которые могут все сломать с какими проблемами они сталкиваются и как мы возвращаем их спокойствие для начала я расскажу немного предыстории ранее в нашей компании так сложилось что была модель разработки когда каждая команда жила в собственном отдельном репозитории устанавливала твой там свои собственные правила как она хотела выбирала себе любую систему контроля версий в которой она хотела работать помимо того что команды жили в различных репозиториях они ещё имели каждый свою инфраструктуру для поддержания вся и процесса кто-то даже разработал свои собственную инфраструктуры чтобы поддерживает этот процесс в общем каждый жил так как считал нужно но в нашей компании есть такая особенность что у нас есть взаимосвязи между командами у нас есть там какие-то общие компоненты зависимости между продуктами и чем больше становилось этих зависимостей с течением времени тем сложнее нам было разрабатывать когда нужно вносить правки в соседней команды расскажу немного на примере для того чтобы всем стало понятнее в чем же все таки у нас была проблема допустим мы хотим внести изменения в команду один в проект 1 и от этих изменений зависит команда три четыре пять так как я рассказываю это на примере реальной команды из нашей компании то будем мы придерживаться реальных условиях которых они работали так вот изменение команды один поставляются в команду 345 бинар на это как до ног нашей задачи сейчас я рассказываю изменение в команде один сделано без поддержки обратной совместимости команды имеют разные системы контроля версий например команды один там пусть будет жить perforce и команда три в ките команда 45 в tfs и каждая команда имеет свой собственный сиай какой то что же делать разработчику который хочет внести подобные изменения свою команду и поправить зависимости в тех кто от него зависит во первых ему нужно найти всех своих клиентов и это могут быть совершенно разные репозитории которые нужно пойти поискать нужно понять там вообще кто тебя использует может быть у кого-то спросить может быть где то есть какой-то реестр всех кто тебя использует а может быть он уже устарел и скорее всего кого про кого-то забыли и нашли только спустя время как нибудь команду 7 которая на самом деле тоже зависит от правок в команде один а может быть таких клиентов еще больше есть команда какие-нибудь 89 и они будут находиться с течением времени там например через два дня через неделю через месяц нам постоянно будет приходиться возвращаться к этим правкам к этой задаче и направлять каких-то еще командах все таки для тех кого мы нашли нам надо поправить сделать эти изменения поправить у них чтобы они теперь были совместимы с нашими новыми изменениями и нужно подстроиться под разные системы контроля версий которые в которых каждый день разработчик не работает git lfs может еще какие то системы контроля версий от абсолютно неважно вот нужно пойти в каждую вспомнить как вообще с ней работать может быть даже каких-то моментах загуглить как с ней работать и внести эти правки но наконец-то мы преодолели это препятствие вниз приправки запустились какие-то валидации он ее билды в этих командах и они круто если они не упали и все зашла с первого раза это успех но скорее всего они упадут потому что мы что-то сделали не так нужно будет понятен что мы сделали не так разобраться с ошибками а так как это какая-та инфраструктура которую мы видим первый раз но может быть не первый раз но все равно мы не каждый день не работаем нам нужно сначала понять как в ит-инфраструктуре посмотреть логи почему упал build почему упал тот или иной тест общем мы потратим на это много времени последние то необходимо синхронизировать изменения между всеми этими командами но для того чтобы мы могли в последствии поддерживать мы справились какие-то баги в нашем коде и что такое нам нужно чтобы изменение в этих командах были одной версии поставленные если в каком-то случае при каких-то обстоятельствах мы поймем там при внесении изменений в одной из команд что нам нужно опять поправить изменения в команде один то нам придется пересобрать этот билд и опять перед поставить во всей команды но новую версию наших правах а теперь давайте рассмотрим все те же самые проблемы с ту же самую ситуацию если мы возьмем и переложим весь наш код в общей репозиторий и сделал ему это в общем репозитория еще общую инфраструктуру которая будет использоваться при комментах в этот репозиторий первая проблема найти всех своих клиентов но она как-бы перестаёт она очень легко решается в общем репозитории можно взять поиском поискать найти всех кто тебя используют и как бы в них поправить кажется что тут мы потратим гораздо меньше времени на выполнение этих шагов дальше проблема что нам нужно подстраиваться как под какую-то не привычную систему контроля версий она на самом деле тоже отпадает потому что нас в общем репозитории единая система контроля версий и мы берем и делаем правки в привычный нам среде в которой мы работаем каждый день при и валидации коммита у нас запустится валюта ционный бьет в инфраструктуре где мы каждый день работаем все уже знаем как что смотреть нам все привычно мы совсем разбираемся и на это тоже не надо будет тратить время чтобы там подстроиться под какую-то другую непонятную ну и последняя проблема синхронизации изменения между командами она конечно тоже останется но в таком общем репозитории с общим фрай мы можем взять настроить эволюционный build общей где по всем зависимости у нас сразу при кометных все перед тестируется на этой общей январе и даже там вольется мастер бинар даже с подменой пинар этой вот поставки которую я говорила до этого кажется вся эта проблема тоже становится более менее решенный и вот мы посмотрев что на самом деле такой подход в целом решает нашу проблему мы в какой-то момент приняли решение что нам необходимо переехать всем в общей репозитории помимо того что переехать в общее репозитории и перевести всех в общую фру это решит как проблему что у нас будет одна вообще инфра на этот общий репозитории и все будут пользоваться одинаковыми старыми инструментами так и решает проблему что каждая команда поддерживая свою in fruit тратила на это ресурсы сейчас это будет единое централизованное место и не каждая команда будет решать одни и те же проблемы по кругу а вот эти проблемы решатся один раз и будет все сосредоточено в одном месте вот как я уже сказала мы приняли решение переехать моно репу сделали там общий git репозиторий выбрали подход к разработке транг бездетным вот на этом графике показана скорость доставки изменений какой она была когда мы приняли решение ехать в моно репозитории какой она стала поставить мы переехали этой как раз вот на примере той команды на который я рассказывала о наших проблемах кажется что это время сократилось в десятки раз что говорит о том что начали двигаться в правильном направлении что же такое моно репо в нашей компании это git репозиторий размером порядка 50 гигабайт в него делается около 350 паре квестов в день и и в нем имеется около 150 уникальных контрибьютором день но как я уже до этого рассказывала для нас минарета это не только общие репозитории это еще и общая инфраструктур обмана репы надо есть часть инфраструктуры которые мы на самом деле разрабатываем внутри нашей компании она на этом слайде в виде на красной рамкой это общий сборочный конвейер асгард общей инфраструктура тестирования хаев распределенное хранилище артефактов и общая система сборки базиль систему управления репозиториями pipeline ами ажур devops и у нас есть для всей этой free единое окно поддержки дальше хоть это и не является темой моя презентацию но я пережил ваши вопросы зачем мы написали собственную инфраструктуру какие проблемы мы решали и я немножко таком в качестве обозрения сейчас расскажу зачем что же у нас за инфраструктуры зачем мы и написали какие проблемы мы решали для того чтобы впоследствии вы как-то могли лучше прочувствовать нашу проблематику погрузиться в большую в наше кто как у нас все устроено и так общий сборочный конвейер асгард вот тут на картинке слева представлена как он выглядит его гуй в среднем за день он обрабатывает порядка трех с половиной тысяч пилотов и 30000 драбов сразу логичного прочитаешь такое джаббы у нас бедных состоятельных рабов то есть один build он может собираться под различные конфигурации там какой-нибудь компонент мы соберем сразу под винду и под linux под мак еще там по зависимостям после компонентов какие-нибудь продукты и каждая такая сборочная единица это джабба для того чтобы понять масштаб нашего conver я тут привела картинку что в день он там в пиках потребляет до 5000 процессорных ядер и так почему приняли решение написать асгард во-первых мы хотели добиться высокой утилизации агентов и вас и интеллектуально распределять сборочные джаббы что это значит в обычных системах для каждой такой сборочный день саджа бы выделяется какой-то определенный агент на в котором она собирается или выполнять там какие-то действия в рамках сборочной процедуры у нас в компании очень разнородные эти джаббы у нас за и джаббы как который там могут 30 минут просто компилировать и там будет стопроцентная загрузка циpкa а бывает джавы которые пять минут идут перекладке thirty факты пакует что нибудь что нибудь и кажется что на каждый из таких грибов как-то жирную выделять по агенту можно их взять скомпоновать все умна и параллельно выполнить на одном агенте несколько таких рабов тем самым сильнее утилизировать железо также мы хотели добиться отказоустойчивости потому что я это еще расскажу дальше своей презентации но забегая вперед скажу что у нас очень большие билды бывают в которых десятки сотни ja баф бывает они очень долго длятся там часами то порядка двух-трех часов и если мы один из сайт агентов она скрыта пропадет а мы ждали тому же 21 часа из-за этого упадет бьют то нам очень дорого это все перезапускать ждать и поэтому мы хотели свою отказа устойчивую систему иметь также мы не вчера приняли решение написать асгард мы занимаемся этим с 2017 года если меня сшибает не изменяет память ваш даже чуть раньше и на тот момент среди аналогов мы не могли найти того как в котором можно было бы описывать в source контроль и непосредственно своей бьет процедуры с указанием там всех зависимостей поэтому это тоже была одна из причин по которым мы начинали писать асгард также мы не хотели зависеть от каких-то сторонних систем чтобы при их изменениях без поддержки там обратной совместимости с прошлыми версиями нам приходилось переписывать свои билды которых у нас там сотни и тратить на это ресурсы и как так сказать подарок за то что вы имею свой сборочный конвейер мы можем развивать в любую сторону которых какой хотим у нас появилась нашим к двери и функциональность поиска ломающих к битов то есть те если ломаются какие-то там сборочные джаббы тесты которые не ходят валюта ционный пятна по реквизитам мы можем пост-фактум эффектом найти ломающей комет и откатить его например или просто разобраться с его проблемой также у нас есть общая инфраструктура тестирования hive вот тут на картинке представлена печная сессия в хайве с тестами запущенными среднем из рабочий день в не запускается порядка 15000 сессий она обрабатывает порядка 10 миллион на тестовых результатов почему мы приняли решение когда до написать хаев что мы тоже сделали мне вчера тоже очень много лет уже его развиваем первое у нас была потребность в едином управление машинами возможности откатываться к чистым snapshot нам сохранять снапшоты управляется над шутками запускаться там с определенных snapshot of своеобразный такой машин менеджер нам нужен был с чего мы начали вообще развивать наш hive почему нам это так необходимо потому что у нас есть такая специфика наших продуктов что они очень тесно интегрированную в операционную систему там есть различные драйвера и прочей и есть часть сценариев если их там запускать друг за другом на одной машине то они могут выдавать не корректные результаты нам нужно откатываться какому-то предыдущему состоянию машина и все это нужно делать внутри тестов так манипулировать машинами также нам нужна была возможность написания новых машинах сценариев чтобы там на одной машине поставить какой-нибудь сервер на другой агент они как-то между собой общались в процессе теста вы получали тестовый результат автоматически еще одна из причин это нам периодически требуется поддержать запуск тестов на физических машинах каких-то где которые не поддерживают виртуализацию так как наши продукты разрабатываются и под такие платформы например и эльбрус и но такая возможность кастомизировать она в принципе нужно всегда завтра может еще что то появится и нам опять потребуется такая возможность это поддержать также если со сборками я тоже говорила это не зависимость от внешних каких-то инфраструктур то с тестами это еще более критично потому что нас там миллионы тестов и их переделывать если кто-то у нас давно завтра выпустит обновление с отказом от какого-нибудь фито запусков то нам придется переписывать и и конечном этого не хотелось еще одна из причин так как я говорила что когда-то все таки наши команды использовали вообще разную свою всякого рода in fruit до переезда вот вот эту вот man арендную то многие системы они не выдерживали нагрузку которая подавалась нашими тестами на них нас там запускаются тысячи тестов параллель и там в различных системах начинали всплывать всякого рода баги мы начинали обращаться в саппорт и ждать это все очень долго было каких-то фиксов решений и сейчас имеют собственную систему мы сами можем управлять этими рисками решать что нам важно поправить какие сроки в нашей игре и как бы это тоже одно из преимуществ которые мы получили и отказоустойчивых тут все точно так же как с билдами у нас там сотни тестом запускаются вообще тестовых сессий даже запускается белта хсм и там где то потеряем каких-нибудь агентов или что-нибудь мигнет какая-нибудь сеть то и мы потеряем результаты всего билда то для нас очень дорог перезапуск и ожидании его и последнее о чем я расскажу это распределенное хранилище артефактов р на самом на данный момент это на самом деле такая своеобразная прокся с одной стороны которые находятся какой-то букет там может быть все что угодно любые сервера любое хранилище артефактов а с другой стороны это интерфейс и взаимодействие с пользователем они тоже есть самые разные можно там у нас есть иракли утилита через нее обращаться можно там через api как угодно соответственно в чем преимущество мы можем завтра поменять backend убрать оставить другой в тестах сборках в против ничего не надо будет менять для пользователей все останется по-прежнему так почему приняли решение написать р ну потому что это следует из того что я до этого рассказывала это легкое возможность изменить backend не меня интерфейса взаимодействия а второе это использование всех операционных систем когда-то у нас еще даже до написания твой так исторически сложилось что у нас использовались виндовые сам в шар и которыми было сложно пользоваться там из под linux из-под мака сейчас мы пользуемся одинаковые rms под любой операционной системы и разработчикам не надо что то менять для этого как я уже сказала я достаточно обзор на рассказала про нашу инфра это скорее вообще темой отдельного доклада надеюсь дальше это просто поможет понять наши проблемы так вот основная точнее она мне приносит проблемы но погрузиться в то о чем я рассказываю что больше понимать среду в которой мы находимся в нашей компании вот и основной сценарий где вся эта инфа встречается это для разработчика это внесение изменений в код как я уже сказала на strung без development у нас запускай освальдо ционные билды на каждое изменение все перед по зависимости перри собирается и перед тестируется так вот в самом длинном сценарии когда мы вносим изменения какой-то базовым компонентом и перри собираем этот компонент подмножество платформ и перри собираем а перед тестируем все наши продукты под и все ли это платформы он занимает порядка двух с половиной часов и в нем содержится около 150 граммов и так как раз к слову о том почему нам так дорого его в принципе перезапускать и почему это вызывает у разработчиков проблема вот из моего рассказа как раз следует проблемы этого вальта ционного билда первое это большое количество зависимостей а как следствие сборочных и тестовых гробов то есть например если мы к метим в один продукт и он там стабилен 99 процентов что когда таких продуктов становится больше стабильность пропорционально уменьшается вариационного белта да и как раз это и становится для нас проблемой и инфраструктура она как и наше находится в процессе разработки но на самом деле используемую любой другую инфраструктуру она у нас тоже используется в них тоже есть проблемы только преимущества нашей in free что мы сами можем влиять на решение этих проблем это тоже на самом деле оказывает на негативный эффект на эволюционной бетон от этого тоже может падать соответственно это все превращается в просто как нестабильность миграционного да да такого по каким-либо причинам озвученном мною так вот представим сценарий когда разработчик вносит какие-то изменения в своем поваре квесте и у него вальдо ционный build упал сначала разработчика возникает вопрос кто виноват в падении моего же радиационного билда те изменения которые я сейчас сделал своем поле квесте или это что-то сторонняя наведенные какие-то проблемы если это не мое он по разбирался посидел потратил время и вдруг понял что на самом деле виноват не его пури класс не его правки а виноваты какие-то сторонние проблем то возникает логичный вопрос поможет ли сейчас перезапуск сейчас перезапущу этот билд он будет успешным или опять упадет если перезапуск не помог и опять build упал с той же проблемой то когда вообще эта проблема исправит как понять статус еще кто-нибудь знает об этой проблеме кто-то занимается сейчас ее решением или нет мы не хотим разработчиков она эти проблемами рот козам которых он не является поэтому начали думать как вообще помощь нашим разработчикам внутри нашей компании с подобными проблемами начали смотреть на запуске в принципе этих валидации он накупил дав вот на картинке представлен график где по вертикальной оси это длительность этих эволюционных белков по горизонтальной просто астрономическое время зеленым обозначена успешной годы красным неуспешным серые отмененным серым отмененные мы видим что большое количество красных билдов но как понять вот в pull request ах разработчиков они упали потому что разработчик к метил плохой код или они упали потому что у нас короче что-то мигала то продукт какие-нибудь тесты инфра или еще что-то проблематично тогда мы подумали а почему бы не попробуйте не запустить этот валик ционный build просто по мастеру ведь у нас там транг bs development мы в идеальном случае бьет который мы запускаем по мастер он должен быть зеленым всегда мы запустили и увидели что на самом деле это не так у нас есть красные билды есть проблемы и тут мы поняли что сборка по расписание это своеобразное здоровьем и на репы почему потому что в нем можно предотвратить проблемы на ранней стадии то есть мы запускаем бьет по расписанию у нас выявляется какие-то проблемы мы с ними разбираемся и может быть решим эти проблемы даже раньше чем у разработчика на пол request и это стрельнет его легко мониторить но как я уже сказала по мастеру там точно не могут быть виноваты никакие изменения сделаны в паре квестах разработчиков в отличие от когда этим во льду ционные билды падают в пури квестах разработчику то есть мы сразу понимаем если красная значит что-то плохо значит с этим нужно разбираться легко найти ответственных ну собственно по той же причине если build упал значит в этом кто-то виноват и может быть такого чтобы build упала никто не виноват видны какие-то глобальные проблемы на таком валидации он нам голове если там например несколько эволюционных долгов по мастеру падают с одной и той же проблемы нет успешных то скорее всего у нас там вообще что-то сломано и все нужно срочно бежать чинить потому что скорее всего никто комитет не может и для каждого такого падения можно заводить инцидент в рамках которых искать этого самого ответственного о котором я говорила ранее так вот мы внедрили у себя такую практику мы стали запускать валюта ционный бьет по расписанию заводить на каждое падение этого лета ционного белта багу своеобразный инцидент находить ответственного кто виноват в этом и строить различные статистике понимать из-за кого у нас больше всего подают эти во льду ционные билды как с ними работать с этими командами системами как решать эти проблемы систематически как вообще любые другие проблемы в этом билде смотреть этот график и думать как сделать так чтобы эта проблема больше нас не беспокоило больше никогда не воспроизводилась и мы начали так системно работать с этими проблемами и за несколько лет с тех пор как мы начинали это делать это с октября 2019 года до октября 2021 мы улучшили значительное количество успешных вальдо ционных долгов по мастеру если говорить про октябрю 2019 года то в самом начале это было порядка 57 процентов успешных пилотов запущенных по мастеру то есть ну понимаете там либо либо бьет прошел либо упал вероятность почти 50 на 50 и сейчас мы достигли порядка восьмидесяти двух процентов что за эти годы на самом деле достаточно такой большой прогресс но ну тут конечно есть еще к чему стремится но результат налицо но мы начали думать дальше и понимать что дамы как-то системно боремся с проблемами мы их решаем думаем как сделать так чтобы они больше не воспроизводились однако разработчику все-таки когда у него что-то падает на пол и классе от этого легче не становится мы подумали что и разработчику нужно как-то дать обратную связь уведомить разработчика существование проблемы обозначить статус решение этой проблемы ускорить перезапуску сборки если ну она упала не по его вине чтобы он не тратил свое время на разбор проблем в причиной которых он не является таким образом у нас какой-то момент появился робот top мы его назвали хранителем pull request on что он умеет он отслеживает глобальные проблемы что такое глобальные проблемы если валюта ционный бьет падает нескольких разработчиков по одной и той же причине при этом нет вообще никаких успешных билдов то скорее всего комменты в какую-то область невозможны бесполезно перезапускать dota или что-то делать нужно разбираться с проблемой поэтому он ставит такой рацион ебет на паузу сообщает об этом в каждом пурик вести разработчиков вот тут представлена картинка он пишет статус порика есть и разработчика прикладывает венку на инцидент в рамках которого разбирается эта проблема разработчик может зайти в свой пул request увидите что сейчас бил стоит на паузе кто-то занимается проблемой перейти в нее кликнуть перейти в эту багу посмотреть кто занимается когда примерно решится эта проблема когда проблема решается и там закрывается та сборки снимаются с паузой их опять появляется возможность запускать и робот приходят и перезапускает все упавшие билды по этой причине однако существует не дали бы какие-то глобальные проблемы так да все не работает вообще все сломалось и к миссис невозможно вот всякие разные нестабильности там например вот сейчас во льду ционный build упала если его тут же перезапусти тем скорее всего пройдет нестабильности могут быть во многом вмф самих сборках тестах и вот когда у разработчиков пол request и падает вальта ционный build top берет эту ошибку анализируйте и смотрит нет ли у нас уже заведенных инцидентов по этой проблеме если он находит какой-та заведенный инцидент он прямо пишет эту ошибку в полу request непосредственно разработчику под ошибкой пишет что вот смотри уже существует проблема в рамках которые разбираются сад и ошибка это делается для того чтобы разработчик не сидел не разбирался с этой проблемы не тратя на это свое время а увидел что упала точно не из-за него и ставят такие ставят такие билды в очередь на перезапуск почему в очередь потому что например случае глобальных проблем если мы сразу все перезапустим сможет инфраструктуре случится коллапс и начнут выстраиваться очереди поэтому нашего подпишут что вот планомерно вот примерно к такому временем и твой убил перезапустим и разработчики даже иногда не успевает ничего перезапустить руками около шестидесяти девяти процентов упавших билдов по каким-то известным проблемам мы перезапускаем автоматически помимо того что наш робот сообщает о каких-то глобальных проблемах известных проблемах заводят эти проблемы на нестабильность внутри годов пал request of когда например у них нескольких разработчиков встречается одна и та же проблема на паре кого есть он заводит на этой инцидент то наш робот он еще и бот его можно призвать на помощь своем пол request и он отправит запрос на команду которая занимается поддержкой се придет поможет разобраться в проблеме разработчику он умеет запускать какие-то расширенные сборки и тесты прямо в пол request и разработчика и еще имеют ряд мелких возможностей которыми пользуются разработчики но помимо того что тор он каких-то известных и глобальных проблемах сообщать чтобы разработчики не разбирались с не своими проблемами он еще и пишет краткое описание вообще даже всех проблем даже тех которые принесены разработчиками непосредственно в их паре квесте и что самое важное это прикладывается прямую ссылку на артефакты в который нужно смотреть чтобы разобраться с проблемой ну не ходить там по-разному джабал тестом и искать где мне скопировать откуда мне скатерти факт чтобы их посмотреть а он прям дает непосредственно ссылку на эти артефакты еще одна такая маленькая приятность наших роботов это они запускают заек спаренный билды что такое язык спаренные беды это у нас есть бронь в полисе в которой валюта ционный бил действительно 24 часа из 24 часа прошло тут был успешным напоре класс не за completely to which коль ищет все он становится невалидным ебет надо будет перезапустить так вот если мы сделали request пятницу запустили валидации он эй билл он прошел и мы не за completely пол request в тот же день то приходим в понедельник и а билды уже не валидны придется перезапускает так вот наш робот он может ночью подсветится все перезапустить подельник разработчика придет ему будет приятно не надо будет тратить на это время но помимо того что мы как-то информируем пытаемся решать проблему мы все-таки еще дело в механизмы которые позволяют билдом в принципе не падать вот у нас есть механизм флаке тестов которые мы разработали что он делает любой тест можно пометить как флаке можно пометить его руками как флаги и тогда когда запустится валюту ционный пьют и в этом валидации он нам билде упадет тест который помечен как флаги таврида ционный bells равно будет считаться успешно помимо того что это можно делать руками у нас есть еще робот который смотрит в пол request и разработчиков понимает что если на 3 вольта там ну или даже не 3 это настраиваемый параметр на нескольких вальта ционных билдах тест падает по одной и той же проблеме он помечает тест как флаги заводит на это инцидент в рамках которого как команда которая поддерживает этот тест разбирается с этой проблемой дальше когда команда разберется с проблем с проблемой этот инцидент закрывается и тест опять переходят в состояние блокирующего таким образом порядка четырех процентов билдов от всех проходят благодаря игнорированию таких флаке тестов во время запуска билда вот еще одна такая больше и забили ти фича для удобства разработчиков это timeline который у нас есть ауре квестах в этом таймлайне можно посмотреть все запуск эволюционных билдов понять по каким причинам они падали если по каким-то известным проблемам то можно перейти в эти известные проблемы посмотреть что это были за проблем изоляционными долгами как они перри запускались вручную автоматически все это может сделать разработчик чтоб понять почему тот одной паре квест долго заходил например ну и в заключении мне хотелось бы сказать как я уже до этого рассказывала про то что мы многое измеряем приводила различные цифры так вот мы и принимаем какие-то решения на основе цифр в том числе мы научились измерять и удовлетворенность разработчиков что это за метрика такая это количество перезапусков валют ценного бил до последнего апдейта до апдейта пол request a до completo что это значит в идеальном случае в идеальном мире после последнего апдейта пол request a должен пройти один раз вальдо ционный бил быть успешным и парик вес за комп лечиться однако это не всегда так если вдруг мы правда tele pole request потом у нас медитационный билд был красном потом моего просто перезапустили без каких-либо изменений кодов курик вести и он стал зеленым за completely пол request значит скорее всего в первый раз он упал не по нашей вине таким образом мы стали измерять процент пури квестов которые проходят с нулевым количеством ретро эволюционных билдов после последнего апдейта и на данный момент это порядка 91 процент таких pull request of у нас делается в нашу репу с учетом той истории которая рассказывала в принципе это сейчас достаточно неплохой процент но мы не останавливаемся мы будем продолжать работать над нашими инструментами и повышайте удовлетворенность разработчиков в нашей компании всем спасибо за внимание готова ответить на вопросы спасибо лена за очень интересную презентации доклада было очень интересно слушать и на самом деле мы получили очень много вопросов и я даже не знаю с чего начать пожалуй я выберу два и потом мы продолжим обсуждать остальные вопросы в дискуссии первое конечно такое более john doe планируете ли вы планируете ли вы выложите асгор него концов мы думали над этим вопросом планировать нам или не планировать выложить его в pandora's пока мы в ближайшее время мы это делать по крайней мере не планируем потому что в нем есть очень много завязок на нашу внутреннюю специфику которая присуща только наше комп нашей компании однако если там мы начнем работать в эту сторону и как-то разрешать эти зависимости то может быть в будущем я не знаю не исключаю такой возможности что это произойдет но пока на ближайшее время таких планов нет хорошо и другой вопрос как реализовано отказоустойчивость агентов в асгарде мне кажется что это на самом деле тема отдельного доклада как она организована как я уже говорил отказоустойчивость вас гарди мне кажется что я так в двух словах не смогу описать эту схему и рассказать об этом мне кажется что лучше когда-нибудь на конференции придет кто-нибудь в нашей компании расскажет подробнее о том как организована отказоустойчивость агентов асгарде сейчас мне кажется это слишком долго для ответа на вопрос это хорошо так и тут другой такой интересный вопрос есть правильно ли наш слушатель понял что единственный способ жить man репозитории который удалось найти это написать при этом куча дополнительных инструментов все эти дей на самом деле это не единственный способ жить в моно репозитории наверняка можно и найти какие-то решения на рынке чтобы жить в моно репозитории но как я уже рассказала мы писали свои инструменты не только потому что нас моно репозитории а потому что мы решали какие-то свои проблемы специфичные для нашей компании мы хотели какие-то фичи какие-то возможности от этих инструментов получить поэтому мы их начали писать я думаю что если бы мы нашли какие-то аналогичные решения там на рынке среди других компаний то мы бы ими воспользовались скорее всего просто в нашем случае вот у них не было всего того что нам было необходимо то есть принципе да такое случаются когда слишком очень кастомизированные нужды ничего не годится алла так сказать с полки у нас очень много вопросов по этому он я задам последний и потом пойдем дискусы хороша но хорошо да в транг bass очень важно покрытие фичи флагами как получается разбивать все фичи так что было бы легко зак вот фича флагом до вопрос такой сложные неожиданный на самом деле для меня получается получается не все фичи закрепите фичи флагами я могу так сказать то что получается мы делаем там смотрим это на самом деле очень сильно зависит от того что вы разрабатываете как вы будете покрывать это фичи флагами самое главное мне кажется стран без девелопменте это то что мы все права лидируем все чтобы коми тем как можно больше право лидируем каких-то базовых сценариев чтобы вбить в репозитории какой то более менее рабочий компоненты или продукт понятно что мы не покроем там сто процентов покрытия но ряд вещей мы покроем проверим ну как бы фичи флаги да они есть и какие-то фичи проверяются там во время этой валидации вот так хорошо спасибо тебе аллен давай продолжим нашу такое замечательное дискуссию в дискуссии и я задам еще пару вопросов и мы очень ждем всех вас там там же и с вашими вопросами до встречи там"
}