{
  "video_id": "ojr3d52lmow",
  "channel": "DevOops_conf",
  "title": "Лев Алимов — SRE. Неочевидные способы прострелить себе колено в попытках сделать приложение надежнее",
  "views": 558,
  "duration": 2478,
  "published": "2024-08-09T05:00:07-07:00",
  "text": "Всем привет меня зовут Антон я когда-то был ли дом команды сырье в Тинькофф Сейчас я просто разработчик и будучи разработчиком я не раз стрелял себе в колено в попытке достичь стабильности и Лев наш спикер подготовил отличный набор материалов как прострелить себе колено и как это возможно не допустить Всем привет для начала тоже хочу сказать пару слов о себе в 2013 году я был фрезеровщиком и хотел попасть войти потому что мне нравилось всем этим заниматься тогда не было большого количества разнообразных специализаций войти как не было и денег естественно я тогда хотел сделать обоснованный какой-то разумный выбор изучил рынок изучил перспективы и понял что наиболее перспективным направлением будет программирование на Java я вложился разными книжками курсами начал их изучать изучал объектно ориентированное программирование на примере Java изучал алгоритмы и через несколько недель это активности я решил что хочу стать системным администратором за 9 лет Я прошел достаточно долгий и интересный путь от никитиков городской поликлиники до ведущего сайта relobility инженеров платежной инфраструктуре Тинькофф естественно За это время я чинил штуки и разбирал сбои на самых разных уровнях абстракции начиная от физики в кабельных сетях и заканчивая сложными интересными кейсами больших распределенных системах еще я снимаю метрики не только со своих сервисов но и самого себя слежу за показателями чтобы не допускать бой на этой продакшн системе чем буду сегодня рассказывать как разработчики Мы обычно пишем свои приложения пытаемся реализовать какие-то внешние взаимодействия и в какой-то момент мы понимаем что внешние системы нет сто процентных случаях отдают нам ответы Возможно где-то пакетик потерялся запросом может быть сама система приуныла в общем факт в том что внешняя система может нам не ответить наше предложение из-за этого может упасть естественно нам хочется как-то подложить соломку сделать какие-то механизмы для того чтобы наш сервис был отказа устойчиво а есть общепринятые какие-то книжные паттерны они описаны в сыре сырье Book от Гугла про них рассказывают много на YouTube паттерны есть можно брать и использовать Но что если все эти паттерны не так полезны И на самом деле при их реализации сбой можно только усугубить Сегодня я расскажу про то что у паттерна Для обеспечения отказа устойчивости есть собственные подводные камни может быть в каких-то из тех кейсов которые вам расскажу вы узнаете свои системы возможно решите что что-то пора менять до того как сбой на продакшене у вас случился может быть у вас еще нет механизмов отказа устойчивости и тогда вы научитесь учитывать возможные Грабли в работе этих механизмов и сразу сделаете все правильно мы рассмотрим три кейса вымышленных но основанных на реальных событиях на примере которых я покажу как можно разводить Production с помощью использования механизмов отказа устойчивости еще сильнее чем если бы этих механизмов там не было вообще а Первый кейс называется ветра и антиладдай у нас есть какое-то распределенное приложение как у обычного распределенного приложения У нас есть какой-то API и API принимает на себя абсолютно все клиентские запросы какие-то запросыпи состояние обработать самостоятельно Но все-таки по большей части запросов он ходит в какие-то внешние сторонние системы получает от них данные и потом проксирует эти ответы клиенту И как мы уже говорили внешняя система может отказать поэтому ответа Может не получить поэтому он свои запросы повторяет API это Junior Junior берет все самые мелкие задачи с доски пытается их решить с какой-то частью этих задач он способен справиться самостоятельно но чаще всего все-таки задает вопросы тимиряду и поскольку Junior это Junior он не всегда понимает с первого раза и поэтому постоянно переспрашивает а также наши распределенной системе есть сервис который принимает нагрузку от API и отвечает ему какими-то данными сервиса обрабатывает основную часть запросов он состоянии делать это быстро у него Возможно есть какие-то горячие из которых он быстро вытягивает ответы но иногда сервису нужно ответить подробнее детальнее детальных данных у него нет и тогда этот сервис идет за деталями в какую-то внешнюю систему этот сервис у нас Team elit он отвечает на вопросы джинов имеет достаточно широкий контекст компании в принципе способен ответить практически на любые вопросы по большому списку тем но тем не менее тимлид у нас знает в деталях не все и поэтому часто ему приходится обращаться к более квалифицированным ребятам в команде тем Лид хочет ходит к синьорам чтобы ему что-то объяснили и естественно у нас есть какой-то третий сервис высоко нагруженный это может быть какая-то база данных может быть какой-то сервис который просто обрабатывает молотят тяжелые задачи он способен отвечать нашему сервису который стоит посередине способен отвечать максимально Подробно как правило этот сервис занят очень тяжелыми задачами каждый запрос вызывает ощутимую на него нагрузку и поэтому наша вода та сторожа запасы по ресурсам нету Как вы могли уже догадаться это у нас сеньор сеньор знает все в деталях абсолютно про все системы в компании может разобрать любую распределенную систему по винтикам и выложить в деталях как она работает но проблема в том что знания у синьора в голове представляют обычно не прям готовые ответы А это скорее семи линкина какие-то веки на какие-то репозитории в которых сеньор может подгрузить себе контекст и тогда уже выдать ответ на тот вопрос который он получил естественно сеньор обычно занят объемными задачами у него не прилетает по 10 задач на курджина ему обычно дают одну задачу она на несколько недель а то и месяцев Это обычно большой проект который затрагивает большое количество сервисов и людей и как и полагается синьору наш сеньор почти сгорел то есть запасы по ресурсов у него нет если начать сеньора грузить большим объемом вопросов на которые нужны детализированные ответы наш сеньор скорее всего устанет и Возможно даже уволиться Это был обычный вторник задачи появлялись на доске задачи появлялись на доске в обычном режиме джунио брал их в работу поясню Что значит в обычном режиме у нас как раньше было по 10 задач так и сейчас у нас по 10 задач совершенно обычный объем на есть нюанс недавно бизнес захотел новую фичу и для Джана Это означает что все задачи новые то есть Junior не в состоянии выполнить те задачи которые приходят в том объеме без каких-то дополнительных вопросов адрес какое-то время работает у него все хорошо до тех пор пока что-то не случается предположим что у нашего сервиса забились какие-нибудь Тулы может быть на него просто прилетела слишком много вопросов но факт что сервис отвечать перестал Junior начинает паниковать он задает свои вопросы тимлиду тимлид на них не отвечает у Джуниора просаживается перформанс он привык что к середине дня сделано уже 5 задач А сейчас у него висит одна Он задал по ней вопросы ответов не получил естественно наш айпер пытается повторять запросы потому что он получает таймауты он кидает все новые новые ритраи Junior продолжает спрашивать у своего тимлида А когда он получит ответы на свои вопросы Junior паникует у него синдром самозванца Он боится что его уволят за такой перформанс у Тим Лида тем временем Копится счетчик ходячих сообщений в личке от Жена там добавляется все больше и больше новых вопросов по новым задачам там добавляются уточняющие вот эти вот вопросы от журнапа старым вопросам и так продолжается какое-то время осенью тем временем пилится вытаскивал ничего не подозревает про то что там какая-то новая фича про то что там Junior паникует про то что Тим Лида огромное количество задач сеньор пилит И пилить свою задачу его все устраивает у него все классно потоки заканчивается созвон Темп читает быстро читает весь поток сознания пытается вникнуть вопросы понимает что самостоятельно на эти вопросы он ответить жену не может он понимает что ему нужны детали и естественно сейчас что-то произойдет сеньором Тим Лид перенаправляет всю ту очередь вопросов от Джуна нашему сеньору сеньор читает вопросы понимает что прямо сейчас он ответит на эти вопросы не может ему надо залезть вход ему надо почитать веке Ну это достаточно долго он берет пару вопросов и тех кто которая прилетели в первую очередь пытается как-то на них отвечать а за то время пока он отвечал ему в личку навалилось еще сотня новых сообщений сеньор устал идет пить чай а так продолжается какое-то время тимлид продолжает закидывать свои новые вопросы нашему сеньору накидывает накидывается понимает что Синьор не отвечает И тут им редактор включается режим Чайка менеджера по всем вопросам которые он уже закинул сеньора он пытается получить какие-то ответы и начинается спрашивать что там по задачам когда ты ответишь на эти вопросы сориентируй по срокам и так далее и тому подобное и получается что счетчик входящих у синьора начинает расти еще быстрее чем прибывают задачи для Джуна а в какой-то момент сеньор допивает свой чай возвращается открывает личку видит четырехзначное число сообщений и понимает что не очень-то он хотел возвращаться с перерыва естественно идет отдыхать и так продолжается какое-то время сеньор возвращается чаепитие смотрит там еще больше навалило снова уходит такой получается своего рода crashback с этим надо что-то делать естественно приняли решение попросить бизнес прекратить пушить нас по новой фиче откатили релиз Джон стал получать те задачи с которыми он состоянии справиться либо сам либо с помощью не привлекая нашего сеньора мы перестали получать новые запросы для сеньора но Мы помним что наш Тибет хороший тимлид и он запомнил весь тот поток сознания жена и он продолжает пушить нашего Бедного сеньора всеми теми вопросами которые к нему когда-то прилетели сеньор то есть наш высоконагруженный сервис падает потом снова поднимается видит пачку запросов снова падает и так снова продолжается какое-то время Как выходить из этой ситуации абсолютно непонятно В итоге волевым решением команда отправляет нашего сеньора в отпуск сеньор уходит в отпуск его больше нет запросы обрабатывать Он больше не может Тим ли Да мы заставили забыть все эти вопросы по которым он пошел нашего сеньора более того джуну запретили брать Какие бы то ни было новые задачи с доски То есть получается что вся распределенная система была полностью остановлена и очень постепенно мы начали Пускать на нее трафик Сначала один процент потом пять процентов а потом 10-15 и так далее до 100 процентов чтобы система вышла на свою нормальную обычную производительность надо понимать что каждый это итерация нас сопровождалась длительным чтением логов анализам графиков чтобы точно не уронить распределенную систему еще разок это продолжалось несколько часов восстановления было очень болезненным и получается что мы-то хотели Как лучше у нас был какой-то сервис который мы хотели сделать отказоустойчивым мы сделали на нем ретрая чтобы при потере запросов мы могли повторить запрос и в конечном итоге ответ нашему клиенту отдать ошиблись мы в том что ретрай сделали и на API и на сервисе и вчера был хороший доклад про среа и перформанс и там приводил Гриша прям табличку в которой посчитал насколько сильно будет расти количество запросов высоко нагруженной системе если она стоит в конце цепочки и в нашем случае получается что API сделал запрос потом на него три литра потом на каждый из этих четырех запросов наш сервис сделал еще по запросу и по три Петра и того у нас получается что нагрузка на высокой нагруженный сервис выросла 16 раз и Как не допустить выгорания синьора Как не допустить падения вот этого высоко нагруженного сервиса как не завалить его retrime случае если что-то пойдет не так в первую очередь если мы та самая система которая обращается куда-то вовне мы можем реализовать такой патент который называется surcot Breaker смысл этого паттерна в том что мы наблюдаем за тем насколько успешно обрабатываются наши запросы внешней системе при достижении определенного количества по тайм-аутам или иным ошибкам сторонние системы серкан брейкер просто закрывается он перестает отправлять Какие бы то ни было запросы дальше И периодически пингуют стороннюю систему на предмет того жила она или нет пока она не жила мы отвечаем нашим клиентам сразу что мы не можем ответить на этот запрос или возможно направляем трафик на какой-то fallback систему как только наш сторонний сервис начал отвечать мы снова запускаем на него поток правят трафика И тем самым даем ему возможность подняться Под этой нагрузкой то есть не генерируем вот эту лавину из ретраев вторая история которую можно поприменять если мы Клиент по отношению к внешней системе это экспонент шелбок бэков Смысл в том что на каждой последующий тайм-аут мы наращиваем время до следующего ретрая на какую-то величину и Хорошая идея будет умножить это на какой-то Случайный коэффициент чтобы еще более равномерно распределить нагрузку на стороннюю систему Таким образом мы не сокращаем общее количество ретраев но при этом растягиваем их во времени И тем самым даем нашей стороне системе возможность подниматься под вот этой вот нагрузкой Если же мы тот самый нагруженный сервис и на внешние системы которые к нам входят мы никак повлиять не можем на своей стороне можно реализовать при достижении определенного порога мы можем снизить либо качество возвращаемых ответов их количество то есть банальные какие-то ретримиты накрутить и это нам уже поможет мы будем откидывать какую-то часть запросов и при этом под нагрузкой сами падать не будет да клиенты будут пострадывать но в целом это лучше чем прекратить полностью обслуживание и еще одна штука про которую можно подумать она на самом деле не самая простая в реализации но тем не менее можно реализовать в нашей распределенной системе такой механизм который даст понять высоко нагруженному сервису на какой секунде он пытается начать обработать запрос и если мы знаем что наш клиент забудет о своем запросе через 10 секунд то нам на высокой нагруженной системе Абсолютно нет никакого смысла Брать этот запрос в работу на 12 секунде в целом штука реализуемая но повторюсь не очень простая очень сильно зависит от контекста как это все таки делать кейс номер два Называется он слишком умная балансировка в этом кейсе У нас есть клиент на самом деле неважно какой это может быть либо браузер либо это может быть какой-то Android приложение клиентам также может выступать какой-то опять же сервис большой распределенный раскидистой системе важно то что между нашим клиентом и бэкентом стоят какие-то балансировщики даже не какие-то Вполне себе конкретные в данном кейсе были балансировщики распространенные кого-нибудь в качестве балансировщиков используется руки часто используется в общем есть балансировщики инженекс и понятно что за инженексом есть какие-то бэкенды проблема лонжеликса в том что его конфликт статичен то есть для того чтобы исправить чтобы добавить бэкенды или удалить какие-то бэконды нам надо прям натурально зайти на сервер открыть ngx.com или там неважно текстовый конфиг Мы открываем добавляем туда строчку с новыми бэкендами удаляемся строчки со старыми бекондами и запускаем перезагрузку сервисы чтобы он этот конфликт подхватил хорошие идеи в такой вот истории с ручным приводом было бы реализовать какой-то механизм который нам позволит новые бэкенды обнаруживать А старые удалять то есть накрутить какой-то сервис Discovery А когда у нас есть сервис Discovery разумно будет не только выводить бэнды по каким-то определенным спискам но и пытаться понять работают эти бэнды вообще или нет отвечают они или нет то есть давать какой-то механизм халчек Мы тоже подумали что идеи это отличные поэтому мы поставили Консул с помощью которого мы Discovery все наши бэкенды сделали сервис Discovery накрутили Hells чеки А для того чтобы вся эта машинерия могла как-то доставляться до инженексов и новые бэкенды для инженексов могли быть обнаружены мы поставили Консул template у нас были виртуалки и кому-то может показаться что им Эта история не актуальна потому что их сервисы крутятся в Интернете Но на самом деле там примерно Такая же история только вместо консула в качестве источника правды об аккантах выступает cubernetis его доме и рендерингом конфига занимается контроллер и генерит он не конфигурацию на виртуалке а конфликт который монтируется в под воркерами естественно контроллер точно так же как и делает И вообще вся эта штука работает идеально добавляем бэкенды они продвижение цены убираем они оттуда убираются если что-то сломалось перестает направлять трафик на больные ноды направляет его только на здоровые Все работает вообще замечательно мы получили возможность сделать релизы наши разработчики могут релизиться уже без необходимости присутствия инженера которые залезет на серверы подправит конфиг все классно стримы обновляются Крутится вертится замечательно до одного момента что-то случилось Что случилось Не так уж важно Возможно у нас стало побольше клиентов возможно те же самые клиенты стали задавать стали отправлять больше вопросов Возможно это были те же самые запросы то же самое количество было то же самое количество запросов но просто сами запросы стали тяжелее как предыдущем кейсе а возможно дата-центр в котором была одна из наших реплик сгорел Всякое бывает что именно случилось нам здесь не важно Важно то что в какой-то определенный момент нагрузка на одну из наших реплик вырастает и вырастает она настолько сильно что реплика нашего бэкенда падает на Консул Молодец Консул Консул обнаружил что реплика упала Консул templace ходил в Консул Перри рендерил конфиг дернул Джин X трафик пошел на здоровый бэкент вроде бы все хорошо Но наш здоровый бренд принял ту нагрузку от которой упала предыдущая реплика и глупо было бы ожидать что наша следующая реплика не упала бы следующая реплика тоже упала тем временем первая попавшие реплика поднялась поднялась концу молодец и так продолжалось какое-то время в целом все хорошо у нас реплики принимают запросы падают эти запросы отправляются на следующие реплики клиенты запросы повторяются с точки зрения клиента ничего не поменялось по большому счету то есть он как получал ответы от сервиса так и продолжил получать Единственное что Ну может быть чуть подольше это стало происходить но клиенту неважно в целом реально все хорошо ничего не сломалось сервис работает продолжает выполнять свой а потом все взрывается балансер лежит не поднимается клиенты тайм-аутят поддержку приложения просто рвет на части От количества входящих вопросов от пользователей стали разбираться что же пошло не так все же работала работала Работала а потом хрена ты взорвалась в первую очередь мы увидели экспоненциальный рост нагрузки то есть количество запросов у нас выросло и выросло оно по экспоненте а потом Мы заметили что количество коннектов на ногах же нексон тоже выросла и что было Странно что оно выросло намного больше чем выросло количество запросов Ну и понятно что наш бык сложился под нагрузкой в 10 В 10 раз превышающий обычный Flow Да у нас здесь опять случился случились retrike у нас это была сложная распределенная система где количество нагрузки выросло лавинообразно по всей цепочке сервисов это в целом объясняет экспоненциальный рост нагрузки по запросам но не объясняет Почему росли коннекты на инженексе и росли они больше чем намного сильнее чем росли росло количество запросов и не объясняет Почему в конечном итоге инженекс хлопнулся но все кто пользуется наверное знают что он легковесный быстрый и сломать Engine X Но это надо очень сильно постараться чтобы понять почему так произошло надо вспомнить как вообще работает события reloado в инженексе и как она дружит с механизмом сервис Discovery я напомню что конфигу и жены со статичный это где-то лежит Файлик его надо исправить а потом вызвать события reload то есть станет направлять трафик на новые бэкенды уберет со старых только после того как произойдет релот работает следующим образом мы запускаем новый процесс Engine X а он начинает принимать на себя все новые соединения старый процесс должен завершиться Но есть один нюанс старый процесс не завершается мгновенно какое-то время старый процесс продолжает работать и ждет что завершатся все ранее начатые запросы и Казалось бы подняли новую новые процессы он принял на себя все новые коннекты Почему все сломалось то старый ушел Все должно же быть хорошо Консул у нас видел что упал чек на бэкенде далее Консул template ходит в Консул смотрит Что изменилось конфигурация по списку серверов в стриме и рендерит новый конфиг далее Консул template отправляет reload на наш Engine X старый worker остается висеть какое-то время пока обрабатывает коннекты а поскольку реплика упала старые worker ngx ждет пока каждый из тех и заранее отправленных запросов завершится и завершится он должен был не мгновенный там что-то порядка 10 секунд а в единицу времени при высокой нагрузке количество вот этих вот падающих и возвращающихся реплик в минуту шло на десятки если не на сотни новые воркеры продолжают плодиться старые продолжают ожидать пока у них закончится тайм-ауты и в какой-то момент у нас просто не остаётся новых ресурсов в какой-то момент когда ресурсы кончились старые воркеры ждут тайм-аута новым запуститься Просто негде потому что нету ни памяти ни доступных сетевых соединений на тех машинах на которых они крутятся и ngnx целиком перестает отвечать наверное Нет ничего хуже для сервиса чем переставшие отвечать балансировщик сервис Pal поднимались в этом случае точно так же как и в предыдущем кейсе сначала полностью остановили трафик потом перезапустили все инженексы прибили абсолютно все воркеры и стали запускать трафик очень постепенно понемножку с контролем графиков и так далее Это было долго Вот и в итоге что получается получается что мы снова хотели Как лучше мы сделали сервис Discovery для того чтобы обнаруживать сбойные реплики для того чтобы наши клиенты в принципе этих событий не замечали но в конечном итоге сам механизм сервис Discovery наплодил нам настолько много событий reload of что мы забили все наши ресурсы на нотах балансировщиком и в итоге подняться не смогли без полной остановки сервисов Что с этим можно сделать если Вы только начинаете запускать балансировку на своем проекте Если у вас есть возможность быстро и достаточно легко поменять балансировщик имеет смысл обратить свой взгляд на решение которое не требуют сервиса То есть те решения которые состоянии менять конфиг вантами часть из них перечислены на слайде это вой коммерческий режим плюс так умеет ухо прокси есть и все бы было хорошо Если бы Engine X у нас не работали в давно устоявшихся системах где Нельзя просто взять и выкинуть балансер если балансер выкинуть нельзя есть для инженекса опций активные Health чеки GX + умеет так из коробки но вряд ли у вас плюс скорее всего это комьюнити версия для комьюнити версия тоже есть решение там есть модуль позволяющий реализовать эти активные холстчеки и этот модуль позволяет не перезагружать конфиг каждый раз а просто замечать что один из существующих фанфитов стримов перестал отвечать и не направлять на него трафик количество рилотов таким образом из-за недоступных реплик сводится к нулю естественно если вы сохраняете вот историю с Сервис Discovery loademy имеет смысл мониторить количество событий этих релогов если их единицу времени стало слишком много то в принципе можно успеть среагировать как я говорил в этом кейсе клиенты ничего не замечали достаточно длительное время можно было бы имея мониторинг среагировать и что-то сделать до того как абсолютно все схлопнулось Ну естественно имеет смысл накручивать какие-то ограничения по количеству риотов единицу времени да клиенты будут получать побольше ошибок но при этом опять же ваш сервис не накроется чем-нибудь очень неприличным а третий кейс называется доверительные резервирование и это кейс на самом деле жутко собирательный каждый случай сбоев из ошибок в резервировании они достаточно скучные то есть что может быть скучнее покончавшегося место на диске Наверное каждый из вас это проходил и Ничего необычного в этом нет но каждый отдельный случай ошибки резервирования масштабирования он встречается настолько часто что хочется про это поговорить ошибки на самом деле незначительные иногда смотришь на такие сбои когда все взорвалось по причине ошибок резервированию думаешь блин ребят как так было же очевидно что вот это вот работать не должно вы не сможете отмасштабироваться тем не менее такие кейсы происходят и вызывают достаточно серьезные последствия на работоспособность сервиса мы говорили про то что в какой-то момент разработчик приложения понимаешь что сервисы не идеальны они могут перестать отвечать а в какой-то момент разработчик настолько при исполняется что понимает что его то сервис тоже может оказаться не идеальным и логичным было бы решение добавить еще одну реплику сервиса на случай если одна реплика все-таки перестанет справляться разработчик добавляет побольше реплик и верит в то что Когда что-то произойдет оставшиеся реплики будут состояния работать запросы от клиентов и как правило фейлловер роль при масштабировании она теряется постепенно сервисы оказываются переутилизированы и в случае если у нас две реплики Если каждый из них утилизировано более на 50 процентов более чем на 50 процентов мы не оставляем себе шансов справиться с нагрузкой если одна из реплик у нас упадет что-то случается приходит больше клиентов либо те же клиенты стали задавать более сложные вопросы и в какой-то момент одна из наших реплик падает утилизация у нас больше 50 процентов естественно наша вторая реплика не в состоянии обработать больше 100 процентов нагрузки и вторая реплика тоже падает вторая падает не вывозит нагрузку Но мы же умные Мы подумали про то что мы в состоянии масштабироваться мы можем поднять еще несколько реплик и обработать этот сбой так чтобы клиенты ничего не заметно не заметили пробуем масштабироваться и Здесь начинается интересная часть оказалось что мы не в состоянии поднять еще одну реплику старые реплики лежат новые не поднимаются сервис заглох клиенты мучают поддержку Почему так может произойти если мы работаем в Облаке мы можем профакапить наблюдение за квотами в какой-то момент мы пытаемся отмасштабироваться нам Облачный сервис говорит Извини у нас нет квот и тогда надо создать заявку на пол на расширение квот и как правило это процесс не настолько быстрый как попытка поднять еще один подкурнетисе Возможно мы работаем не в Облаке А на своих дата-центрах и тогда когда у нас кончились ресурсы они у нас кончились физические привести закупить привести поставить новый сервер настроить его чтобы он начал принимать нагрузки но Сами понимаете что это неделя то и месяца также у нас могут закончиться скрытые ресурсы даже если у нас был копать эти планинг мы наблюдали за пропускной способностью сети за пропускной способности дисков наблюдали за тем насколько мы утилизируем сепию памяти так далее закупали новое железо своевременно мы могли забыть за купить кое-что еще из неочевидных моментов их на самом деле достаточно много и зависит от того как Какое у вас приложение Но вот самое распространенное это могут кончиться Канны Connect и на тех железных нотах на которых у вас приложение работает коннекты настраиваются для юнитов они настраиваются на систему целиком в одном месте мы можем поправить в другом месте мы можем забыть естественно наличие свободных коннектов мониторит мало кто и в какой-то момент может оказаться Так что наш сервис не в состоянии принять и обработать новые запросы потому что мы исчерпали доступные соединения бывают ограничения на платформе например в кубернете администратор Может залимитировать вам количество поводов которые вы можете запустить оказывается переутилизирован вы за этим не следите снова не можете поднять ресурсы если у вас какая-нибудь бюрократизированная организация Понятно Что добавить немножечко больше подав на импс это не быстро бывают также ограничения на стороне самого приложения одна из самых распространенных штук это соединение базе данных допустим у вас работает 5 реплик приложение для простоты скажем что 10 приложения у каждой реплики приложения есть по 10 трендов каждый из этих трендов пытается обрабатывать параллельно в каждом из трудов открывает одно соединение базе данных на базе данных ограничения 100 и 11 реплику вы уже поднять не можете вернее поднять Вы сможете работать она не будет потому что она просто не подключиться к базе данных и еще из неочевидных вещей которые зачастую даже очень сложно за мониторить И как-то вообще понять что там происходит Это ресурсы на гипервизоре сервис на виртуалках виртуалки выделено какое-то определенное количество CPU вам кажется что ресурсов у вас достаточно а классическая история что на гипервидере происходит лютейший оверкамид и в момент когда все ваши реплики пытаются получить доступ к процессорному времени им Физически не хватает ресурсов А по графикам кажется что ресурсы еще есть и получается что хотели там И как лучше мы заложили масштабирование мы заложились на то что если она что-то произойдет мы сможем добавить несколько реплик но следили не совсем и несколько реплик мы добавить не смогли и процесс добавления и масштабирование он оказался очень трудоемким и затянул нам в итоге время восстановления что с этим делать Ну понятно что полезно следить за утилизацией не терять фэйловер роли масштабирования следить за тем чтобы у вас на всех репликах вашего сервиса было достаточное количество ресурсов чтобы они могли принять нагрузку от попавших реплик имеет смысл мониторить те самые скрытые ресурсы выводить аллерты показывать все это на графиках общем понимать насколько вообще вы способны масштабироваться и естественно нельзя полностью доверяться тому что вы можете поднять еще одну реплику важно оценивать насколько много реплик Вы можете поднять насколько сильно вырастет производительность вашей системы потому что зачастую она не Линейная То есть когда вы поднимаете X2 реплик производительность растет не двукратно то есть важно понимать насколько много реплика в состоянии поднять и насколько хорошо они справляются с нагрузкой может так получиться что в какой-то момент вы словите де блок на какой-то общий для всех реплик ресурс и окажется что есть какой-то предел по вашему масштабированию абсолютно неочевидный и по итогу моего доклада хочу сказать следующее что что угодно может сломаться понятно что сломаться может любая внешняя система понятно что сломаться может какой-нибудь дата центр могут может потеряться сетевая связанность и так далее и тому подобное но сломаться также может и вашей реализации механизмы отказа устойчивости и поэтому важно свои реализации отказа устойчивости проверять на то насколько они действительно хорошо будут работать в условиях сбоя Потому что когда мы первично реализовали и посмотрели на нормальные нагрузки нам кажется что все работает а момент когда происходит трафику ваши механизмы внезапно начинают отказывать и наоборот усугублять сбой и иногда бывает лучше если бы этих механизмов не было вообще чем была реализация именно в том виде в котором она есть сейчас на этом У меня все спасибо за внимание задавайте свои вопросы Спасибо большое Лев отличный доклад Очень интересно очень весело самую замечательные мемы и надо бы Льву потратить наше время на то чтобы ответить на ваши вопросы Кто не хочет задать вопрос или мы будем задавать вопросы чата на Привет Меня зовут раздетский Владимир компания магнит у тебя на сайте было описание того что нужно гонять тесты А расскажи пожалуйста как вы их готовить может быть у вас есть какая-то отдельная команда которая все это дело пишет Потому что грубо говоря возьмем какую-то фишку стрелять ее корень Ну такое себе нужно проверять гораздо больше а Да можно проверять гораздо больше на самом деле у нас есть много команд они разные и абсолютно разные люди занимаются вот этими нагрузочными тестирования есть ребята которые отдали эту обязанность в qa то есть тестировщики на на стадии тестирования проверяют насколько каждая ручка способна переварить нагрузку нагрузку но при этом сама масштабирование они не тестируют и скорее у нас чаще с задачей вот этого тестирования в условиях сбоя справляется House engineering раз активно развивается практически по хаос Инжиниринг я понял да спасибо у меня вот корень вопросы был в том Кто готовит эти тесты все спасибо ответ в случае с хаосом который это среди инженеры в случае с тестированием просто опишки на производительность каждой конкретной ручки Все спасибо огромное самом деле немножко дополнить все чуть веселее У нас есть сервис который делает обстрел и там закладывается как раз таки разное количество сценариев под разные методы того что вам необходимо искать то бишь там не теряется в корень там стреляется весь сервис по каким-то сценариям и тестируется сложный сценарии который много шаговый где На протяжении какого-то промежутка времени нам необходимо ситуацию пользовательскую ещё вопросы лично Да приветствует Еще раз спасибо за доклад Алексей Хоум банк Ну возможно вопрос с легкостью потому что Вы только что пришел в голову когда микрофон попросил Можно ли ориентироваться на результаты нагрузочного тестирования что называется До убийства не в контексте конкретные ручки а в контексте общего количества вызовов и насколько это будет релевантно при масштабировании это будет релевантно как минимум Чтобы понимать запас по производительности сервиса потому что может оказаться Так что вы упретесь не в ограничение по ресурсам то есть не все не в память а возможно в какой-то внутренний механизм внутри вашего приложения опять же количество трейдов какие-то распределенные блокировки возможно блокировки внутри вашего приложения в контексте масштабирования не очень релевантно но всегда полезно понимать насколько ваш сервис или ваша система распределенная способна в принципе справляться с нагрузками какой предел Какой потолок А если вы не знаете своего потолка вы обычно в условиях действуете Слепое Да понятно просто в контексте конкретной ручки готовить сценарий это достаточно тяжело особенно по чуть-чуть что команда нагрузочного тестирования все-таки иногда отдельная команда им самим нужно время на подготовку таких тестов Да вот а взять и задать сервис каким-то конкретным Ну грубо говоря одним вызовом Да и посмотреть сколько он выдержит да э это хотя бы ну помогло бы самими своей инженером понять его пределы Да при планировании он такой частью попробую его оформить более простыми словами У тебя был сайт где ты говорил про джинсы где было экспоненциальный рост нагрузки Вот но говорится что формула А в степени T и получается что у нас возможно где-то ошибся то есть на графике мы видели такую такую фигуру экспоненту Вот и собственно Исходя из этого я сказал что рост был экспоненциальный отлично сразу Хороший вопрос Следующий разве недостаточно поставить лимитрия на запрос и тайм-аут если система падает мы немного побьемся перекроем канал если единичный случай работать свою если немного побьемся перекроем канал это Breaker же есть такой фактор просто зачастую он не применяется и делают тупую типа rifi там три раза ретрай 5 раз в конечном итоге в большой цепочке вызовов это вырастает просто жутчайшие нагрузки на последние высоко нагруженной системе много говорили про масштабирование и у меня Вопросик Как она у вас устроено Ну понятное дело что вряд ли какой-то специально обученные инженеры идет и меняет количество там сервиса наверняка используется Какой может быть и может быть кеда связке пожалуйста что у вас на виртуалках У нас сейчас это делают инженеры она в губернатисе есть именно VP Вертикаль именно вида не горизонтальная вертикальная масштабирование А почему так потому что святые в текущих реализациях cubernatis у вас будет рестарт на этот вопрос кстати ребята когда рассказывают про нашу платформу они отвечают я сейчас подробно не готов ответить хорошо спасибо большое В разработке и горизонтальное масштабирование на в первую очередь решили взять вертикальные его довести до идеала а потом взяться уже за следующую операцию Я понял Спасибо огромное за ответ кто-нибудь еще вопросы Мы готовы отвечать на ваши вопросы давайте наверное тогда пойдем в дискуссию и возможно там еще кто-нибудь что-нибудь вспомнил что хотел бы спросить Да мы будем находиться отвечает на ваш вопрос Спасибо большое"
}