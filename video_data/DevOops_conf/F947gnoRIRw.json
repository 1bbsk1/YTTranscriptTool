{
  "video_id": "F947gnoRIRw",
  "channel": "DevOops_conf",
  "title": "Григорий Кошелев — SLA — друг или враг разработчика?",
  "views": 156,
  "duration": 3717,
  "published": "2022-07-06T04:09:39-07:00",
  "text": "Всем привет Меня зовут Саша и сегодня я буду помогать григорию кошелеву рассказывать классные вещи Гриш Привет Привет Расскажи пожалуйста немного о себе где работаешь Чем занимаешься Я работаю в контуре уже наверное последние целых семь лет занимался там очень разным вот последние несколько лет занимаюсь инфраструктурой инфраструктура это про всякие разные инструменты и сервисы которые предоставляются другим командам То есть мне пользователи это не клиенты компании какие-то физические физические или юридические лица А это получается другие разработчики То есть получается делаешь инструменты для разработчиков Да если быть конкретным то мы делаем инструменты для телеметрии То есть это все что нацелено на обзорубилете систем это трассировки метрики логи там другие виды телеметрии там допустим связанные там сервис Discovery еще какие-то вещи то есть очень много таких вот вещей которые строятся вокруг некоторых событий из пользовательских приложений пользовательских Я здесь подразумеваю как раз других разработчиков то есть наверняка тебе знакомы проблемы и разработчиков и связанные с эксплуатацией Ну да то есть у меня вот грубо говоря там сидят рядом коллеги которые как раз занимаются там вопросами качества и так команда называется кутим то ку это как раз Quality Вот и как раз они изучают в том числе по снортомы которые там заполняют команды и там тоже какие-то практики некоторые формулируют ему потом в том числе в телеметрии это реализуем вот один из примеров такой Это был сервис центре то есть ребята проанализировали факапы поняли что не хватает инструмента для трекинга новых ошибок мы соответственно со своей стороны такой инструмент предоставили и таким образом там телеметрии из приложений которая идёт она автоматически попала в центре люди получили сразу возможность настроить альянтинг по своим правилам своим каким-то нужным Вот и соответственно тоже мы здорово довольно прокачали именно качество именно сервисов в части именно реагирования на какие-то ошибки звучит довольно круто причем Я так понимаю разработчики сервисов они очень много в этом участвовали это самую сложилось да то есть это полностью прозрачно пользователи с точки зрения доставки телеметрии потому что у нас она все через единую шину отправляется вот а уже именно конкретно настройки проектов настройка там центре алиартов и правил это уже естественно там отвечает за эта команда Они уже сами определяют как это им надо сделать как это организовать к слову говоря про центре у меня даже был было вступление На прошлом де вопсе год назад по-моему Год назад где я как раз рассказывал о том как вот такой большой компании как контур переезжали на вот этот Инструмент там конечно были Борис страдания но и Happy End конечно какой-то там тоже присутствует все-таки у нас получилось это сделать вот и самое главное что сделать сервер полезный сервис для наших пользователей это самое главное Пользуйтесь самое главное что получилось хорошо давайте теперь немножко ближе к теме твоего доклада которые на за который называется свой друг или враг разработчика вот ну очевидно уже Я думаю всем что про дружбу с разработчиками ты явно очень много знаешь вот давай немножко про свой Это обычно же воспринимается как что-то такое про бизнес и вот хочется понять Как это вот разработчиками дружбы или вражда как она вообще может появиться Да ты совершенно прав что вообще SL изначально это про бизнес и вот тут как раз чтобы не спойлерить Я ставлю это на доклад постараюсь ответить так чтобы как вообще вот эта Дружба Дружба когда Мир Дружба жвачка осуществляется И вообще может ли она быть и способным именно это этот я думаю как раз входит доклада и узнаем Хорошо тогда предлагаю не затягивать и перейти собственно к докладу Спасибо Саша Итак всем еще раз Привет меня зовут Григорий И как мы уже выяснили работу в компании контур и сегодня мы будем говорить про slf будем говорить про факапы будем говорить про то какие выводы из этого можно сделать и где там жизнь по другую сторону если свет в конце туннеля Прежде чем я начну я сделаю такой небольшой дисклеймер что то что я буду рассказывать это некоторое вышли инцидент который произошел вымышленной системе которая случилась вообще какой-то вымышленной Вселенной это по мотивам художественного произведения Незнайка на Луне возможно кто-то из вас читал либо произведение либо своим детям вот у меня очень сильно любит это произведение вот ему несколько раз полностью от корки до корки перечитывал по мотивам от этого произведения как раз и будет некоторая история все совпадения являются не преднамеренными случайными а только говорится Мало ли что Итак какой у нас сегодня план во-первых поговорим как устроен этот мир и автоматизация в нем Чтобы понимать во-первых что мы хотим делать с чем работать что там происходит как это устроено Вот такая некоторая вводная будет Вот Потом мы разберем некоторый крупный инцидент который случился и как собственно говоря было восстановление от этого всего все системы все это вот экосистема которая была создана и работала и далее мы как раз плавненько перейдем к таким аббревиатурам как SLS и по пути там еще разберем другие трехбуквенные аббревиатуры и наконец дальше мы будем говорить непосредственно про надежность поскольку инцидент когда что-то ломается это как раз где-то из области надежности вообще посмотрим от чего она зависит и как можно на это повлиять в конце традиционно будем какие-то выводы подводить вот ну и сразу важное замечание вообще говоря может ломаться много что вот в конце концов может сломаться вообще все в какой-то системе Вот это может ломаться Ненадолго это может ломаться надолго плохо когда ломается надолго но такой Иногда случается вот при этом может случиться Так что нечто когда-то сломавшееся но иногда и не возвращается в первозданном виде это тоже жизнь с которой иногда мы сталкиваемся но для любопытных в презентации оставил несколько ссылочек там тоже где-то что-то сломалось поэтому могут потом после презентации когда я выложу пройтись по этим ссылкам и там поизучать где в мире бывает иногда ломается вернемся к нашей истории и к нашей Вселенной Итак жизнь на Луне как она устроена вот для тех кто читал это произведение знает о том что Лунатики живут на поверхности внутреннего ядра Луны и они живут в 9 городах вот как-то города расположены по карте Луны и что мы про них знаем о том что в каждом городе расположен космодром в который прилетают многоразовые ракеты то есть еще задолго доила намаска все было придумано чем все замечательно и как это работает Зачем нужны ракеты во-первых в них летают коротышки из одного города в другой или там например на землю земляным коротышкам также там еще возят различные товары Ну то есть Они хотят там семена перевести продукцию еще что-то и как формулируется в данном случае задача Нужно научиться планировать и контролировать заполняемость ракет Ну представьте мы взяли ракету решили запихнуть не слишком много и она не смогла взлететь там или упала по дороге как бы никто такое не хочет Поэтому нужно это уметь делать вот ну и поэтому коротышки придумали очень важную штуку что нужно следить за каким-то товарооборотом и организовали фактическую товарооборотную службу сокращенно будем называть ufts В общем не такой ft сделали и как раз картошки могут ей пользоваться как они это делают они декларируют свои товары которые импортируются или экспортируется с Луны Как это работает на самом деле Давайте вернемся к карте Луны вот где карту получают туда идите это не в этой Вселенной здесь все для людей то есть для коротышек и в действительности можно подать декларацию в том филиале Где ты находишься Независимо того куда у тебя ракета прилетает Независимо того куда она направляется в какой город Представьтесь к и он хочет что-то отправить его получить вот на ракете которая будет с другого космодрома Ну соответственно приходит в свою локальную систему которая находится в его городе декларацию дальше как-то оно передаётся дальше и соответственно Все работает прозрачно для данном случае пользователя Давайте попробуем представить О какой же архитектуру для этого можно избрать смотрите здесь у нас 9 городов поэтому организуем в каждом из них центр обработки данных и в каждой из них у нас будет размещено что теперь там будет некоторые экземпляр вот этой информационной системы которая будет отвечать за это за этот обмен данными там есть довольно жирная скорость база а под капотом у неё отказоустойчивая система хранения данных чтобы вдруг что-то ломается уровни системы хранения мы могли там поменять диски все было бы замечательно Ну понятно что иногда может сломаться что-нибудь более масштабное и тогда конечно нужны бэкап поэтому там предусмотрено системе некоторые периодические бэкапы которые попадают ленточные носители Все надежно классический интерфейс а теперь Давайте немножко поговорим про СУБД СУБД у нас будет действие лицензировано только на один сервер потому что очень дорого держать несколько серверов такая вот своего рода экономия Ну поскольку у нас есть отказоустойчивая система хранения данных то вроде бы это не так страшно потому что она поддерживает горячую замену диска то есть сломался диск мы поставили новый всё замечательно Ну а в случае как каких-то таких крупных сбоев можно останавливать логов и это сделать со штатными средствами БДС все вроде бы хорошо при этом вот При этом если что-то происходит более масштабное и прям все ломается то мы можем взять и восстановить нашу базу с ленточных носителей такой классический enterprice при этом смотрите поскольку мы говорили о том что у нас есть 9 содов в каждом из них происходит некоторые обмен то здесь можно организовать некоторую локализацию обработки Представьте что у нас 900 и все что нужно обмениваться внутри этого отсюда исключительно в нем то есть Представьте что у нас есть ракета которая находится в этом городе и коротышка которая тоже хочет воспользоваться услугами транспортировкой ракеты тоже находится в этом городе весь обмен который нужно осуществить Он будет осуществлять осуществляться только в информационной системе которая находится в одном единственном соде то есть такая некоторая децентрализация и соответственно Если вдруг нужно передать на другой космодром какие-то данные то здесь уже организуется информационный обмен между этими информационными системами как при этом будет организован транспортный уровень а транспортный уровень в данном случае будет обеспечивать взаимодействие информационной системы каждый из каждой То есть у нас есть некоторые точка естественно из нее можно будет данные передать в любое вот некоторую подводку мы такой провели То есть как все это выглядит себе представили А теперь давайте вернемся к тому самому Shit happens Я буду рассказывать о некоторых хронику инцидента то есть некоторые День Икс отмечены на временной шкале и мы сейчас пронумеруем дни соответственно в будущее от этого момента и в прошлое последующие дни будущее будут отмечаться положительными числами соответственно одни до инцидента они там вот так вот у нас в какой-то один из дней X пришел Отказ А случилось что в одном из содов в системе хранения данных сдох один из дисков что в этой ситуации делают инженер коротышка он идет и заменяет неисправный диск ровно процедуру возможно делают и даже процедуры не в первый раз что при этом произошло неправильная цивилизация нового диска и было потеряно 2 ТБ данных при этом был развален весь дисковый массив суммарно Примерно 100 терабайт беда понятно что с этим уже один коротышка не справится нужно называть в помощь и поэтому в течение примерно суток пытались восстановить данные на уровне дисковой системы точнее делать своими силами то есть теми коротышками которые находились в этом соде вот также обратились к саппорту потому что понимали что случае очень сложный и нужно нужна помощь вендора по системе хранения данных по СУБД и соответственно вот спустя сутки примерно было принято решение о том что нужно устанавливать базу данных из БК в этот момент спустя один день такое решение было сделано Но есть один нюанс как известно всегда здесь бывает какое-то в данном случае но бэкап он примерно 6-дневной давности Что это значит что если мы будем восстанавливать систему то мы восстановим к почти недельного отставания От текущего момента от момента именно Когда произошел инцидент при этом был дан прогноз Что закончится это к утру то есть грубо говоря еще примерно часов 20 Можно даже поменьше по масштабу посмотреть это будет восстанавливаться вот Но что произошло на самом деле на самом деле восстановление заняло значительно дольше А почему это так произошло Ну во-первых восстанавливать пришлось более 60 терабайт данных и процесс в итоге занял порядка 30 часов и Давайте попробуем понять Вообще что в этот момент у нас происходит на вот этот момент состояние когда восстановили базу из бэкапа во-первых сервис уже не работает два с половиной дня при этом Потерянные данные за 6 дней к счастью немножко повезло и уцелели рядологии Ну как повезло Это так или иначе было заложено в некоторую архитектуру этой системы и поэтому удалось восстановить транзакции из ряда Лога То есть это еще заняло некоторое время и при этом было беззатратно утеряно данных уже за 11 часов то есть значительно удалось сократить вот этот интервал данных за который интервал времени за которые были все-таки потеряны эти данные Давайте посмотрим что получается в этот момент времени происходит сервис уже не работает три дня и беззатратно потерянные данные за 11 часов кажется стало получше мы вместо 6 дней получили всего лишь 11 часов и Давайте попробуем подвести какие-то итоги понять Вообще что у нас происходит и по делам какие-то выводы поэтому Саша Возвращайся к нам и Давай попробуем обсудить какие выводы здесь может быть наши слушатели могут сделать и написать какие-то свои идеи в чатик Давайте подумаем что вообще здесь происходит Ну вообще это очень душераздирающая история конечно очень жалко коротышек не хотел бы их вместе оказаться насчет выводов так сразу сложно сказать напрашивается конечно очевидный вывод про нормальное Дело нормально будет Да вот вопрос что делать Какие действия еще можно в этой ситуации предпринять так вижу что в чатике никто ничего не пишет О вот уже три дня да Это довольно плохо Вот они могут например испортиться Представьте что вам нужно перевести какой-нибудь молочко его не доставить протухнет там Через пару деньков или там не знаю везете там помидоры или там что карташи были там арбузы огурцы и прочее вот они не могут храниться долго означает что они приведут какой-то негодность довольно большие будут убытки для коротышек и конечно Это очень печально еще идеи могут быть ночью более конкретная идея У меня например пока данным не поступила еще предложение более конкретная идея наверное таких ситуациях может быть надо заранее как-то думать вот думать заранее это очень хорошая мысль вот которая возникает какой-то момент то есть вот В текущей ситуации ты теряешься действительно что делать потому что у тебя очень мало входных данных и непонятно что ничего обычно возникает Совершенно верно еще и Паника сейчас и поэтому я обратно переключусь на презентацию и вот этот маленький мальчик он как бы в таком же недоумении находится как мы вообще какие выводы можно сделать потому что Да вот там как раз в чатик уже пишут о том что не проводилось регулярная проверка процедура восстановления с бэкапа Вот то есть это уже верно верная идея что вообще говоря здесь регулярная проверка это некоторая процесс получается некоторые такой системный подход и вот как раз о нём мы сейчас дальше и будем говорить о системном подходе что предлагается в качестве подхода Я думаю уже все догадались что речь пойдет сейчас как раз о том самом salay сервис level griment вот я это переводится как Соглашение об уровне сервиса вот я тут чтобы предложение было закон логически закончено я добавил слово качество то есть Соглашение об уровне качества сервиса и вот Давайте подумаем А зачем вообще нужен Sli Давайте попробуем голосовать в чатике можно соответственно оставлять какие-нибудь варианты ответов можно достать вообще за любой вариант Вот сейчас буду варианты накидывать Вы голосуете согласны не согласны с этим там ставьте соответствующий номер ответа номер буквы Давайте смотрите Sli нужен для того чтобы продать сервисы подороже то есть у нас есть Поэтому мы можем просить использовать или много раз больше денег например кто согласен с тем что такое можно сделать вообще хороший план я вот сходу не могу решить Хороший ли это план для себя пока я пока еще никто свои варианты не сложный возможный пункт такой бизнес слова все-таки здесь мы же занимаемся разработкой да то есть мы создаем код моего эксплуатируем мы занимаемся тем чтобы он работал Поэтому нам это пока не очень понятно ладно хорошо давайте может быть какой-нибудь другой вариант рассмотрим может быть оставить конкурентов вот у нас есть конкурентов нет поэтому люди идут к нам и пользователи к нам прибегают Кто считает что такой вариант вполне возможно поставьте ответ Б если согласны с таким тезисом что в принципе тоже для этого может быть Сделано в нашей системе вот Ладно но пока вы думаете над этим вопросом над этим именно конкретно пунктом мы пойдем дальше и сразу следующий разберем там дальше Вы будете как-то голосовать то может быть не хотят голосовать Люди потому что они знают что же там будет дальше Итак третий вариант это удовлетворить запросы клиента вариант 7 если вы считаете что несколько вариантов подходит можно за несколько соответственно вариантов голосовать Ну и Давайте наконец четвертый вариант он будет последний это быть в тренде Ну как бы там Google Сергей пук да Вот как раз Люди спрашивают что неплохо весь список увидеть то есть Google сырье Book Это модно поэтому надо быть в тренде и поэтому будем отставать и не делать это круто Вот и вариант D соответственно такой вот Давайте попробуем проголосовать и понять Ну да там какие-то гипотеза о том что секунд сейчас работает и прочее степени для родных людей что-то для разработчиков что-то для бизнеса Сейчас посмотрим сколько здесь менеджеров собралось варианты б б ц вариант заодно выясним если кто-то кто хочет быть в тренде пока никто не хочет летом кстати говоря про soli там не так много там по-моему одна отдельная глава Да там правда немного она не совсем про это скажем так в чатике уже появились первые ребята менеджеры которые там хотят дороже вот такие прям которые прям проектов которые за финансы все такое прочее разные варианты приходят в целом сразу скусили варианты вот начали такие массы сыпаться вариантом цен то есть запросы клиента вот Хотя справедливости ради каждой из этих пунктов имеет право на жизнь потому что каждый из этих вещей она некоторые смысл в себе несет Действительно это может быть может быть не целью по крайне из причин почему slave сервиса появился смотрите запросы клиента и тут важный вопрос Когда с коллегой зовут Вадим Он спросил вообще что такое качество я задумался Конечно вот контексте того что до 3 запросы клиента общего просто хорошие Что такое качество и я решил посмотреть а как к этому относится в мире то есть вот я как-то по-своему понимаю а может быть мир то как-то по-другому это считает вот зарылся во всякие разные стандарты ГОСТы и прочее нашел Вот Что Вот есть такой ГОСТ за каким-то номером в котором говорит Следующее о том что качество это совокупность свойств продукцию обуславливающих ее пригодностью терять определенные потребности в соответствии с ее назначением что очень много слов и там и стоит международный стандарт тоже там много много слов поэтому давайте я сейчас просто выделю самое важное соответственно в первом случае удовлетворить определенные потребности во втором это удовлетворять потребности потребителей Ну кажется про одно и то же и в целом качество здесь подразумевается несколько в таком виде как мы и ожидали то есть потребности клиента Ну и Давайте посмотрим А как же устроены слои там довольно много пунктов вот сейчас мы с вами по ним пробежимся начнем с формального договора определим Все как надо вообще заключать там как описывать стороны сервиса ладно шучу конечно Мы это с вами делать не буду Я все-таки больше хочу вас наоборот замотивировать использовать этот инструмент а не того чтобы от вас отпугнуть от него поэтому конечно же об этом не будем говорить поэтому мы будем говорить о том что действительно касается каждого И вот я утверждаю что это касается каждого вот поставьте плюсик если Вы согласны что касается каждого то есть там каждого разработчика каждого эксплуататора и минус если не согласны с этим тезисом Саша ты как считаешь касается каждого или нет Ну ладно придется распределить Да я считаю что это скорее касается каждого Потому что ты уже немножко знаешь о чем выше буду рассказывать У тебя есть такой конкурентное преимущество перед нашими слушателями но в целом Да понятно что это будет касаться каждого и попробуем сейчас Как скажем провести доказательства этого вот смотрите мы говорим о том что это вот такое формирование ожиданий От качества работы сервиса А вот у кого и вот здесь важно понимать что это вообще говоря делится на уровне Какие уровни во-первых для пользователя вот смотрите есть сервис есть соответственно пользователь и мы можем нашему сервису говорить что у него такой об этом соответственно декларирует нашему пользователю Представьте что ваши пользователи не очень притязательные И вот такое там 90 процентов Допустим доступности или там объема работ которые вы готовы выполнять его полностью устраивает он счастлив вот замечательно сервиса может находиться что-то еще и соответственно могут быть какие-то другие команды которые тоже могут предоставлять какие-то сервисы для вашего продукта который вы уже зарабаты непосредственно для ваших клиентов такой другой команды Например является инфраструктура то есть некоторая инфра которая тоже предстоит вам какой-то какие-то сервисы какой-то набор инструментов Вот и Представьте что инфраструктура предоставляет несколько иной уровень качества чем вы ожидали ну и соответственно тот который вы там дальше пытались как-то транзитивно транслировать в этом пользователя ну и соответственно что происходит там в вашем сервисе соответственно не сможет не сможете выполнить ваши силы Ну и пользователи будут негодовать Это не то что они будут Хотеть то что они будут ожидать И вообще говоря Sli еще можно формировать для самих себя например в каком случае вот есть у нас инфраструктура которая предоставляет допустим хороший уровень качества Ну там Понятно В каком смысле хороший ну 95 получше чем 90 или там 85 как было в предыдущем слайде Ну вот и смотрите этот уровень качества позволяет нам обеспечивать некоторые свой уровень солей и всё будет хорошо для пользователя я напомню как раз для Сами мы это вот та команда которая делает именно сервис который пользуется вверху клиенты Вот и в этом случае инфраструктура предоставляет некоторый уровень и вот тут вопрос вот если вот он нарушается мы должны уметь отвечать на вопрос а факап уже случился или еще пока не случилось и что делать нам если допустим уже сервис который предоставлялся нижележащим уровнем инфраструктуры например он становится по качеству хуже становится Да и что это делать то есть пока ты уже случился Какие наши действия должны иметь какой-то план и Да там может отвечать на вопрос Там они уводят ли всех нас вот но сразу скажу в нашей компании так не принято и мы задаемся другим вопросом а что мы можем в этой ситуации сделать чтобы все-таки качество сервиса которые Гарантируем нашим пользователям оно выполнялось Вот Но это мы говорим про сами себя про самих себя когда говорим про сервис Но вообще говоря у инфраструктуры то тоже есть уровни Какие Ну во-первых это программное обеспечение которое мы ставим соответственно за софт который там пишет инфраструктура или там какой-то готовый софт который мы ставим Не знаю там допустим ставим постгрис или там кавку используем другой инструмент это все программы включения которая может сломаться и все это есть софт Он работает на поверх какого-то железа или там допустим можно еще здесь как промежуточный звено между полой железом поставить не знаю допустим система регистрации там контейнерализацию например да все такое тоже можно добавить в зависимости от того как мы хотим декомпозировать то есть в целом уровне можно выделять Ну вот понятно что под железо оно где-то находится сеть которая должна работать обеспечивать там работу всех сервисов и пользователь мог Достучаться до нашей сервисы и получить то что он ожидает ну и наконец наверное последняя по списку но не по значению это люди в конце концов там сеть поднимают люди железо существуют люди по тоже так или иначе доставляется с благодаря разработчикам благодаря devops-инженерам и в целом если вот у нас есть дата-центр и в нем сломалась сеть и при этом человек который занимается этой сетью до центра и с ним и он сломался тоже дату с ним заболел например то чинить как бы эту сеть если вдруг она сломается уже сломалась как это будет затруднительно и понятно что тоже все очень важный и неплохо бы тоже уметь оценивать в качестве такой метрики например существует бас-фактор что что-то если с человеком случится то есть ли еще кто-то кто соответственно и сколько таких людей которые про это что-то знают и могут соответственно это починить вот поэтому нас будет волновать сегодня та часть которая называется спецификация село и мы про это сейчас и подробно с вами поговорим Вот это аббревиатура как сервис levelctive переводится как уровень сервиса и опять же тут сам добавляю слово качество чтобы логически закончить это приложение то есть уровень качество сервиса на самом деле речь идет не про какой-то салон Это обычно идет про какой-то множество таких целей Почему мы идем и тут важно понять Вообще выбирать чему эти цели должны удовлетворять чтобы и формулировка и вот сейчас мы разберем несколько важных пунктов во-первых они должны быть повторяемы Почему Потому что если у нас что-то случайно получилось достичь какого достичь какого-то уровня качества то это не считается мы должны уметь это повторять из раза в раз если мы это не умеем повторять то плохое соответственно плохая цель и не надо ее брать второе это то что мы должны уметь измерять Это означает что мы можем некоторую получить метрику некоторую чистилку и умеете ее потом сравнивать вот у нас было сегодня такая чистилка завтра будет такая и вот есть некоторые чистилка которую мы декларируем допустим в качестве слова внешний мир и вот мы больше это у нас этого порога меньше этого порога находится диапазоне очень нужно уметь как-то все сравнивать это нужно уметь как-то все это измерить далее нужно всегда говорить о каких-то значимых вещах значимые то есть они действительно несут какой-то смысл Как это можно понять ну во-первых если вы говорите допустим в отношении пользователей там все понятно у пользователей есть некоторая цель которую они преследуют Пользуясь вашим сервисом и если там для них вот важно именно получить Такой сценарий то наверное Вот именно доступность этого сценария это будет важный метрик если допустим речь про ваши какие-то собственные собственные внутреннее село то всегда так или иначе вы понимаете какие-то ключевые метрики которые есть в вашем сервисе на которые вы будете смотреть не знаю будет это все доступности или какие-то другие показатели именно те которые важны именно для вас то же самое то что вы можете предъявлять допустим для инфраструктуры которые вы пользуетесь то что для вас важна допустим тех инструментов которые вы пользуетесь и так далее следующее это про достижимость про достижимость В каком смысле вот вы допустим поставили перед собой некоторую цель что вы там хотите стопроцентную доступность сервиса вот Ну так если подумать то окажется что все-таки это что-то нереальное и такой вряд ли получится плюс достижимость она тоже определяет некоторые там количество ресурсов которые нужно это потратить И здесь тоже важно формулировать то что реально можно сделать там за тот грубо говоря бюджет который который мы обладаем далее мы будем какие-то примеры разбирать и там мы наверное это увидим то где реально достижимость она как она зависит от того какой архитектуры Мы выбираем или какими инструментами пользуемся и наконец последнее по списку также не по значению это то что вот эти цели которые мы должны выбрать и которым должны идти они должны быть управляемые Что значит управляемая означает что тот показатель который мы оставим себе в качестве ключевого он мы можем еще Влиять на него на вот эту метрику то значение которое мы должны получать Потому что если мы это не влияем то соответственно зачем нам это нужно мы если этим управлять не можем то соответственно и повлияет это никак не можем и повторить Мы тоже соответственно это не можем потому что это нас не зависит вот в этом случае такой Конечно такой соло будет бесполезны при этом важно понимать что да мы можем говорить что это от нас не зависит Ну допустим это предоставляет инфраструктура и вот тут Важно мы можем соответственно предопределять каким-то им требования к слову и соответственно то что для нас было неуправляемым для них внезапно становится управляемая инфраструктура которая уже сама там делает некоторые сервис и они соответственно могут уже сами определить да готовы они такой уровень поддерживать и соответственно делают какие-то процедуры у себя чтобы это этому соответствовать Ну понятно что они так по цепочке могут зависеть от каких-то других нижележащих уровней В общем так это потихонечку собирается в такую матрешку и что важно вообще вот эти вот все slo они должны заряжать некоторую качественную характеристику нашего сервиса Вот какие качественные характеристики данном случае я выделяю во-первых это безопасность безопасность смысле Security То есть если вам пользователь доверяет некоторые данные и вы обещаете их не раскрывать мошенникам но наверное это важный показатель качества вашего сервиса важные салон второе это надежность надежность в смысле того что был сервис он работал потом раз и сломался Да вот и потом подчинился И вот как раз то насколько он ломается как учительница это есть показатель этой самой надежности то что будет Как раз интересовать про Дальше можно говорить про производительность о том что мы там обязуемся там какой-то объем данных допустим переваривать не знаю в контексте какой-нибудь инфраструктуры Когда речь идет о шине данных там например количество событий которые можно передавать через шину или там количество много запросов которые можно делать к сервису от такого рода показателя плюс также это речь про летосе да то есть что там запрос должен выполняться не дольше там допустим строки то по времени вот и тоже что Про что не стоит забывать это тоже довольно важная характеристика это поддерживаемость сервиса то есть именно смысле мы берите то есть о том что вот вы сервис предоставили может даже надежный Вот но чтобы обеспечить надежность там нужно постоянно приседать В общем это невероятно больших усилий стоит со стороны группы поддержки группы девапс инженеров Вот и тут тоже важный показатель насколько там все это полезно для в других ролей вот в этом таком производственном процессе как я уже говорил мы сегодня будем говорить больше про надежность потому что у нас и факап был как-то связан с надежностью сломалась сломалась сломалась сломались данные сломалась бы да Вот и вокруг этого много чего не работало всё то самое надежность которой мы хотим чуть глубже заглянуть но надежность она естественно тоже не какая-то такой вакуумная вакууме slo тоже на самом деле можно его декомпозировать эту надежность и Давайте попробуем это сделать Первое это устойчивость к высоким нагрузкам и тут Важно это не про перформанс а именно про перформанс который производительность которую мы рассматривали на уровне с надежностью это именно речь о том что у нас система в ней произошел допустим какой-то всплеск высокий высокий всплеск по нагрузке и то что система у нас сломалась или не сломалась вот эти вот такого рода показатели там есть у нас троттлинг и прочее далее устойчивость к отказам зашёл какой-то сбой инцидент и И вообще вернулась ли система обратно или нет плохая система которая один раз сломалась и больше никогда не восстанавливается ненадежная система и Наверное это не то что мы хотели бы делать и соответственно тоже вот такую цель Неплохо бы для себя как-то тоже ставить далее это вот такой уже более частный случай это про высокую доступность это про то что в принципе тут наверное все понимают о том что вот пользователь пришел сервис работает значит для него доступен если пользователь пришел а сервис для него не доступен Это значит что это невысокая доступность потому что пользователь хотел что-то сделать а у него не получилось вот понятно что там под капотом как-то все можно хитро спрятать от него но тем не менее это то на что нам всегда нужно в данном случае ориентироваться далее Это восстанавливаемость восстанавливается в том смысле что вот если у нас пришел какой-то отказ мы систему восстановили Но вопрос насколько Мы хорошо восстановили потеряли мы какие-то не потеряли какие-то данные целиком остановилась система не целиком такого рода вещи и тоже в паре к нему можно добавить именно про целостность данных из того что данные это были восстановлены все замечательно насколько вот эти данные консистент между собой то есть Может быть получилось так что мы какие-то данные восстановили какие-то не восстановили Но от того что мы восстановили данная зависимое зависящее от каких-то других не восстановленных то понятно что ценности какой-то в этой в таком действии будет немного и конечно тоже нужно уметь измерять про что мы точно сегодня не будем говорить мы не будем говорить про устойчивость к высоким нагрузкам потому что факап о котором речь была он никак с нагрузка не был связан будем строить соответственно обсуждение вокруг вот этих вот четырех пунктов Итак устойчивость к отказам для того чтобы быть устойчивым к отказом то есть что-то ломается мы могли дальше продолжить работу нам очевидно нужно иметь некоторые запас то есть некоторые должна быть избыточность вот этого не знаю ресурса который выходит из строя можно выделить несколько несколько соответственно уравнение организации это избыточности давайте каждому из них поговорим но первое Это холодный резерв грубо говоря когда у нас есть железо готовое может быть даже у него операционная система стоит но оно даже может быть не запитана она просто может стоять там пылиться в дата-центре Соответственно что нужно будет делать если вдруг что-то сломается нужно будет соответственно чтобы инженер пришел подключил питание там запустил операционную систему Поставил туда весь необходимый софт и там допустим сечь про какую-нибудь базу данных соответственно пошел наливать туда бэкап какая-то такая штука и понятно что такой подход он требует некоторые часы а может даже и десятки часов на восстановление вот следующий уровень это теплый резерв обычно это не говорят в контексте веб-сервисов это больше на справедливо к базам данных это некоторый такой подобие знаю например асинхронной репликации то есть у вас есть некоторые сервис некоторые некоторые получаются instance базы которые в которые идут данные мастер да Вот соответственно фолловер в который данные будут синхронной реплицироваться понятно что случае если какого-то сбоя то у нас на нем будут не полные данные Возможно там инженеры нужно будет прийти запустить какие-то скрипты там какие-нибудь может быть там откуда взять 3D логи которые можно будет на вот этом на соответственно вот этой второй реплики прогнать восстановить вот если допустим в первом случае это требовалось там часы на восстановление здесь мы там это займём минуты может десятки минут да третий уровень горячей резерв он еще по-другому называется Active Passive это когда вот у нас есть текущий Вот какой-то вот Active получается instance который обрабатывается запросы и есть рядышком вот такой еще один точно такой же как Active только который к себе там грубо говоря данные например реплицирует стеречь про базу данных случае там с субт там допустим традиционных баз это как раз там подход Когда у нас есть асинхронная репликации мастерслав Ну понятно что там в этой ситуации уже участие для восстановления от разработчика или инженеры не требуется это должно происходить автоматически Понятно В случае третий вид на сервис который там засветительствует что все там старый мастер умер Да здравствует новый мастер это всё нужно будет производить понятно что в этой ситуации там восстановление оно тоже сколько это должно занять потому что нужно зафиксировать что мастер уже не мастер Он уже это занимает некоторые секунды То есть у нас получается такой был подход часы минуты Сейчас уже секунды наконец-то там последний вариант это уже сбалансировкой по-другому еще называется Active actic То есть когда у нас есть два равноправных инстанции которые каждый из которых обрабатывает запрос Ну и понятно что если у нас в один из них запросы не прошел там что-то сломалось то всегда допустим можем притравить опять же тут со звездочкой да то есть некоторые особенности там тоже допустим патент запросов или что-то такое Но это мы оставим сейчас за скобками важно то что у нас есть еще один который может продолжить обработку мы можем сделать и успешно выполнить наш запрос который мы хотели сделать вот в этой ситуации там уже здесь о каком-то не знаю там Времени доступности приключений особо и нет смысла говорить потому что это уже какие-то миллисекунды то есть запрос не получилось пошел сразу в следующую реплику вот что происходит у нас соответственно при движении от Cold standby к Low Balance Ну во-первых у нас растет стоимость такой инфраструктуры и сложности Почему Потому что в одном случае нам нужно было обеспечивать там балансировку нагрузки или там синхронную репликацию которая тоже там влияет при этом еще и на производительность но при этом Да все это растет но что самое главное во всем этом случае это то что уменьшается время восстановления то есть мы начинаем восстанавливать меньше Но если вспомнить нашу инцидент коротышками которые там меняли диски сломалась то это больше похоже на такой Call Stand Bike Когда нужно было просто опять там грубо говоря с нуля всё разворачивать и накатывать такой как раз и по маслу именно то что за него там часы Ну на самом деле десятки часов до времени Вот как раз что-то похожее на это и произошло то есть понятно что если бы там использовалась Э не один сервис BT а допустим два и даже в варианте там не знаю синхрон или синхронный репликации то соответственно время восстановления оно заняло бы значительно меньшее время но Давайте двигаться дальше и поговорим про доступность вообще доступность это такая штука которую можно считать очень по-разному вот я расскажу лишь про один из способов про самый простой способ как его там можно описать Да система в которой происходит некоторый отказы То есть это то с чем реально Приходится работать Чем приходится жить вот некоторые второй отказ И вообще говоря в системе которая продолжительное время работает мы так или иначе можем считать метрику среднее время между такими авариями При этом когда вот у нас Бавария происходит у нас некоторое время уходит на восстановление тоже есть некоторые такая Метрика в среднее время на восстановление мы тоже можем соответственно такой метку вести ну и соответственно всё остальное время когда мы ждём следующее аварии это время когда мы работаем и в данном случае то есть доступность можно считать следующим образом то есть время когда сервис сработал к вообще ко всему времени То есть там одну метрику можно определить на другое место Но как правило так вот никто не делить не считает в таком виде они работают а принято говорить по правилам девяток вот Напишите в чатик поставьте знак процента если вы с девятками сталкивались чтобы понимало Как нужно нужно ли подробнее на этом останавливаться вообще понятно что девятка Может быть одна там самый большой вариант там 7 девяток вот что вообще важно тут отметить но вот смотрите можно вот доступно считать за разный промежуток времени можно считать допустим за год можно считать за месяц за неделю за день Вот и понятно что времени доступности оно соответственно будет разное тут у меня соответственно в каждый столбике в год вместе с тестом указано значение вот этого времени доступности разрешённое скажем да такой легитимное время недоступности для указанного уровня количество девяток вот ну и если вспомнить наши инцидент когда там сервис не работал порядка трёх дней и предположить что это в этой системе не было ни одного факапа а за этот год то лучше на что может рассчитывать это система это на соответственно две девятки если там мыло соответственно формулировали там за месяц Ну понятно что это там всего лишь одна девяточка и ещё звучит довольно грустненько при этом и Хочу привести пример систем которые бывают Вообще супер надежные вот как-то я гулял по Новосибирску и изобрел в один из дворов где такое объявление увидело о том что даже на секунду нельзя закрывать проезд машинам потому что они там всегда должны иметь возможность проехать это то что секунду в год Ну это получается Даже лучше чем 79 какой иногда бывает Но это конечно же речь не про информационные системы и такие вещи как правило никто не гарантирует Вот мы с вами поговорим про высокую доступность Давайте немножко поговорим про восстанавливаемости тут тоже будет несколько аббревиатур тоже трех буквенных как это любят в этом стандарте но давайте начнем с первой смотрите у нас вот есть отказ и в действительности когда мы начнем восстанавливать Ну вот на какой-то момент в прошлом то есть Recovery Point jackit то есть точка которую мы восстановим случае с тем факапом которым обсуждали Это же было аж целых шесть дней но удалось снизить потом до часов факт типа изначально вот это вот рпо это было аж 6 дней и вообще в целом Если вы там не знаю придете К бизнесу и Спросите вот у вас там типа может случиться факапом когда-то случится обязательно как бы должны все понимать вот э-э сколько данных Мы готовы потерять Вот и как правило ни один бизнес не скажет что это можно данный терять вообще без проблем как правило вам все будут говорить что это 0 минут прямо вообще нельзя Вот и тут Важно в этой ситуации донести такую идею что вообще говорят они бесплатно и это будет реально стоить определённые затрат и в плане инфраструктуры созданий и архитектуры приложения ну и наконец вторая вторая Метрика от Recovery Time objective это получается время которое нам потребуется то чтобы восстанавливать систему Ну и в случае вот этого факапа который мы разбирали там аж целых три дня плюхались и восстанавливали все это и главное что восстановили самое главное но тем не менее Вот такие метрики их полезно соответственно считать Когда речь идет о восстанавливаемся системы Давайте перейдем к целостности данных там история какая соответственно есть у нас распыленная система вот у нас была там несколько городов был там соответственно там первое экземпляр 2 экземпляра находитесь карты далеко друг от друга и Вот соответственно база в одном из содов она приуныла Да и что будет соответственно если мы потом вот эту базу восстановим то есть мы восстановили допустим на какой-то момент в прошлом но данные не те которыми обменивались между собой информационной системы 12 то есть в этой ситуации когда это произойдет у нас во второй системе будут какие-то данные о которых первая система даже и не знает Вот в случае с коротышками Да такое такая проблема произошла Что произошло сделать благо что в принципе там архитектура позволяла там запрашивать дополнительные данные пользователей и по сути там восстановление консистентное состояние между двумя базами информационных систем там пришлось реально напрягать уже пользователей им соответственно при там догружать данные в первую систему там из каких-то своих локальных ковшей там из какой-то своей системы там может что-то не знаю еще какой-то короче там было прям более страдания вот ну и нужно понимать что там так некоторые вещи не были восстановлены но это уже как говорится история умалчивает на самом деле это не все и Давайте поговорим про такую штуку как поставьте плюсик пожалуйста те кто вообще слышал такой термин и минус соответственно кто не слышал такой термин как изящная деградация Сейчас узнаем много ли среди наших слушателей действующий фронтейндеров или бывших фронтейнеров потому что практически все вот с кем разговаривал практически все фандендеры прогресс-дегрейшн слышали пока вы отвечаете в чатик я расскажу Вообще покой деградации речи но сначала пример из frontendo расскажу там история какая-то Представьте что в Темные времена когда был интернет Explorer и был там какой-нибудь старый-старые версии приложение нужно было их поддерживать коллеги тоже там когда все время рассказывали страшилки о том как они поддерживали шестой интернет Explorer Представьте что там в нем не реализованы какие-то модные фичи которыми Все любят пользоваться и которые работают в других браузерах вот и что происходит соответственно он не будет работать Да И вот такая деградация её изящнее никак не назвать потому что пользователь видит белый экран и не знает чем дальше делать можно сделать ему чуть получше и сказать что О'кей вот у тебя браузер старый не поддерживает и Что продолжит работу в сервисе вот тебе ссылочка для того чтобы скачать браузер который умеет Вот это конечно изящная деградации назвать сложно но по крайней мере ситуация допустим Когда в браузер отключены скрипты и сайт хотя бы как-то работает это есть та самая э изящная деградация но если говорить про информационные системы в целом и общем можно эту деградацию соответственно тоже выделить и описать какими-то отдельными пунктами Давайте попробуем это сделать во-первых можно деградировать по производительности но Представьте что у нас есть некоторые сервис и что произошло там допустим часть ресурсов у нас выпала и мы соответственно не можем уже обрабатывать столько же там транзакций делать столько же действие сколько делали до этого Поэтому мы начинаем уже троттлить запросы наших пользователей и соответственно наши сервисы правильные код ответа возвращает чтобы там система которая с ним интегрируются Они понимали что здесь сейчас их троллят и там соответственно надо подождать там прийти чуть позже за данными и так далее Это понятно второй вариант - это когда может страдать у нас функциональность и какой-то блок он может перестать работать самый простой вариант у нас там допустим есть интеграция допустим с картами с картами что-то случилось и мы просто в нашем сервисе Окей мы не показываем карты но вся картами суда там географические карты Да но при этом сам сервис работает там пользователь может зайти там в личный кабинет там не знаю перейти По каким-то ссылкам пользоваться этим сервисом Но вот Единственное что он не будет видеть это вот они видите допустим текущую точку не знаю Представьте что это какой-нибудь сервис такси Да и он показывает Где находится сейчас в этот момент машина Вот вы можете видеть о том что класс машинок он едет она находится вас там в пяти минутах но просто не видеть вот красивый красивый прямоугольничек раскрашенный который движется там по улицам города ничего страшного но тем не менее сервис ты сам продолжает работать такая тоже деградация имеет место быть может быть деградация по данным То есть у нас допустим часть данных просто пропала или какой-то блок данных вспомогательных он стал недоступен Ну например какой-нибудь полно текстовый поиск у нас перестал работать как раз тоже функциональности да деградация и мы когда пользуетесь что-то ищет мы допустим показываем данные США но не показываем там из холодных данных вот мы при этом мы пользуемся явно говорим что вот Дорогой наш У нас сейчас сервисом есть мы испытываем некоторые проблемы Поэтому ты видишь не все данные может быть деградация по пользователям это особенно актуально Когда у вас какой-то гео распределенный сервис там не знаю Вы предоставлять сервис там и в Европе там и в Америке там и в Азии например да и у вас происходит какая-то проблема локальная в содах которые сложно прям в Европе и соответственно Да там европейский пользователь например что-то не увидит случай там с теми которые находятся в другой географической локации у них сервиса будет предоставляться в полном объеме такое тоже может быть в целом тоже система подковому принципу нужно делать если возвращаться к той информационной системе которая столкнулись коротышки там в принципе тоже по пользователям деградация происходила Почему Потому что ведь проблема произошла только в конкретном соде соответственно там где была локальная обработка в одном там соответственно все было хорошо давайте посмотрим чуть в деталях А как это этот сервис деградировал в момент соответственно вот этих всех происшествий смотрите в какой-то момент получается вот сервис у нас не работал тут при с лишним дня Вот но и в какой-то момент было принято решение что перейти на бумагу то есть Окей там не через струны документооборот все это передавать а просто передавать бумажные варианты и там как-то дальше это все обрабатывать вернулись такое в каменный век грубо говоря и вот тут наверное вопрос наверное будет Уже к вам а как вы думаете почему ждали два дня вот напомню что решение перейти на бумагу отказаться от операционной системы которая сейчас не работает и не ждать её восстановления вот перейти на такой вариант почему ждали два дня Почему не сделали раньше то есть мы не работали 3 дня но могли бы принять решение раньше о том чтобы как-то продолжить документооборот и восстановить систему Напишите свой вариант почему вы думаете все-таки два дня это было это важно довольно момент и который может быть наверное как раз нас к выводам подведет но тем не менее Павел прям от прям самое яблочко попадает своим вариантом ответа то что до последнего надеялись что быстро восстановят и вот теперь давайте как раз мы перейдем с вами к выводам и о них поговорим вот смотрите SLS это то что дает нам предсказуемость вспомнить эти метрики про то что как как быстро мы восстанавливаемся да сколько данных Мы можем потерять Вот все вот эти вот вещи Они как раз нам именно про предсказуемость что мы это если оценивать умеем то есть соответственно предсказывать тоже как-то учимся То есть это речь именно про работу с рисками То есть это то что как раз очень часто является зоной роста в разработке в производственном процессе именно работу с рисками Итак Какие три действия можно вообще сделать чтобы начать как-то вот эту предсказуемость как-то с этими рисками работать но на самом деле вот по истории которые мы сейчас с вами обсуждали это три вещи Первое это еще проверить свои бэкапы если они конечно же у вас есть надеюсь что вы делаете бэкапы вот второе это посчитать эти метрики которых мы сегодня с вами говорили от которых как раз Как видно очень много будет зависеть в плане ожиданий пользователей в плане того что вообще понимание нашего понимания то Можем ли мы восстановить не можем как быстро Мы это можем восстановить но если мы соответственно эти риски увидели да то о них рассказать менеджером чтобы это была не только ваша головная боль об этом подумали и другие люди нашли не знаю Вам новых ресурсов и в ответ на вопрос Зачем еще один сервер или там еще Зачем один центр да с вашей системе чтобы Вы могли уже это отвечать не не просто для того чтобы система стала по моднее получше использовать новые инструменты а потому что это реально необходимость для бизнеса чтобы он не оказался в такой жопе Простите меня но вообще и тут бывают некоторые сюрпризы А Какие они бывают вот некоторые вот пришли два человечка и вот один рассказывает о том что у него в сервисе будет доступно 5 9 на секундочку пять девяток это пять минут за год недоступности довольно хороший уровень очень хороший рассказал в ответ скептик спрашивают вообще ты в курсе о том какая доступность у твоего дата-центр в котором ты собираешься разворачивать свой сервис внезапно что люди реально разработчики они не знают и не думают об этом типа не понимают выясняется что вообще центр он предоставляет три с половиной девятки и здесь внезапно это аж целых 20 минут но не за год уже А в месяц и вот тут возникает такая ситуация что Мы вроде проектировали мы там делали супер надежную систему Да только как бы это оказалось данной ситуации зря потому что вот есть вещи от которых как я помню всегда говорил о том что именно то что мы должны уметь контролировать а мы это не контролируем и поэтому нужно уметь какие-то другие решения соответственно строить Ну понятно здесь нужно просто больше до центров и тогда сервисом будет все хорошо но тем не менее вот на вот этой соответственно такой оптимистичный пессимистичный не знаю как с какой стороны посмотреть ноте я заканчиваю свое повествование Вот надеюсь что вам было это интересно и готов отвечать на ваши вопросы Что здесь всякие разные контактные данные куда можно прийти и с какими-то вопросами я тоже могу всякое отвечать потрясающую жизнь тоже доводилось в такие ситуации попадать причем оказывается Вот на месте Вот того парня который слева который пропять девяток рассказывает это правда Спасибо тебе за доклад вот очень классно структурировано все Да пока нам вопросов еще не прилетело у меня есть такой фундаментальный вопрос которым я сам уже давно Думаю интересно твое мнение вот даже при наличии вот такой разложенной схемы то есть что такое надежности чего она стоит как есть составляющие даже такое разложение но все равно показывает что там очень много всего и вот обычно самый такой главный вопрос С какой стороны вообще к этому подойти особенно если скажем так мы такого не делали Ну вообще не имеем опыта или очень мало опыта Это очень хороший вопрос на который Возможно есть очень много ответов вот я расскажу тот про который знаю который так или иначе практикуется у нас в компании то есть Первое это то с чего мы начали этом мы начали заполнять пасмуртом по факапам то есть что это означает что прошел какой-то инцидент и инженеры взяли и записали вот такой там что-то произошло что они делали по шагам вот Какие действия помогли им соответственно там потушить этот пожар и соответственно сделать выводы относительно того А почему этот факап произошел что чего у нас не хватает чтобы этого не случилось и Какие действия можем еще сделать чтобы такого не допустить дальнейшем какую-то декомпозицию проводили и следующим шагом для этого было это кажется анализировать вот эти посмотрим и выявлять вот такие самые горячие вещи которые заставляют наибольшую проблему что туда можно включить Ну во-первых допустим самый простой пример приведу из которой очень часто стрелял это то что вот был получается код который там живёт Там репозитории у него акционирование всё замечательно он там предоставляется автоматизация на уровне cycd до организованность там и тесты гоняются и код доставляется в продакшн вот он внезапно оказалось что а конфигита они живут несколько иной жизнью и сервис доставлялся включаться код-то один а конфиги к нему были другие него соответственно Вот это версия Они просто не было да у них и вот оказалось что огромное количество факапов только из-за того что за диплоили а конфиги какие-то не те или конфиги не те за диплоиль или еще какая-то такая штука вот и тут пришло понимание что конфигит вообще-то тот же самый код к счастью скажу этому не сейчас придумали Это уже давным-давно Это было сделано но сам факт того что типа вот просто перейти к этому моменту что давайте как бы тоже конфиги хранить в репозитории давайте тоже для них сделаем версионирование давайте тоже соответственно автоматизируем процесс доставки этих всё это закрыло просто огромный план проблем Вот то же самое вот уже тоже в начале мы обсуждали про центре когда информирование разработчиков а появление новых ошибок в системе то есть тоже это это мы не просто так решили о давайте это сделаем потому что клёво же там типа что-то новое инструмент надо ещё затащить больше инструментов как бы все равно прикольно как-то с большим количеством инструментов работаешь там тебя резюме такое большое жирное очень много всего но нет На самом деле мы получается проанализировали и поняли что очень много там я сейчас не помню Сейчас конкретные цифры надо вот заглянуть в мой тот старый доклад по центре но по-моему что-то на уровне там 10-15% от всего количества факапов соответственно можно было бы там локализовать или даже избежать просто имея вот эта лертинг полномам ошибка это прям помогло нужно понимать что это мы не остановились и там вот как раз дальнейшие действия были это А давайте мы короче будем не по факапом учиться да попытаться как-то заранее это прогнозировать начались у нас именно учения какого города мы берём отключаем одну реку сервиса и смотри как она себя ведет там допустим мастер да и смотрим что из этого произойдет в контролируемой среде когда мы все контролируем и ничего страшного не происходит после этого мы такое делаем там допустим в целом дата-центром и смотрим как при этом все восстанавливается а заполняется примерно Похоже как вот факап то есть типа вот мы такое сделали еще было там то-то типа вот здесь типа у нас получилось потом вот это быстро восстановить здесь что-то не получилось Тут надо было то вот этого не хватило То есть в принципе там аналитика проводится примерно такая же как случае факапа вот но здесь главное важное отличие от факапа реально то что у тебя пользователь не пострадали Ну и следующий уровень это когда ты уже эти учения делаешь не получается на каком-то тестом в этом стенде А в продакшене то есть ты говоришь о том что твой сервис гарантирует что у тебя там выпадение там одной реплики там одного до центра и приведёт к деградации сервиса у тебя спрашивают А докажи А ты говоришь А вот и взял рубильник такой вырубил до центра и все сломалось Вот это важно важный момент важный момент действительно все будет ломаться в начале и я не знаю можно ли сделать так чтобы оно не ломалось Но по моему он всегда будет ломаться по крайней мере в начале Но вот как раз вот как бы за одного этого битого инженера двух не битых дают да то есть инженер который сломал Он уже как бы научился он стал опытный поэтому Какие крутые инженеры у которых Вот именно есть крутые истории Да факапов которые они смогли зарешать Да вот это вот это наверное важный вот этот опыт который может инженер извлечь Вот и поэтому когда там рассказывают допустим тот же github рассказывает о своём факапи и делает публично довольно круто вот говорит о том что они тоже умеют делать выводы над этим и как-то как-то растут и мы как бы тоже такие компании тоже ориентируемся и понимаем что вот этот процесс совершенствования его конечно не надо останавливать опыт факапов он бесценен это правда это да Ну что у нас кажется закончилось время Поэтому Предлагаю перейти в дискуссию Наверняка у нас есть еще какие темы развивать это зрители тоже всех приглашаем Да все подключайтесь будем обсуждать Спасибо"
}