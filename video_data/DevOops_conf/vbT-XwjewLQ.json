{
  "video_id": "vbT-XwjewLQ",
  "channel": "DevOops_conf",
  "title": "Александр Тарасов — Linkerd: the Hard Way",
  "views": 580,
  "duration": 2803,
  "published": "2022-11-02T15:01:31-07:00",
  "text": "Всем привет Я рад что присоединились ко второму Дню конференции debooks 2022 Меня зовут Лёша кирпичников я покажу разработкой в контуре и со мной сегодня в эфире Независимый Эксперт Инженер с большим опытом и компьютером Саша Тарасов Саша привет привет а я надеюсь что все кто к нам сегодня присоединился хорошо выспались потому что нас ждет один из немногих докладов программы конференции помеченные значком хардкор это сложные Я рад что именно Саша нас сегодня приведет поэтому довольно сложному содержанию потому что Саша делает классные доклады объясняет все На мой взгляд понятным языком прежде чем мы начнем Я хочу напомнить вам что у нас есть чат конференции чат доклада в котором вы можете задавать вопросы Я эти вопросы буду читать по ходу эфира и потом в конце если он останется время то в эфире а если нет то дискуссионный комнате задам Саше мы с вами сможем их все обсудить голосом если вы захотите остаться и я еще хотел попросить вас друзья пожалуйста оценивайте доклады потому что это очень важно для программного комитета член которого я тоже являюсь и если вы поставите Саша докладу хорошие оценки то через год или может быть Если повезёт Раньше я посмотрю на это скажу да наверное достаточно позвать классный доклад был вот так что ведь пожалуйста доклад Саша пожалуйста Давно ли ты вообще вот эту тему сервис mash и Копаешь Мне кажется что Давненько но последние годы полтора Примерно вот от начала разного рода экспериментов с разными сервисамэшами в общем-то до того чтобы принципе глубокое Погружение в linkerde там разработка extension of под него и в общем-то contribution непосредственно в корр продукт детей надоело еще может уже побросить действительно ли от него такой толк большой от этого сервиса машине сейчас ну основной долг который вообще в принципе я вижу это политики авторизации мы сегодня коснемся ближе там к концу доклада потому что это действительно с точки зрения Security Zero Trust внутри в общем-то Продакшен контура Да это Маст хэв вещь и как бы делать ее самому наверное не очень круто Поэтому лучше взять какое-то решение готовы уже и здесь в принципе предоставляет набор примитивов для этим управлять вот остальные фичи прикольные сами по себе они тоже сегодня поговорим Вот но как мне кажется имеет чуть меньшую ценность Слушай ну раз без этого не обойтись то я предлагаю дальше не тянуть я передаю тебе слово давай начинать доклад поехали лично Да всем привет Ну в общем-то доклад называется линкер дизайн hardway значит обо мне уже немножко сказали я могу лишь вот сказать что вот у меня есть соцсети гитхабчик Twitter можете подписываться ставить лайки звездочки и так далее вот вообще почему Да я могу рассказывать что-то про linkerde то есть помимо то что контрибьютор у меня еще есть такая ачивка Роза там серию цикл статей который были написаны там на медиуме от сами в общем-то создатели занимаясь полтора последних года но Даже несмотря на все это я все оставлю такой слайди к отказа от ответственности То есть за все Ваши действия ответственность несете только вы сами не надо говорить что какой-то чувак он там на докладе что-то рассказал бы поэтому сделали зафарились и все Ну давайте в общем-то начинать доклад не подразумевает что будет какого-то сервиса в один до каких-то Я подозреваю что наверное Если вы сюда пришли то вы знаете о том что такое хотя бы сервис меша хотя бы с одним каким-то решением точно знакомы вот да и основной вопрос на котором будем сегодня отвечать это вообще принципе может ли сервис просто работать и сам по себе линкер делал именно позиционирует себя как такой минималистичный простой без пользователей очень Ультра легкий решение Да с минимальным набором каких-то функций на которые вам скорее всего точно нужны Поэтому в принципе Вы можете поставить села или керди нажать там Выполни команду Install инициировать себе кастом и финишный потом соответственно устранить собственно говоря сам линкер И даже если вы скажете что-то Ну это для тестовых каких-то целей то в принципе может Здесь что-то такое флажочек замечательный чей в котором вы получите уже Вполне себе Хабиба берите Control plain сервис и это на самом деле работает то есть вы просто это делаете и у вас это все взлетает то есть нет такого что что-то не взлетело и Понятное дело что установка через сила это все-таки больше на попробовать и как бы Must have решение это конечно использовать hiluncharte тем более для продакшн окружения но установка Hill чартов тоже достаточно простая то есть нужно добавить Ту самую любите конфигурацию сделать отдельный ноут пул для Control plain А в этом нет никакой особой магии То есть просто обычные там вот селектор retions и проставить request или МИД и на прокси да то есть на сам дата Play по сути Вы можете их поставить так и вы хотите Но вот по моей практике такая стратегия оверсайла когда request очень низкие лимиты достаточно высокие работает вполне себе прекрасно и проблем с ней незамеченным сама по себе миграция тоже достаточно простая То есть вы берете есть Волшебная аннотации можно кинуть на под можем накинуть их на name Space чтобы более предпочтительный путь после этого вы идете ре диплоити вся ваше приложение у вас они контейнеры стартуют подцепляется прокси все это прекрасно работает без каких-то разрывов да то есть прекрасно существуют в какой-то момент времени и приложение которое внутри сервиса мыши те кто снаружи который еще для которой еще как бы не замешаны по сути дела вот также скорее всего вам не понадобится делать каких-то добавлять большое количество ресурсов Ну там под контролем Понятно желательно отдельный выделить и немножко возможно придется накинуть в общем-то Нот в для того чтобы поддержать ресурсы вот дата плей-офф но в принципе класса 10 тысяч потов проблем каких-то с этим нет ресурсного верха это очень низкий что подтверждается разного рода бенчмарками например в сравнении с истью линкерди очень легкие с точки зрения потребления ресурсов таких как CPU и Memory в этом прессе можно здесь сказать что вот у нас все работает доклад окончен Но конечно нет значит вопрос основной конечно в том А что именно у нас работает И если мы посмотрим на это с этой точки зрения то мы увидим что у нас работают например michell tls да между сервисами какие-то базовой политики авторизации работает на уровне L4 оси и достаточно просто подключить любой НГС который у вас есть будь то инженец Амбассадор там или что вы используете в принципе читаете инструкцию все достаточно Понятно написано и там каких-то подводных камней особо нет Однако из вещи которые не работают и в рамках доклада мы будем в общем-то делать то что вот эти нерабочие вещи как которые нас не работают последние деньги декора мы будем делать их рабочим а делать Это мы будем с помощью так называемых extension of Их существует достаточно большое количество мы не будем в этом докладе затрагивать экстеншн мультикластер Потому что тогда наш доклад просто не влезет по времени это нужно еще один Слот доклада занимать поэтому все наши манипуляции мы будем делать в рамках одного кластера с этого и начинается наш в общем-то тяжелый путь продакшну к удовлетворению всех в общем-то наших хотелок Поэтому вот посмотрим снова на наше на текущее состояние Да и займемся кронжабами Казалось бы в чем проблема с корнем женой может быть Ну вот мы просто сделали аннотацию навесили ее на namespace пошли там перезапустили наши контейнеры передипло или Все пошли пить кофе потом возвращаемся у нас крон жабы в целом вообще в принципе же в Кубе Просто бесконечно работают Проблема в том что сама-то Джо база завершилась еще нет и в принципе он не знает что ему надо завершаться а поэтому есть простой вариант решения проблемы вы говорите что Ну не нужен мне никакой сервис меш на cruncher Бах и все красивая элегантно но иногда все-таки нужно сделать так чтобы сервис наш работал в корнжабах и тогда вот Давайте с этим разбираться Да чтобы в этом разобраться надо понимать что линкер de Proxy есть такой вот замечательный хук которому можно по завершению самой команды что-то послать постом например через get и прокси тоже будет завершаться после того как ваш основной процесс завершается но выглядит все некрасиво поэтому есть более красивый вариант такая утилита называется linker deoid которая по сути дела предоставляет враппер для ваших основной команды Да и делает shutdown автоматически прокси Ну то есть по сути дела делать тот же самый courl по сути дела На а ну вот вопрос здесь с точки зрения менеджмента Как нам вообще получить этот Бинар во все наши джипы это надо пересобирать до керема Джи там что-то потом перекатывать очень много всего надо делать не очень приятно работа поэтому Давайте искать другой способ Вот другой способ заключается в понимании того как вообще принципе работает подано кубе Да мы можем создать волю пустой название затем сделать Unit контейнер да и просто скопировать нашу утилиту взять контейнер слингер вей там скопировать его наш этот самый Volume и потом уже в основном контейнере при маунтить тот же самый Volume и использует нашу команду для того чтобы в принципе делать shutdown linker de Proxy и проблема будет решена таким образом смотрите вот у нас и будет такая карта которую мы будем заполнять по ходу доклада вот мы пошли использовать linkerdi и оказывается Так что нам уже для того чтобы кран джебы функционировали нам нужно добавить еще linker Down такую утилиту и принципе по ходу доклада сюда будут появляться еще разного рода дополнительные инструменты и Open Source решения Поэтому в текущем состоянии мы победили проблему с кронжабами Теперь давайте разберемся с обсервабилити но многие ставят вообще используют сервис Именно для этого например реализовано в таком экстеншине который называется linker Device него надо поставить отдельно в общем-то ставится точно так же он часто дает он вам то что вы получаете даже борды такие Непростые лаконично но в то же время достаточно функциональные да то есть там есть метрики по ней спейса есть Реал тайметрики для определенного сервиса можно смотреть визуализацию потоков запросов это все работает достаточно хорошо скажем так особенно если вас маленькая количество клоунов все эти метрики они физически хранятся в прометеоусе поэтому который там бандится вместе с Exception и соответственно Вы можете туда подключить графа новский даже борды обычно и получить унифицированные метрики да там по золотым сигналом для ваших приложений Что называется бесплатно без модификации кода приложения но так как мы используем прометеус то у нас возникает следующая проблема что-то конечно не бесплатно одно дело Когда у вас там учебное 10 ворклоудов другое дело когда у вас там ворклоуда в сотни они генерируют огромное количество сэмплов за день и проблема в том что эти сэмплы еще и высоко кардинальные высоко кардинально достигается тем что по типичная Метрика линкордида которая отбрасывает прокси она содержит себе название подсос и под Destination ну и соответственно так как мы знаем что генерация имен подсоса под destinational такой Рандомный процесс то у нас получается высокая кардинальность чем больше поводов чем больше связи между кодами тем больше соответственно кардинальность Ну и с течением времени она только растет Обычно вот поэтому решение например Вы можете держать данные там например в течение часа это максимум что оно может там уже 16 Гб более оперативной памяти все равно стабильности латенсии в общем-то оставляет желать лучшего Если попробовать использовать какие-то менеджер решение например тот же прометеус для от Гугла дату Монарх то это дорого относительно медленно работает и может вообще не заработать Потому что есть у монарха например ограничение на 55 лейблов и вот вы закатив метрики можете увидеть такую картину что просто Google отказываться работать с метриками от linkerdi поэтому в этот момент как бы нам пора окунуться в чудный мир Open Source и разного рода решения и начнем мы тоже с прометеуса да то есть первая задача которую будем делать это уменьшать количество лейблов И если в этом случае нужно найти место где у вас конфигурируется в общем-то конфликт и увидеть что порекомендованному конфигу от ребят делается следующее манипуляция То есть он берет все лейблы спода который только может запихивает их вот такой темпаре маят и потом соответственно кладет их в реальные там лейблы до этого он совершает какие-то определенное количество действий соответственно самый простой способ и эффективный зарядить лейбл это просто убрать вот это вот штуку да то есть не кидать лейблы пода в лейблы до метрик мы теряем конечно при этом часть какую-то информации но по опыту она не Критична но зато это позволяет действительно уменьшить лейблы и уменьшить кардинальность в том числе но это не все не этого недостаточно да и прометал все равно работает медленно Иногда вы можете видеть разного 504 гидвой тайм-аут ошибки Поэтому нам нужно посмотреть какие метрики у нас высоко кардинальные для этого есть такая значит такое запросик который выдаст 10 наиболее кардинальных метрик и топ-1 в ней будет такая Метрика называется respons lightnings bucket и как можно понять это letency ответов сервиса внутри сервис это гистограмма у нее очень высоко кардинальность достигается это высокая кардинально за счет того что используется дополнительные бакеты то есть не только там 1 10 тысяч и так далее кратное десяти но и соответственно там 220 40 тысяч 50 тысяч и так далее Ну и понятно что как бы все это перемножается им мы получаем очень высокую кардинальность соответственно чтобы с ней побороться нам нужно добавить наш конфиг такую штучку чтобы дропнуть вот эти все дополни киты которые начинаются на 2 3 4 5 и в точности Конечно можно немножко потерять но это опять-таки не критично Зато кардинальность падает очень сильно в пять раз и с этим уже можно как-то работать и даже при этом в любом случае даже если вы так сделаете то скорее всего в какой-то момент кардинально будет высокой прометеос будет давать данные не очень быстро поэтому имеет смысл посмотреть на решение которое заточены на то чтобы работать с высоко кардинальными метриками такими как например Виктория метрикс да то есть она достаточно просто заре плейсить заменить прометеос в данном случае этим решением и плюс еще является в том что конфликт Виктории метрик Да он в общем-то там совместим с конфигом про митоза который у вас уже есть единственное что стоит включить парс чтобы те проперти которые поддерживались вы в конфигеть удалили сделали его чистеньким увеличить количество максимально хранимых серий и уникальных сериях потому что любом случае скорее всего эти границы так или иначе достигните дефолтные которые идут внутри Виктории метрикс и на тех же самых ресурсов на который вот запускался например бандл про митоз мы вместо одного часа метрик теперь можем держать 30 дней что в общем-то кажется хорошим им правнутом Да но что если нам нужно больше если мы хотим иметь например какие-то исторические данные то давайте как вы их тоже сделаем чтобы хранить исторические данные нам нужно какое-то другое решение скажем тот же самый Монарх Да от Гугла скажем менеджер и мы можем из них поставить Монарх exporter который в общем-то будет скрепят метрики из Виктория Метрика сладкий Google Cloud точно также можем поставить Монарх фронтенд и подключить к нему графа Но для того чтобы в графане работали какие-то даже борт которые нам нужны вскрывать Понятно бум будем не всеми таки только нужные соответственно в данном случае например скребется метрики по количеству запросов Да к сервису и времени ответ этого вполне достаточно Таким образом мы уменьшаем кардинальность метрик и метрик становится мало сэмплов И решение которое значит если делать в прямую в лоб дали туда все метрики оно за ним будет вас пилить например там тысячи это может быть десятки тысяч долларов данном случае будет белить например там сотни всего лишь сотни на долларов То есть там разница в 10 и более раз что кажется хорошим проводом Окей помните нашу вот эту карту Да у нас вернее слингер уэйд и тут же добавляется разного рода решений которые нам тоже нужно знать как ты их поставить с ними работать и так далее Окей значит мы проблемы решили наши даже барды теперь работают стабильно она у нас продолжает какие-то вещи не работать и следующая на очереди это распределенная трассировка делается она с помощью такого extention который называется linker Jagger И основная проблема любой трассировки распределенной в том что вам нужно делать проподейшен нужен правильный хидеров скажем так и линкер не умеет например работать только с форматом Zip кино X B3 и все и больше ни с каким соответственно в любом случае вам нужна какая-то доработка приложения обычно делается с помощью либо инструментацией либо с помощью проброса заголовков мне например нравится второй вариант просто сделать приложение заголовках а все остальное Пусть за нас сделает сервис у нас будет такая схема сбора трейсов на примере например вызывает сервиса сервиса вызывает сервис сервиса в общем-то умеет делать про Pocket Edition данном случае Engine X и нити трейсы и начинает химичить в оппоненметре коллектор Application просто напросто пробрасывают три заголовки отбрасывает спаны соответственно протоколу sensus и дальше тот же самый оппонентный коллектор Сбрасывает спаны В Джаггер тут надо помнить что у нас разнообразие протоколов Что например Несмотря на то что формат будет у вас xb3 и linkerde имеет Именно его ожидает Да в прокси и работает именно с ним но в то же самое время отбрасывает метрики он по протоколу Open sensus например вот в данном случае config enginex достаточно просто модифицируется мы включаем компрессинг не забываем о том что нам нужно включить значит соответственно именно zipkin до коллектор прописываем к нему там адрес sample Raid если нужно можно добавить разного рода там теги которые тоже будут работать например если у вас есть риквеста или кастами ради какой-нибудь и дальше нам нужно уже работать с нашими трейсами То есть опять-таки если мы это просто так оставим то у нас будет некоторые проблемы Ну например вот вы здесь видим что в принципе на самом деле один URL и по нему надо делать статистику А вот зипкин или Джаггер они не об этом не знают и вас просто будет mess в ваших VII да То есть вы будете видеть каждый такой запрос что в общем-то не очень Поэтому нам нужно сделать как-то так то есть Нам нужно обработать наши трейсы Да чтобы они были в каком-то унифицированном виде Согласно тому как вообще у нас построен например те же самые поэтому телеметрии конфигурацию Нам нужен залезать и делать Для нее планы обработки трейсов Да не забывать что у нас нужно два ресивера то есть один для зипкина который использует например один процент который использует уже самое важное здесь в процессорах это поставить корректно ими сервиса для линкер диспланов да то есть такие сервис точка почему потому что иначе у вас просто будет везде написано и так далее Что в общем-то не очень желательно сделать некоторую опускаться урлов Да чтобы не дай Бог там ничего не оказалось секретного Там и так далее мало черным разработчик может них написать сделать некоторую обработку спанов Ну например брать те же самые Циферки схлопнуть их до такого айдишника до одного то есть мы знаем как у нас например ID генерируется какие-то паттерны Ну и за экспортировать тут Однако возникает проблема с тем что дефолтный Джаггер который поставляется это обычный all in One который все хранит мемори и соответственно это ограничивает вас количество символов которые вы можете хранить во-первых во-вторых но понятно что при каждом старте все будет теряться поэтому вам нужно тащить в общем-то какой-то свой Джаггер тащить к нему БД например в нормальную Кассандра ластик Search или там тащить что вам больше в общем-то нравится в данных решениях ну здесь опять-таки можно использовать какие-то менеджеры решения Но тот же например Star Driver Вы точно также можете скажем Google Клауд э-э экспертиметрики для этого достаточно указать другой экспорт и в общем-то его настроить Да корректным образом чтобы там работали разного рода сервисный аккаунты и у вас был вообще доступ и в конечном счете мы получаем вот такие вот в общем-то красивые рейсы да то есть видно что тут у нас разного рода сервисы написаны так далее про линкорде прокси делать общем то нет ни слова Но с минимальной модификации опять-таки приложений и все это дело прекрасно работает линкер и здесь справляется после работы напильником в данном случае вот мы снова видим нашу Да вот эту карту и на ней теперь появляется новое решение такие как драйвер трейсинг Да Джаггер с которым Мы работали Open телеметрии тоже самое коллектор Вот и если посмотреть на текущее наше состояние то мы увидим что у нас уже много что работает и остаются политики авторизации Можно спросить почему не в рабочей не рабочих в общем-то секции Сейчас будем разбираться Итак политики авторизации значит что это и зачем это надо если посмотреть на типичную микросервисную архитектуру то можно увидеть вот какой-то такой граф да то есть видно что здесь много сервисов есть много связей между ними и соответственно порождает некоторые проблемы с точки зрения Security Ну то есть ваш уровень защиты на самом деле определяется уровень защиты самого слабого звена и представим что у нас есть фронт есть несколько бэкендов и хакеры приходят Как ваш контент И после этого если у вас нет никакой авторизации какие-то приватные то можно за фигчить любые данные оттуда Включите как-то с ними работать нанести ущерб если у вас еще шифрование трафика внутри контура нет то скорее всего здесь задача хакера просто еще больше облегчается скажем так вот поэтому авторизат политики авторизации они делают так что вас ограничивается некий доступ к сервисам Вы можете ограничить его более того Так как использование того что шифрует ваш трафик внутри контура и тоже осложняет жизнь нападавшему в интерьере в традиционной политике появились начиная с версии 2.11 и были довольно простые То есть тут есть такая терминология которая нам сейчас придется использовать чтобы до было понятно следующий слайды Соответственно что сервер когда мы говорим чаще всего это подразумевает порт приложения в этой терминологии сервер авторизойший на по сути дела некоторая политиков то реализации для сервера вообще почему в принципе политики авторизация по умолчанию работают да то есть каким-то по умолчанием Работают но в дефолтном режиме у нас разрешены Все ко всем подходы то есть неважно Вы внутри контура не внутри никакой авторизации не требуется и в принципе по задумке авторов и начиная Сникерс версии 2 12 Действительно Так вы можете включить так называемый strictem tls Mod И после этого у вас будут доступны только Connection и которые внутри сервис но никак иначе в версии 2.1 цена как было проблема с Health Check она была в том что как только вы включали этот strek not у вас все осыпалось красным потому что куплет не мог авторизоваться куплет получал 403 но и понятно под юри стартовали что в общем-то было Ну и использовать этот мод было в принципе невозможно поэтому решение проблемы с халчеками состояла в том чтобы мы добавляли для специальной значит сервер для портов наших приложений сервер для портальный админ и писали для неё некоторую полисе которая этих разрешает как минимум нам нужно сделать например политики для порта приложений для порта linked Proxy Да соответственно для этого мы делаем сервер Ставим на него определенный лейбл это важно да например что в данном случае этот сервер Type comman на него будут навешаны впоследствии comman политики и мы с помощью под селектора мы матчем все приложения которые так или иначе внутри меша Да по там нотации Control Plane Nest Там и так далее либо любой другой который Вам нравится затем нам нужно сделать политики для куплета в данном случае это кастом сервера авторизации которого есть селектор который мочит опять-таки по лейблам например комом да то есть мы таким образом выбираем все сервера которые у нас есть с таким лейблом и затем нам нужно авторизовать куплет и тут начинается уже платформа специфические вещи Ну они зависят что конкретно вы используете например приходит запросы с него по схеме вида там 10 нм132 например таким образом нужно сделать какой-то template он выглядит не очень красиво но работает при этом Вот и также помимо этого нам нужно авторизовать экспрессивно всем thls коннекции то есть мы говорим что любой клиент С каким он не пришел все таким образом можно зафиксить проблема с чеками тут Главное понимать что когда включается этот мод например что у Вас например какой-нибудь JX или прометеус уже внутри сервис меша иначе может получиться Так что они тоже не будут иметь доступа и там хорошо Если вы метрики не сможете получить гораздо хуже Если вы на странице в приложение увидите 403 причем как бы который вы совсем не ожидаете значит следующий мод это динамод который запрещает все кроме того что разрешено в данном случае его стоит Включать тогда когда вы уже полностью настроили Все все политики последний шаг Давайте дальше мы будем разбирать такой вот реальный нереальный пример у нас вот есть несколько нспейсов Да тут видно что есть Ingress есть какие-то сервисы могут ходить каким-то какие-то нет Если вспомнить нашу схему с чеками Да вот в которой мы добавляли то если нужно добавить специфичный полисе скажем так мы не можем просто взять и для всех например сделать один сервер для всех портов всех там сервисов и навести на него потому что для каких-то сервисов она должна применяться например для каких-то нет помимо этого мы не можем взять второй сервер например для какого-то порта затем Нужно обязательно следить потому что данном случае у Вас могут быть просто не будут понимать что происходит и данном случае Вы будете получать ошибки на которые вы совсем не рассчитываете то есть нужно запомнить такое правило что для каждого порта должен быть один единственный сервер никак иначе И вот уже соответственно этот сервер мы можем накладывать любое количество авторизационных политик да то есть мы можем взять эти политики с холщёками какие-то общие политики и какие-то политики специфичные для данного сервиса Ну например какие-то могут быть политики Скажем мы можем авторизовать чтобы получать метрики в данном случае для всех серверов которые помечены комом Ну а скорее всего плюс-минус вас это все серверы в системе вы говорите О том что мы вот эти про металл сам и его разрешаем аналогичным образом мы можем сделать например для engines единственная разница здесь в том что как бы имеет смысламить другую какую-то лейбл например и делать эти политики только для тех сервисов которые реально нужно выставить наружу и для какой-то простоты Например если вас какая-то изоляция внутри на emspace Вы можете сделать политику что вот запросы внутри name Space А вы можете ходить друг другу сервисы могут ходить Ну а Извне там соответственно какие-то уже другие политики например как в данном случае через какой-то гид вей Да внутри давать доступ номер только к нему скажем от сервисов из других namespace Вот Но в данном случае как видно некоторые сервисы все-таки сервис 2 тут не может ходить к сервису один Несмотря на то что ни в одном на спейсе поэтому Казалось бы ну давайте сделаем Просто политику которая будет говорить что сервис один может ходить к нему может ходить только сервис 3 например и все как бы все достаточно просто но проблема в том начинается тогда когда у вас там условно сотни сервисов и нужно сделать Сотник политик и в данном случае обязательно эти политики нужно шаблонизировать их нужно генерировать на этапе например установки приложения через схему Есть два вида конфигурации значит первые дефицит на конфигурации всегда когда вы говорите что к данному сервису может могут ходить только определенные какие-то сервисы то есть мы их знаем мы их прописываем и такой способ он его легко понять его легко шаблонизировать но нужен идентифицировать всех клиентов Что может быть сложно усложнить значит само по себе конфигурацию и вообще в принципе Этот способ выглядит не очень нативно но зато Вот получается очень простой конфигурация понятно что для всех например там каких-то сервисов Да по какому-то лейблу мы разрешаем определенные другие в данном случае нативная конфигурация выглядит по-другому да то есть мы говорим что у сервиса есть деппенденции обычно мы знаем каким сервисом наш сервис ходит и соответственно мы можем их прописать Однако это более естественный способ писания но Тут нужно решать задачу обратную задачу по сути дела стройте обратный Граф И для этого уже генерация например политик будет более сложная скажем в данном случае нам нужно создать специальную политику зависимости для сервиса 3 назовем ее сервис 3D так как политики работу только в рамках одного name Space нам будет нужна дупликация этих политик по всем нам спейсов которых могут быть потенциальные соответственно для них мы Поэтому нам нужно делать цикл по некоторым Property Spaces затем политика вешается именно на зависимые сервисы а не на того кто вызывает Да с помощью тех же самых селекторов и мальчик спрашивает данном случае мы смотрим есть авторизационно политика некоторые там лейбл Есть в наших depender значит не он вешает полити и эта политика получит по сути дела авторизует целевой сервис выглядит немножко сложновато но реализуемо и получается достаточно красиво решение по крайней мере с точки зрения описания этих деппенденций разработчиками вот поэтому что нужно Здесь держать голове что у нас все политики применяются только в рамках одного namespace поэтому будут требоваться дубликаться политик поэтому желательно менеджмент делать весь через схем что у нас для каждого порта может быть только один сервер Это должно быть одна к одному до relation и помнить про платформа специфические вещи такие как например там цифр куплета Откуда приходит запросы например потому что они тоже могут приходить тех цифр которые не подозревали скажем так до тех пор пока сервис мышь не внедрили начиная с версии 2.12 появились новые фичи например такая httpe Round новый ресурс который позволяет авторизовывать запросы и делать политики к конкретному пасу на порту сервиса и появилось разделение серверов который стал дебрикейт на три новых ресурсов поле семестр не только схема политики чуть сложнее соответственно то есть Теперь мы можем навешивать на сервер так называемый и к нему уже подключать два идентификации либо это сетевая аутентификация либо это аутентификация по мечом ЛС соответственно также появляется http rout который на который То есть вы можете взять какой-то там URL например API конфиг и накрутить на него традиционный полис и таких раунтов их может быть много для одного сервера в чем плюс вообще принципе Раута в том что он позволяет делать классные вещи Например если у вас есть два Ingress например внутренний внешний вы внутренне Вы можете разрешить все запросы к серверу например а для внешнего ингресса только на какой-то URL например то что должен торчать наружу только и все то есть ко всем остальным пасом он этот внешний ингресс доступа иметь не будет если например он будет взломан то опять таки хакер сможет сходить только на какой-то например и паблик которого скажем защищен Однако как только вы добавите но где-то себе раунд хотя бы один для сервера то вы столкнетесь обратно Опять с ошибками с хел щеками потому что куплет у вас снова не будет авторизоваться происходит Это потому что по умолчанию LinkedIn для вас уже создает htperal на самом деле имприцитный который авторизует чеки Но как только вы добывая добавляете новый то имприцитный больше не работает и поэтому вам нужно сделать эксплицитный раунд и политики для соответственно решение проблемы с хлорщиками Похоже на то которое мы видели ранее только нужно создать теперь новый ресурс называется Network offication например мы будем авторизовывать некоторые кластер White Network опять-таки в данном случае она платформе на зависимая Ну для ГК е мы уже знаем В принципе как у вас генерировать затем нам нужно создать http rout который мы навесим на холще то есть да Его нужно прикопать к нашему серверу и или там набору серверов например да и соответственно затем указать По какому URL в принципе у нас находится халчек Ну данном случае И после этого на собственно говоря создать авторизацию полисе которую мы при оттачиваем к httperal то и навешиваем на нее то что вот у нас есть наш Нетворк аутентикейшн Да чтобы запросы билета были авторизованы Вот и в принципе тут все классно но вот если здесь посмотреть У нас есть такая штука как таргетров если вы могли были внимательны ее где-то увидели вообще что такое таргетров это по сути дела некоторые референс на ресурс Да из другого ресурса Но в данном случае например resation policy может при авторизоваться вот авторизуется тут к httperal к некоторым и в целом таргетров может вести На что На тот самый http раунд на сервер и на весь Space в целом и это на самом деле большая проблема потому что теперь нет селекторов то есть то что нам позволяло удобно выбирать какой-то субсет до серверов теперь отсутствует теперь нам нужно все как-то прописывать и вот такая штука например как нативная конфигурация ее становится сложно очень сделать у нас получается много дупликаций политик сложный менеджмент много серде ресурсов и так далее это все нас ведет к тому что у нас начинается проблемы использования такого массированного значит авторизационных политик скажем у нас появляются разного рода например ресурсы которые уже стали не актуальными такое тоже такое может быть например да у нас возникает проблема с обсервабилити чтобы понять например вообще какие Какая вообще авторизация чем вообще там накрутили и так далее особенно если у вас есть какие-то проблемы возникают То есть например где-то пошли 403 ошибки и важно еще об этих 403 ошибках узнать поэтому Нам нужен еще и мониторинг всего этого дела с точки зрения описали ресурсов мы можем использовать extention который из команда чеки он за вас придет про Человека эти ресурсы какие-то рекомендации Ну типа Например у вас для порта нету сервера А вот тут авторизован полисе который никому не притачен А вот тут и сервис сервер без каких-то политик что в общем-то странно Там и так далее Вот можете как-то это дело зафиксить Какой инструмент диагностики также с точки зрения обсерва берите есть команда называется на УЗИ до который вы можете скормить например на какой-то дипломмент который вас интересует в данном случае и получить какие у него есть сервера сервера соответственно какие есть политики авторизации да то есть вот видно что здесь есть и старая антипрет от сервер от resation есть новые авторизишн полисе Если у вас есть раунд политики какие-то то они тоже будут здесь прописаны вместо звездочек будут конкретные пасы для сервера которые которым отдачатся все эти авторизационной политики есть еще более быстрая команда То есть это аналог команда УЗИ в exentions она на больших барклаудов примерно в 10 раз больше 10 раз быстрее отрабатывает условно говоря вместо минуты будете ждать вот а значит с точки зрения мониторинга нужно добавить валерт менеджер конфигурацию соответственно Ну например смотреть что у вас увеличилось количество динаев то есть Метрика которая говорит что у вас есть Дина и по tcp dynain прям почти обе эти метрики Нужны мониторить но и соответственно выводить себе Вот как не так вот скажем флаг Ну и если вас долгое время эти политики например увидите долгое время Дина это логично уже создавать инцидент и говорить о том что что-то идет явно не так где-то какая-то Мисс конфигурация либо какие-то баги либо где-то неправильно в традиционной политике прочитались и прочее вот Окей в текущем состоянии Мы видим что у нас все работает теперь все зелененькое все что мы хотели мы все сделали и на нашу карту вот на эту добавляется еще extension nickerde Easy aus который позволяет проводить диагностику и Hell Да с помощью которого мы в общем-то организуем чарта для наших приложений чтобы политики авторизации генерировать да то есть не делать их вручную ни в коем случае поэтому Давайте здесь сделаем некоторые выводы по нашему докладу Ну во-первых линкер декор то есть Core функциональность она действительно просто работает в принципе базовые вещи как там еще лтлс балансировка запросов они гораздо соответственно они работают из коробки там достаточно простая документация вы просто их читаете делаете там ну каких-то особых проблем и подводных камней Нет с точки зрения опять-таки но требует изучения экспертизы в области счет пати решения такие прометел с Виктория метрикс и так далее Также желательно разделять Да соответственно сразу перейти к разделению метрик оперативных например там для Реал тайма использовать соответствующие решения и Лонг Терм 40 что для метрики исторических использовать какой-то другой например решение Ну тут зависит опять-таки количество вашего знаний Какие решения Выберите и так далее довольно большой Простор но в то же самое время об этом стоит думать всегда помнить Что распределенная трассировка обычно требует доработки приложений и для того чтобы делать красивые трейсы именно с помощью linkerdi да вам нужно знать вообще как принципе работы топителем 3 коллектор настраивать для него правильный pipeline в соответствии с тем как Какие трейсы вообще в принципе вы пытаетесь лить Ну и иметь какое-то решение которое будет хранить эти самые трейсы понятно что дефолтный Джаггер который идет бандится с линкер для продакшена не очень подходит вот Хотя для на первое время это может быть и норм решения скажем так а авторизационная политики они хорошо работают каких-то простых случаев это как бы есть некоторые проблемы например с теми же самыми халчиками в любом случае не так или иначе требуют кропотливой работы да по их унификации для того чтобы потом сами не запутались что у вас где мониторинга Да за ними соответственно чтобы разбирать какие-то инциденты связанные с ними и помнить что они имеют ряд ограничений настоящий имеет некоторые двойственность не совсем понятно Вроде бы как бы сервер от resation он объявлен depricated но пока не выпилен и с новыми политиками есть некоторые проблемы с точки зрения гибкости их применения Вот и поэтому скорее всего они будут как-то эволюционировать и как-то меняться и думаю в следующей версии там словно два 13 что-то еще поменяется и какая-то часть доклада например станет не актуальной момент вот и самое главное вывод который хочется здесь сделать помните нашу карту Мы начинали с того что нам нужно было внедрить сервис который просто работает а по факту у нас получается кроме самого сервиса мы должны разбираться и понимать нужно хотя бы на таком хорошем инженерном уровне достаточно много разного рода вещей Да скажем там И как работать подданные в губернатос правильно и как собирать метрики и куда их сложить Какие решения эффективны какие нет Писать править конфиги в них разбираться то же самое можно сказать про трейсы разбираться глубоко в авторизационных политиках и даже писать свои собственные решения потому что готового инструментария например по чеку описать ресурсов в ванильном кардии их например силой Туле их просто-напросто нет Ну на этом в общем-то хотелось бы доклад закончить и опять-таки Добавляйтесь ставьте лайки звёздочки и так далее оценивайте доклад и есть у нас временно вопрос какой-нибудь один я думаю можно успеть ответить да спасибо Саша очень плотный информационно доклад и слайды с невероятной скоростью мне кажется если бы я не раньше не успел посмотреть до доклада я бы с трудом успевал за ними следовать но хорошо что любой человек может как я сделать также и скачать слайды а-а на странице доклада и посмотреть их скопировать оттуда код или его изучать посмотреть как там всё называется А у нас действительно есть буквально пара минут на вопросы и вопрос у нас как раз один есть Я думаю мы успеем на него ответить в эфире А все остальные с вопросами могут прийти к нам дискуссионную после доклада А вопрос Это от Александра Михеева который кстати пишет что доклад офигенный и благодарит Саша тебя за офигенный доклад А вот вопрос про aw Слот балансер WS Application lot bounter по моему как его авторизовать то есть вопрос как бы из блока доклада про авторизацию Я не знаю у тебя есть Саша экспертиза по WS нет экспертиза такой глубокой по WS но например Я примерно представляю скажем с Google клаудами там тоже есть собственные условно говоря Лола балансер А в любом случае есть всегда как бы опять таки это платформа специфическая вещь например То есть вы должны посмотреть С какого айпишника приходит вам запрос от этого балансера если вы можете определить сыр Вы можете его авторизовать Да по политикам условно говоря написать Network от resation соответственно и авторизовать по какому-то цифру Если вы узнаете если вы не знаете ли он Рандомный то к сожалению Вам это вам это не сделать и то что вы можете реально сделать Вы можете поставить какой-то Ingress между как бы вашими сервисами и балансером тем же самым известным в нашем случае мы вот рассматривали на примере ng-микса то есть прекрасно работает схема когда есть угловый лот балансер внешний и он балансирует запросы между там условно инженексом который находится внутри кластера cubernetes и уже начиная с инженекса вы накладываете политики Ну то есть мы считаем что в инженекс приходит все запросы и мы все равно все должны так или иначе обработать Ну а к ним доступ Понятно У вас есть столько слова вашего балансера по другому запрос прийти не может ну а дальше уже начинается вы строите политики авторизация так как вашингресс он внутри межконтура А по-другому у вас политики авторизация просто работать не будут на 100 там уже нет никаких проблем то есть уже когда делает запросы у него уже есть А дентите и В этой дентите можете авторизовать Но вот такая схема с прокси Она работает вполне себе хорошо Ну то есть по сути по сути рецепт в том чтобы во-первых сначала посмотреть документацию WS и понять нет ли там какого-то более конкретного там диапазона до айпишников описание Откуда приходят эти запросы Чтобы прописать их вручную себя в конфигурации если это не находится то тогда просто Давайте этот лот балансер Эй даблэсовский как некого внешнего клиента который тебе приходит и нужно Между ним и сервисами поставить просто свой ещё один Ну да тут вопрос же как бы в доверии То есть если вы считаете что как бы ваш контур можно попасть только с этого ЛБ но по сути вы правильно организовали сеть то вам дополнительно авторизация не нужна то есть политики авторизация таких основное назначение это Zero trus policy уже внутри контура тогда когда к вам запрос вашего контура уже пришел и вот что дальше с этим запросом делать Кого куда можно его Отправить куда нельзя и так далее Вот это уже про политики авторизации скажем так больше это все-таки про внутренний контур чем внешний Ну если вы используете опять-таки extention мультикластера например то есть когда вам нужно там несколько классов вопрос его не было в докладе нас губернатоса то тогда политики авторизации еще вы можете делать так чтобы авторизовывать запросы из другого доверенного вам кластер Да особенно когда их там скажем множество Это вопрос как бы здесь нет но такие политики можно сделать Вот Но это требует как бы еще какого-то слота доклада для того чтобы объяснить как это Как это что-то работает но в принципе она есть можно заморочиться и сделать слота на еще один доклад У нас сейчас нет Поэтому я предлагаю обсудить мульти кластеры все остальные классные интересные вопросы которые есть у меня и у всех остальных слушателей дискуссионной зоне а фирмы на этом заканчиваем Всем спасибо за внимание Жду вас дискуссионки Всем пока Спасибо что пришли"
}