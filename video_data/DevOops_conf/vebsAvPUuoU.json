{
  "video_id": "vebsAvPUuoU",
  "channel": "DevOops_conf",
  "title": "Владимир Медин, Сбер — Синхронизация производства. Скорость, надежность и простота артерии DevOps",
  "views": 148,
  "duration": 1890,
  "published": "2024-12-02T01:25:50-08:00",
  "text": "Меня зовут Владимир Медин Я из Бера мой доклад называется синхронизация производства скорость Надёжность и простота артерии ПС о чём И чем полезен будет наш доклад мы поговорим о сложности тривиальный задач в м Enterprise может быть попробуем оправдать его а может быть и не будем совсем расскажем про архитектуру решений High devops и собственный опыт который мы проходили объясним ценности служебных сервисов devops и важности их качества коротко об окружении Да периметр сетей банка поделён на изолированные сегмент сегменты в свою очередь поделены на сетевые зоны сетевые зоны принадлежат соответствующему этапу разработки программного обеспечения в каждом этапе есть инструмент хранения будь то код или артефакт какую проблему мы решаем Да из-за изоляции требований к системам хранения к сегментам артефакты и код дублируется от системе к системе среда разработки жёстко изолирована от среды эксплуатации производства это последовательная цепочка этапов между системами хранения А проблема у нас отсутствует автоматическая последовательная передача артефактов кода от этапа к этапу а схематически отображён предстоит У нас есть простейшая схема есть пункт а пункт Б есть файл который нужно переместить из одной точки в другую точку мы добавляем сюда сетевую изоляцию У нас есть две зона разработки и зона продакшена и сетевая изоляция абсолютно и на самом деле это не просто пункт а пункт АС в котором хранятся артефакты кроме несса У нас есть ИТ Для чего нужно перемещать артефакты в нес между Нексуса и между битами я расскажу чуть позже в целом этот цикл у нас подразумевает в пункте ается сборка Эми и и в дальнейшем нужно произвести если привести к нашей привычной схеме Да схеме цикла производства автоматизации весь этот процесс должен быть автоматизирован через integration dely кстати очень любит сокращение и dely мы называем C CDP влом у есть по этого всего бено куда я думаю многие знакомы со стандартами PS здесь история плюс-минус похожа и сегмент pss тоже в банке обязательно присутствует Так мы обрабатываем карточные данные и у этой сетевой изоляции есть ограничение да Передача она в принципе возможна но только проходя два обязательных параметра аргумента возможно только системное взаимодействие то есть пользователь не может работать с этой сетью с этой изоляцией только Доверие системе второе ограничение только ме пакета нельзя перемещать всё что угодно и как угодно только система и только ограничения по размерам в целом мы на этой схеме отображая весь цикл да ту самую артерию дипс которая происходит слева у нас источник события это у нас ННС в случае формирования пакета дистрибутивов для Нексуса это у нас инженерс инженер да либо разработчик который свой код конфигурирование разние складывает в Мы из точки А в точку перемещаем ВС Это для потребления в продакшн зоне для деплоя И конфигурирования всего того чего мы сделали Метрика которая контролирует и отображает зрелость всего этого процесса это да Time это момент когда уже задача разобрана Нужно кодить формировать дистрибутивы и как можно скорее доставить их до продакшена есть е много других метри которым измеряется этот цикл эта проблема она влияет на конкретну метрику лед тайма как мы будем решать нам нужно разработать систему синхронизации обязательно обеспечить систему процессами безопасности достичь максимально быстрой работы так как это одна из ключевых метрик devops Ну и конечно же это надо было сделать вчера классически у нас есть требования они могут быть функциональные нефункциональные если по большой группировке у нас функциональные требования - это контракт качества интеграция и контроль целостности Ну то есть мы должны соблюдать SL мы должны быть автономны в сопровождении мы должны работать прозрачно для всех кто пользуется этой системой либо потребляют эту услугу мы должны быть с интегрированы естественно системами хранения между которыми должен перемещаться тот и или иной артефакт и мы обязательно должны следить за целостностью Ну потому что целостность дистрибутива это бы такая не Метрика Да а условия безопасности того что не было изменено посередине что дистрибутив дошёл в продакшн именно Тот который был разработан и который должен принести пользу людям из нефункциональных требований Три больших блока Наверное они классические для большинства систем Независимости это Инженерная система либо продуктовая скорость Надёжность и эластичность Да мы должны ма быстро выполнять свою бизнес функцию потому что мы напрямую влияем на производство всего банка на то как быстро доставляется изменения в продакшн мы должны не иметь зависимости на смежные системы или на смежные системы чёрные ящики которые мы потребляем дополнительно в работе своей мы должны иметь разные каналы по скорости то есть мы должны иметь резервирование чтобы скорость ни в коем случае не уда постит про в голове что допустим не будет работать О мы ВС равно должны продолжать работать ну потому что система достаточно Критична и процесс критичен мы не должны иметь точек отказа и мы должны быть готовы к отказу интегрированных с нами систем Ну потому что нессы бед бакеты - это тоже система имеющие свой жизненный цикл своё сопровождение свои проблемы и мы должны понимать что какая-то система может выключиться мы не должны ВС остальное Весь остальной процесс работы останавливать из эластичности горизонтальное масштабирование потоки растут разработка растёт мы должны уметь обрабатывать очень большой объём данных и причём разносов данных и должны уметь управлять типами потоков Итак на самом деле оно и раньше как-то работало эта проблема она была издана и сетевая изоляция Она наверное в любом энтерпрайз существует Да даже если не Прай а серьёзный подход к разделению управления ландшафтом да сете и как это пыталось работать ранее да это был Это был СШ это было всё что угодно что хоть как-то могло выполнять первичную задачу Вот Но это всё постепенно обрало проблемами Да у нас допустим появляется новый тип репозитория npm там подключили инженера сопровождения SCP У нас появился докер А давайте там включим вот ВТО это кое-как работа но масса не было управления логикой загрузки не было никакой обратной связи доставлен тот или иной артефакт успешно или неуспешно каждый запрос в поддержку уходил в разборы А какой же инженер какой тип синхронизации здесь настроил мы попадали в такие ситуации когда уходил депс инженер вместе с ним приватные саж ключи вообще всё не работало уходили недели на разборы таких ситуаций И самое главное это работало долго Мы приступили к задаче у нас в левой части это источник в правой части получатель У нас есть Nexus A bbet B и мы начали разрабатывать я буду вести схематично каждый сервис как который реализует данную функциональность и буду рассказывать его назначение как мы это делали а при загрузке изменений в Next bitbucket он генерирует вебхуки в хуке есть полезная информация о том что было изменено да В каком артефакте что было изменено кто это сделал Ну всяческая информация это отправная точка начала работы нашей системы у очень простая задача это неконтролируемая нагрузка на самом деле от ивентов мы не знаем В каком количестве каждый день могут загружать артефакты либо в NEX либо код вот мы поставили apate который первично весь этот Шквал принимает на себя крайне простой сервис горизонтально масштабируемый готов выдерживать очень большие нагрузки в дальнейшем классический приме взяли дис и складываем туда в быструю базу данных все эти ивенты Профит Ну это быстро работает Это просто настраивается мы разграничивать нагрузку от ивентов до уже ядрово до ядрово части всей системы Затем в работу включается вокер Он обрабатывает сообщения из очереди стерилизуют их и уже загружает в персистентные уже в более-менее понятном формате для нашей системы для дальнейшей обработки тоже очень простой готов выдерживать большие нагрузки первично начинает управлять приоритетом обработки там достаточно кондовый конфигурационные моменты которые мы уже пробуем распределять нагрузку дальше в игру вступает менеджер диспетчер задач он именно берёт задачу уже из персистентное данных обрабатывает их скачивает обекты из систем хранения источников складывает Уми чанки и отправляет в буферное хранилище Почему чанки если вернуться назад У нас по ограничениям безопасности возможно пересечение среды изоляции вот да посередине пунктирная которая линия только ограниченный размер и только система и здесь таск маджере начинается самая магия то есть берётся исходный артефакт Исходный код комита репозитория и начинает преобразовываться в обезличенной массу небольших количество небольших файлив дальше у нас есть простой сервис трансфер у него очень простая задача грубо говоря две функциональности Get и пост - Это ресто вый сервис который М может отдать тебе файл постом может сложить тебе файл плюс может кое-какую Мета информацию для передачи между сегментами выравнивания окружений передавать простой и быстрый работало затем вступает также в работу таск-менеджер но только на другой стороне на стороне получателя и главная его задача была из обезличенных чанков собрать всё в исходный вид в исходный пакет дистрибутив или образ в исходный репозиторий проверить контроль целостности провести того что ничего не изменилось по пути и прочитать Мета информацию куда должен быть доставлен тот или иной объект соответственно он уже и загружает в конечные получатели изначальные объекты это всё Мы назвали мувер Ну задать мы его так и назвали по сути очень Простая история мо сказать почти из пяти бинаре как у ку Бернеса но не совсем Эта система благодаря своей функциональности может работать не только в одну сторону Она может работать получатель один точнее источник один получателей много у него есть функциональность зеркалирование удаление и довольно-таки много внутренней логики которая позволяет весь контур банка а может быть и немного предела его захватывать и синхронизировать между собой объекты и оно заработало но мы конечно же ошибались Да и ошиблись в нескольких местах самая первая проблема в apate приходят Все изменения из нессо и бит баке тов в эти изменения могут быть мусорные есть изменения которые мы вообще не должны обрабатывать Ну допустим обновление Мета информации которая никак не влияет на конечный дистрибутив или код и нас начало заваливать просто бесполезно истории мы не могли это дальше правильно стерилизовать и завалили ядра Ну в середине цепочки нашу систему также были события которые с первого взгляда были полезными но обрабатывать их не стоило допустим в режиме обычной синхронизации мы не должны обрабатывать пки об удалении объектов тем более с продакшн зоны допустим Ну немного отойти образ Он всегда скачивается в среде исполнения И если мы его удалим через систему синхронизации в следующий момент когда сервис будет переезжать с узла на узел он просто не поднимется как мы это решили Да мы внедрили грубые фильтры Ну то есть очень грубые если грубо говоря не J не принимаем в работу если J но в jy есть поле которое имеет тот или иной параметр не берём в работу Мы в десятки раз снизили лишнюю нагрузку которая сильно паразитировать решение уже описано как итоговое Но решалось это всё поэтапно и довольно-таки длительное время зная сложность задач блокирует всю очередь что имеется в виду под сложностью Ну то есть перенести дистрибутив жарк Да это скачать конвертировать также есть нюанс в бид бате Да вы же надеюсь знаете или может быть видели что скачивание репозитория может зависеть не от того Какое количество файлов и какого размера Они там находятся а от того сколько было коммитов Ну то есть Может быть один файл и 1 миллион коммитов и внутри структура хранения в трепова ет хранение Мета информации уже в стерилизованное задачу и в итоге мы не понимая что сложность может быть разная из-за скорости разности этих скоростей начали начинали ломать полностью всю синхронизацию то есть стопари события в очередях стопари задачи на скачивание на загрузку также у нас были бесконечные петли события действия события Ну то есть любое изменение загруженное в неус би порождает мы загружаем по своей работы Тоже в Nexus или bbet артефакт или код и это тоже порождает webhook первый запуск который мы сделали привёл нас к тотальному самостоятельному де досу Ну то есть мы просто запетляла и начали бесконечно всё между собой гонять А как мы все эти проблемы порешали мы внедрили систему приоритетов с очень тонкой настройкой там от глобальной Какого типа это Nexus Какого типа репозитории bbet до более мелких по сложности Какие размеры должны быть сколько должно быть получателей и сквозным Куи дом начали это всё отслеживать мониторинг отдельная тема эту систему мы долго тоже оснащались разные узлы чувствовали себя по-разному если это сложная задача и там большой файл качается он мог полностью съесть место дисково пространства на том или ином узле таск менеджера и это приводило к выходу его из строя и мы не контролировали сколько на какой уровень сейчас забита файловая система либо уровень загрузки цпу и рама насколько Им удобно сейчас будет подхватить какую-то задачу так как есть система приоритетов нам нужно было уже оценивать Какой узел этот приоритет обработает быстрее и сделали внутреннюю систему селлинг так называемый также мы сделали тонкую настройку параллелизма бовой обработки то есть чтобы можно было обрабатывать сразу пачками те или иные объекты разделяя по сложности по приоритетам мы В итоге добились стабильной конструкции которая очень редко требует своей Дона Рой у нас была проблема И в буферном хранении в трансфере в котором было всего две задачи А я напомню что из нефункциональных требований у нас было обязательно горизонтальное масштабирование и первое решение все трансферы между собой были объединены оним э насам то есть по НФС протоколу это была общая дисковая пространство и мы начали упираться в его производительность Ну то есть используя стандартную инфраструктуру Мы начинали убивать не сам трансфер мы убивали диски которые под ним работают решение оказалось простым относительно мы сделали тупи протокол и трансферы сколько бы их не было знают сколько сейчас в кластере трансферов знают Какое количество свободного места есть на каждом из них и могут между собой балансировать запросы если какой-то из узлов вышел из строя фры которые нано уже не пугают быт напугали ранее продолжают расти при этом очень большими темпами по Некс мы ощущаем нагрузку в виде 2 с поно примерно 2,5 млн артефактов в сутки и у нас всего 24 инсталляции Нексуса и семь типов репозиториев в каждом из них bbet принимает в себя пол милна коммитов в сутки все их нужно синхронизировать по разным правилам у нас 12 инсталляции и два типа репозиториев но это в принципе всё между собой множителем является какой получили мы результат В итоге Ну то есть та картина которая была изначально когда былинки CP drbd Ну в лучшем случае среднее по перцентиль это было 60 минут на один объект Да первое внедрение нашей система снизила этот показатель с 60 минут среднего до 20 минут и текущая работа показывает примерный показатель 1 минута на тот или иной артефакт в Независимости времени суток и при отсутствии очень больших пиковых нагрузок пиковые нагрузки ВС равно бывают но сильно далеко от среднего мы не поднимаемся повлияло это в итоге всё натам Ну то есть мы в 60 раз сократили йм тот кусок когда разработка готова она уже протестирована её нужно как можно скорее до продакшн донести мы его сократили в 60 раз сза что у было желание глобально изменить ситуацию да и это желание оно было у нас изнутри потому что мы сами деос инженеры в систему строили деос инженеры Да мы постоянно использовали свой сервис То есть это был такой интересный момент когда ты сам разрабатываемая ешь и сам потребляет свою работу если работает твоя система плохо то ты изменения этой системы не можешь донести быстро у нас была Свобода действий в принятии решений даже не знаю как это сказать хорошо или плохо У нас не было Архитекторов У нас не было аналитиков мы было просто группа Энтузиастов инженеров которые решили исправить ситуацию в корне и изначально Прежде чем начать эту делать систему мы построили правильный дес внутри команды Ну то есть чтобы Мы понимали мы не будем отвлекаться на это у нас есть чёткие сборки мы знаем безопасность мы умеем это всё быстро тестировать Мы доносим свои изменения пока иде же не угасло в нашей голове ну и плюс первые запуски они приводили к деградация это имело большое влияние И чем быстрее мы могли сами доставить на пром тем лучше нам было потому что альтернативы этой системы уже практически не было на самом деле эта система кроме пресловутых пяти бинаре она ещё обросла другими вспомогательными сервисами со временем сервисы эти необходимы для того чтобы в целом объект строился в ландшафт банка да то есть у нас в инфраструктуре очень много инструментов которые обеспечивает производство Да и Инженерный опыт и инфраструктурный опыт и опыт автоматизации и безопасности У нас есть дополнительный сервис midle который занимается публикации реста к нашем к нашей системе управление ядром управление доступом управление направлениями чтобы это было всё полностью автоматизированном режиме У нас есть естественно панель администраторов webui у системы есть несколько линий сопровождения через которое можно управлять работай всей системы отслеживать её работу и в случае критической ситуации подключаться и оперативно выходить из клинча также так как одно из требований функциональных была прозрачность работы системы для всех участников обязательно было до оснащение системами нотификации Да системы уведомлений это и почта и сберчат и всевозможные там есть даже системная интеграция когда в системе можно получить ответ Если целенаправленно самому включать синхронизацию У нас есть такой функционал если какая-то деос команда хочет сама управлять этим циклом и это стало намного сильно влиять на количество тикетов там на первую линии сопровождения и многие вопросы стали решаться намного быстрее а и всё это завершили с чат Ботом Одна из последних разработок которую мы добавили в этот сервис это бот в нашем корпоративном мессенджере с искусственным интеллектом он обучается на базе вопросов и ответов на нашу первую линию сопровождения и предоставляет быстрый ответ также помогая сориентироваться что как где дела Где ваш артефакт На каком этапе Как долго ждать или быстро выйти на первую лини сопровождения оповестить о проблеме а заключения и наверное напутствие не все инструменты devops можно найти github of doer Ну точнее там можно найти многое но это всё будут какие-то элементы части систем Да какую-то проблему порешили Здесь и сейчас если проблема выходит за рамки одной функциональной зоны github и docker уже не помогут нужно будет придумывать своё там где вы придумываете своё и не работает практика нужно включать фантазию пока мы проектировали эту систему и разрабатывали многие вещи которые мы туда вносили мы знали и получали из опыта работы с базами etcd с ку бернем с редисом сопровождение каких-то конечных прикладных систем через автоматизацию циклов уже нескольких команд или групп команд и это фантазия нам позволила всё самое лучшее что мы в своём опыте наработали внедрить в эту систему и наверное лучший сервис делает конечный пользователь я говорил там где что нас привело к успеху это то что мы сами пользовались этой системой и это позволило нам сделать е в короткие достаточно сроки влечение дополнительных ресурсов на этом У меня всё спасибо большое а так вопросы наверное А вопросы вопросы вопросы лес рук не вижу никого вот человек молодый Добрый день спасибо за доклад у меня вопрос касаемо икера Почему вы не смотрели тот же Т Например почему выбрали у вас я так понимаю работает как Или как именно как из нашего наверно болезненного опыта мы много раз Спока его работоспособности он нам не нравился как решение Вот И там где зани этим было положительном имен Как таковым это выбрали Не потому что зал в интернет нагул дис везде написан а потому что это было Из нашего опыта лучшее решение и он нас не подвёл ни разу по большому счс там на картинке он падает и всё недоступно Кстати да на одном слайде я упустил кажется момент сброса очереди на файловую систему и так как он стоит сразу после вот мы вте внедрили внутреннюю очередь на момент если гасил чтобы мы внутри в сервисах на файловой системе сбросили точно также события чтобы они не были утеряны Ну то есть падение редиса приводило к безвозвратной потере потому что у себя персистентное по приме на сча отключения при воз его в рабо заново закивал очереди на место события в одном экземпляре в одном экземпляре работает нет нет это кластерный режим я не помню какой именно но они бывают отключается Ну любая инфраструктура она может выйти из строя по той или под Ну то есть выключился допустим развалился получился Spin пока ты чинишь Spin у тебя допустим база данных только на чтение работает ну это я просто сейчас пример из головы говорю вот и ты с этим ничего не поделаешь у нас проблемы в банке каждая система каждой системе присваивается класс критичности по классу критичности есть требования по надёжности которые надо реализовать и там сист вы от Ну потому что это бывает перебили кабель не знаю пробили хладагент в стене и даже само устойчивая там база данных неважно что которая в интернете позиционируется как Мега устойчивая всё равно упадёт если света нет Здравствуйте спасибо за доклад как ки Уже на половину моего вопроса Вы ответили что вот если дис падает Да Какую очередь вы используете вы сказали то что складывайте файловые хранилища а не рассматривали какие-либо другие варианты Нет не рассматривали просто локально складывать сервис у себя под ноги те объекты которые не смогли дойти до редиса по той или иной причине по возврату редиса в работу всё из файлового Ну из файлового пространства учитывается и складывается обратно больше ничего не смотрели потому что оно один раз у нас Стрельно один раз починили больше ни разу не возникло работает Не трогай можно просто ради интереса там изучать что можно как решить но оно больше никогда нас не волновало понял Большое спасибо Да у меня вопрос такой как вы конфигурируется И второй вопрос в Open Source как-то вы будете это публиковать или нет спасибо спасибо вопрос Вообще болезненный Я не стал его на самом деле на презентации оповещать сначала управляли всё через автоматизацию конфигурации каждый сервис имел у себя протяну конфигурационного файла и конфигурирован когда их количество начало увеличиваться Ну у нас просто деплой на развёртывание шаблонизатор просто там неимоверно увеличиваться по размеру наш код конфигурации до начал расти до 1.000 Вот и мы приняли решение это всё собирать То есть сейчас система инициализируется с базовой настройкой там почти у самих сервисов нет конфигурационных файлов а перенесли это всё в базу данных управляется через UI В итоге Ну то есть приоритеты просмотр нагрузки мониторинг Управление потоками отключение подключение новых систем всё вывели В итоге в UI Сначала думали что вообще без юаа сможем обойтись но система потребовала и подключения сотрудников поддержки В итоге первой линии так или иначе ну с такими объёмами не возможно там одному двум даже небольшой команде это всё выдержать всё перенесли в итоге ВБ интерфейс вот на первый ответил вопрос на второй вопрос в Open Source мы встраиваем внутри в Тим продукти платформа инженерную которую строим это вот платформа объединяющая эти все системы в одно окно чтобы не было включение контекстом но наверное для Open Source для финтеха да для PCI ДС Да но довольно-таки маленький наверное рынок будет на такие на такую потребность То есть если там брать какую-то историю ещё до сбера я тоже работал опять же в финтехе там был PS задача была абсолютно такая же но там работало всё на РН и всех всё удовлетворяло допустим вот ну и там объёмы были совсем другие такие объёмы загрузок артефактов и коммитов в сутки Ну наверное не всем таких клиентов можно на пальцах счесть А если их на пальцах чести у них такой объём наверное у них есть деньги построить свою такую же систему ответил вопрос слева да спасибо за доклад Скажите почему не использованы нативные механизмы нативные механизмы заливания репозиторий там до или там в Нексусе тоже есть Маги Дафа что-то такое То есть я так понял вы просто режете файлы на кусочки там потом мы его собираем по прикладному протоколу будь то либо ма либо там мы загружаем это всё обратно через do Push То есть это мы используем нативные расказа это сза участия"
}