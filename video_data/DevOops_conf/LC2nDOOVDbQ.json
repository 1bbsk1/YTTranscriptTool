{
  "video_id": "LC2nDOOVDbQ",
  "channel": "DevOops_conf",
  "title": "Роман Бойко, Дмитрий Шитов — Паттерны бессерверной архитектуры и лучшие практики",
  "views": 375,
  "duration": 4373,
  "published": "2021-04-30T01:29:20-07:00",
  "text": "роман дмитрий сергей рад в и благодарю а то что решили присоединиться к нам сегодня я на самом деле уже дан универ куски моим новым любимом городе санкт-петербурге буквально рядом с вами со студией но не вне и конечно же сегодня действительно это очень интересный так вот все знают что обернитесь это наше новое все но мне кажется он уже наша какая-то данность которая вокруг нас и с которого сложно но есть будущее и это будущее на самом деле очень уже рядом и вот как раз без серверные штуки сервер с линды это очень интересная тема и сейчас очень не хватает как мне кажется сценариев когда эту технологию этот подход к разработке надо использовать и я надеюсь что сегодня ребята как раз и расскажут про эти сценарии расскажут как по их мнению правильно и хорошо использовать эти штуки и возможно ближайшем будущем мы как и java и я купер нить из отомрет и сервис будет нашим настоящим даром on 1 и маленький такой дисплей комментарий просьба вопросы задавать действительно в чатике и в конце будет спасена зона зон комнате тоже приходите мне кажется нам есть о чем поговорить роман да привет всем действительно сейчас сервер лес наверное становится все более и более популярным поэтому мы хотели поделиться неким опытом рассказать как мы считаем и как мы видим уже на примере тех кто использует технологии лучше делать и лучше реализовывать ваши приложения плюс ну как обычно у нас совпали конференции daewoo пс и invent мы там по 1 выкупа объявляли много чего конькового интересного что не вошло в доклад я думаю можно будет потом опять же в конце обсудить я расскажу кое-что что появилось в том числе всех с нового красном и про контейнеры есть и интересные обновления вот единственное хотел сказать что сам доклад у нас будет записей потому что как видите мы из разных локаций мы тестировали соединения она не всегда была надежным и хорошим поэтому мы сам доклад записали чтобы точно все было хорошо поэтому мы будем относить на свободные в чатике можно действительно задавать сразу вопросы на которые мы постараемся как можно больше и подробное ответить дима ты вот как раз до явный пример того почему наш доклад в записи и роман уже не имеется послушать если вы не против в дело привет всем меня зовут роман я работаю solution architect там и ws мы сегодня со мной еще мой коллега дмитрий из компании 1с рарус и мы сегодня хотели поговорить по сервер лес по то как какие есть подходы как правильно реализовывать тельные паттерны сервер лес архитектурах соответственно у нас доклад будет такой с одной стороны теоретически то есть я буду рассказывать какие теоретические правильные вещи а дима он больше у нас так от сохи он будет рассказывать как они это у себя внедряли что у них получилось на какие грабли они наступили чем следует подумать добрый день уважаемые коллеги и соучастники конференции да как казалось те подходы которые мы применили на своем проекте оказались паттернами для меня это была неожиданная приятная неожиданность оказывается мы тут до чего-то такого глобального мировых практик додумались и да я буду рассказывать про практический опыт которым получили на своих на одном из своих проектов юго-восточной азии разумеется этот опыт и мнение субъективные и особо ценным будет то что рома нам расскажут как нужно было на самом деле делать даст какие-то подсказки и если бы мы их получили во время ну проект шел бы полегче в некоторых частях поэтому давайте начнем да отлично но для начала давайте немножко окунёмся в историю и посмотрим а когда вообще у нас сервер нас появился то есть быту это различные мнения и есть разные отправные точки вообще так если считать по традиционному первым и w с сервисом который был разработан и откуда вообще пошло понятие сервер лес был сервис к и ws лямда он был запущен в 2014 году то есть ответственно уже шесть лет прошло с тех пор как у нас появился сервер лес подход и как мы начали о нем говорить но с другой стороны вы удивитесь но сервис такой как amazon s3 он был запущен в 2006 году и по факту это был вообще самый первый сервис в амазоне запущены на нашей платформе он был даже запущены раньше чем и счету и если сейчас посмотреть на то определение сервер с которая дается то по факту с 3 это тоже сервер наш сервис то есть мы там тоже не управляем серверами мы получаем готовы сервис который мы используем который автоматически масштабируется автоматически отказывал устойчивы и так далее то есть по факту сервер лес пришел к нам гораздо раньше чем мы об этом знали и мы его уже давно используем но тем не менее если посмотреть на всю школу к серверу лес мы шли достаточно долго и вначале естественно все было на земле когда их не было никакого облака действительно мы тогда физически работали с серверами с железом таскали его ставили в стойке что-то настраивали потом появилось облако то есть железо стала от нас чуть дальше но при этом все равно мы работали еще достаточно низко уровнями к концепциями такими как виртуальной машины такие как какие-то сервера что туда ставили софт патч или наказывали там linux и windows к кому что нравиться и нам нужен был администратор который поддерживал сопровождал как далее дальше технологии все больше и больше развивались и соответственно облачные провайдеры такие как и w стали предлагать более высокоуровневые сервисы в которых вам уже нужно было меньше заботиться о администрирование операционные системы и больше получать какие-то готовы кубики при помощи которых можно было их комбинировать и получать в результате готовые решения так мы постепенно начали двигаться в сторону сервер лес подхода и в принципе сейчас если мы посмотрим на нашу платформу есть очень большое количество сервисов и ws и это действительно не только лямда хотя в первую очередь всегда когда начинаем говорить про сервер без все вспоминают сервис для мда но это и большое количество других сервисов реализующих различные функциональности такой как и базы данных и сторож и обмен сообщениями и аналитика и соответственно все эти сервисы позволяют нам стоить гибкий масштабируемый отказоустойчивые решения достаточно быстро комбинируя эти кубики и составляет какой-то год готовое законченное решение но зачастую получается что-то вроде инструментов много а как правильно их использовать какие подходы лучше о чем нужно помнить не всегда очевидно вот как раз сегодня я и расскажу про какие-то типовые паттерны как их лучше использовать как правильно настраивайте и соответственно получать желаемый результат при использовании таких сервер для сервисов ну а дима дима больше расскажет то практическое применение то есть то что они уже попробовали что они сделали и что у них из этого получилось соответственно дима сейчас расскажет про свой проект это я бы мы будем рассказывать про опыт который мы получили во вьетнаме на проекте нашего совместного предприятия 1с и новейшим то есть этот проект вырос у нас уже в отдельный business unit сейчас у нас есть подразделение то есть это совместное предприятие вместе с между 1с 1с рарус и группы вьетнамских инвесторов и мы разработали систему которая называется менеджмент информацию систем для low cost авиакомпании во вьетнаме бюджет то есть это первая частная авиакомпания социалистической республики вьетнам и в прошлом году еще долго вид кризиса они обогнали по объему пассажироперевозок на внутреннем рынке национального авиаперевозчика вьетнам airlines бизнес очень быстро растущие серьезны и если взглянуть на архитектуру то что мы увидим ну в общем то наше решение состоит из двух частей то есть это монолитное приложение на базе 1с enterprise 3 о-го-го трехуровневая архитектура есть сервер приложений 1с есть сервер баз данных на базе пост грех и сквер и есть сервер для доступа через тонкие web client и так если укрупненно и есть без серверная часть которая применяет сервисы и пегий твой линда ис-3 здесь для упрощения архитектурой я ее не отражал но мы также конечно же используем с 3 куда же без нее до очень интересный проект и действительно как видим даже в таких проектах можно использовать сервер лес но вообще интересно откуда вообще 1с то взялся в облаке расскажешь как вы вообще дошли до 1с и лямбды вместе да если сейчас вот так оглянуться назад точнее начать именно так как мы начали этот доклад то звучит достаточно неожиданно и даже дико что вот это вот та самая 1с которая у всех бухгалтерия то есть это первая ассоциация наверное все семь приходит нам на ум а тут вдруг облака amazon web solutions сервер лес такой прогрессивный прогресс картинга текнолоджис то есть мы и в целом проект получился достаточно нетипичный началось все с нетипичного запроса что нужно внедрить 1с во вьетнамской авиакомпании в то время как то и в россии не так много проектов на платформе 1с для авиакомпаний конечно в последнее время так число растет но все же авиакомпании 1с но прогнали несколько несколько дыма примеров тестов на один срп и решили что ну должно должно взлететь и после подписания контракта мы начала самое интересное то есть клиент решил посовещавшись на уровне совета директоров решили приносить об все что можно по максимуму вымазан поэтому фактически нас поставили перед фактом ребята сейчас нужно чтобы это все работало в amazon web solutions и да кстати мы внедряем тут стандарт безопасности iso 27000 01 пожалуйста разрабатывайте теперь так чтобы ваша система соответствовало работает на стандартной безопасности собственно так мы пришли к идее твои потому что но только архитектура с применением gateway удовлетворяла той трактовки этого фреймворка а а iso 27001 и чтобы соответствовать стандартам и обеспечить необходимый уровень безопасности ну и почему лямда ну потому что у нас неравномерная загрузка то есть у нас есть тяжелые выборки раз в месяц когда мы собираем дамы продажи каких-то коммерческих вещах в общем подтягиваем данные из их бухгалтерской системы то есть это достаточно тяжелые запросы и есть ежедневное обновление данных оперативных которые там обновить нужно нам master data какие-то справочники куда кто летал какое расписание было это достаточно легкие запросы поэтому лямда здесь подходит как нельзя лучше потому что раз в месяц мы можем там нагрузить линду так что там отработает тяжелые запрос его во все остальное время в общем то просто вызываем простая функция для обновления master data и оперативных данных в общем то вот так мы пришли к такому какую архитектуре очень интересно спасибо дим соответственно и сегодня дальше мы сейчас посмотрим на какие-то уже паттерны и посмотрим как векторы из этих паттернов о своем проекте ребята применили и что из этого вышло мы начнем с самого наверно известного паттерна не только за в рамках сервер лес подхода но и принципе сейчас большинство web-приложений они используют в том или ином виде с архитектуру с подход соответственно даже если вы начинали разрабатывать какие-то уже сервер лес приложения то в большинстве случаев вы скорее всего начинали с чего-то очень похожего на то что сейчас видите на слайде то есть это и пегий твои это для мда для бизнес-логике и это хранилища данных но скорее всего это в 99 процентах случаев будет д н м а д б если только у вас нет каких-то определенных требований к базе соответственно казалось бы простой достаточно паттерн здесь все очевидно легко реализовать но тем не менее опять же здесь есть тоже некоторые нюансы о которых нужно помнить самого начала чтобы не наступить на очевидные грабли и получить тот результат который мы хотим давайте посмотрим чуть более подробно ну в первую очередь да скорее всего вы об этом подумали но иногда это не так очевидно нужно естественно все логировать все данные метрики по вашему приложению записывать и для этого естественно есть уже готовые сервисы нам дано уже из коробки интегрируется с co2 чем для записи логов и метрик плюс все вот эти сервисы которые здесь есть такие как липяги твой лиан д д н м а д б и многие другие они интегрируются с другим сервисам таким как x-ray и позволяют делать tracing вызовов смотреть собственно как у меня запросы ходят от одного сервиса к другому понятно что наверное на такой простой диаграмме это не так важно но когда у меня все-таки будет несколько сервисов они друг с другом как то взаимодействует то здесь tracing будет очень важен ну и соответственно логировать нужно в каком-то определенном формате то есть желательно сразу структурировать логе и собирать всю необходимую информацию для того чтобы в дальнейшем можно было отлаживать код и смотреть что с ним происходит дальше тоже не всегда очевидно вещь особенно когда мы говорим по публичный и pr and point и и иногда забывают ограничить доступ к api и соответственно к вам может действительно в какой-то момент времени прийти очень большое количество запросов и с одной стороны это плюс сервер леса что мы очень быстро можем масштабировать большое количество ресурсов и отработать и обработать большое количество запросов но с другой стороны это может быть и минус в плане того что действительно мы хотим все это обработать или нет и если мы по каким-то причинам не хотим или там у нас есть определенные лимиты с точки зрения средств которые мы хотим потратить на нашу архитектуру то данный от масштабируется мы потратим много денег если особенно это публичное реально смогут зады досить если мы опять же его никак не защищаем не закрываем поэтому нужно сразу по при проектировании таких систем думать о том как мы будем регулировать входящий трафик ну первое это можно там действительно просто ограничить количество запросов и продлить их дальше это можно с точки зрения безопасности опять же если у нас теперь не должен быть абсолютно публичным то естественно нужно на него при повесить какую-то аутентификацию и авторизацию самое простое здесь опять же с интегрироваться с еще одним сервисом таким как амазонка огни то причем все делается из коробки быстро и удобно соответственно в кабине это у нас будут храниться пользователем и их там будем аутентифицировать и затем через 5 лет вы авторизовать на доступ к тем или иным ресурсам но с точки зрения security есть еще второй важный момент это где мы будем хранить параметры секреты для доступа из лямда функций часто опять же используют не самый защищенный вариант это когда в линда функции прям через переменные окружения подставляют те или иные логины пароли для доступа к внешним ресурсам так делать ненужно лучше использовать какой-нибудь готовое решение типа secret менеджера в которой мы будем хранить наши пароли и 5 ключи и по необходимости и дизайн для функции мы сможем их получать и соответственно использовать ну а для доступа к внутренним амазонских сервисом естественно нужно использовать ролевой доступ с разграничением по ролям того кто кто и куда может ходить дальше с точки зрения перфоманса опять же самый такое наверное известный сценарии котором все говорят как же у лямды есть cold start и она долго запускается при обращении что с этим делать для этого тоже есть решение то есть мы можем использовать pro-vision конкор насти для тех случаев когда нам действительно нужно чтобы лямда всегда быстро отвечала на наши запросы либо если действительно у нас высоконагруженные система и все время запросы к нам приходят то мы лямды держим достаточно долгое время в запущенном состоянии просто даже как они отработали и соответственно в этом случае у вас тоже cold start of не будет поэтому большинстве случаев это проблема такая надуманная но опять же всегда можно есть инструменты как ее улучшить и как от нее избавиться плюс опять же не все помнят например что и пегий твой в пгт есть разные варианты and point of например если вы используете региональные and point это они работают с печки пе-2 протоколов соответственно опять же с точки зрения сетевой скорости это может быть большим плюсом еще важный аспект с точки зрения вообще таких сервер лес технологии они в принципе гораздо могут быть дешевле чем использование традиционных вычислительных мощностей науку на основе виртуальных машин либо контейнеров но тем не менее здесь тоже есть ряд особенностей то есть понятно что в случае с лямбдой мы платим за время ее исполнения и за объем памяти который выделен функции и здесь есть и интересная особенность которая заключается в том что от объема памяти который выделяется функции еще зависит сколько процессор на во времени этой функции будет выделено и иногда бывает выгодней выделить функции больше памяти и получить больше процессор на во времени соответственно от работает быстрее по времени чем выделить функциями меньше памяти и она будет дольше по времени работать вот для того чтобы найти такой оптимальный баланс соотношении с одной стороны производительности с другой стороны времени исполнения есть отличный инструмент alpen сорт называется power тюнинг при помощи которого я могу для каждой из функций создать ее профиль и посмотреть действительно в каких рамках находятся оптимальное соотношение с точки зрения цены и производительности каждой конкретной функции соответственно это все что я хотел сказать по rest паттерну и дима сейчас расскажет о как они реализовывали этот паттерн и что у них получилось в их сценарии до начинали мы с такого простого варианта вот он оказался паттерном rest то есть когда у нас один с со дергает ссылку на 1 и 5 га и твои и пик gateways соответственно делает in boucle на лямда функцию или мда функции который написан на сишарпе делает запрос кредо реплики из сервиса rds у нас в качестве источника базовых не amazon и вот целый кластер relational database на microsoft sql и в ответ мы получаем какой-то результат этот результат отдается обратно вадим с единственная здесь такая особенность если вы видите что два облака объединены через репите пиринг вот это накладывает определенные определенные сложности но вот например функция get airports достаточно просто нет ну то есть эта функция которая получает лист то есть список аэропортов куда летали наши пассажиры на наших самолетах то есть мы сделали этот запрос и он вернул нам этот список то есть джейсон файлы со списком аэропортов но когда запросы становятся потяжелее то фичи отдельных сервисов они складываются в такую киллер фичу которая может прикончить определенный паттерн и заставить чесать голову что же делать дальше в нашем случае это было первое это то что максимальное время жизни пегий той это 30 секунд и если 30 секунд раунда функция не возвращает результат то собственно бей quay закрываются и мы ничего не получаем даже если вычисление произошло следующее это то что лямда функция должна выполниться в течение 15 минут вот и как в какое-то в какие-то моменты мы сталкивались с этим ограничениям но в ходе подготовки к этой презентации ну мы уже получили ряд советов и я думаю что рома нам расскажут как на самом деле можно сделать этот паттерн более применимым вы всего лишь применив несколько таких hand of hand of я думаю что мы бы некоторые функции оставили в сил синхронных вызовах если бы знали это до того да действительно как мерно самое такое первое с чем часто сталкиваются многие из заказчиков когда начинают работать и paidi и твоим мыслям дай действительно по 15 минут в лиам где слышали наверное большинство аппарат 30 секунд в 5 тыс вышла уже гораздо меньше людей и и и иногда это вызывает такой некий диссонанс то есть мы считаем что у нас функция живет 15 минут соответственно отправляем в нее какой то запрос через ebay gateway но и пегий твой работает в синхронном режиме соответственно он ждет ответа от этой линда функции но ждет он ровно 30 секунд соответственно если за 30 секунд лямда функция ничего не вернет то на эпики твоей случится тайм аут тайм аут вернется в виде ответа на запрос в принципе наверно это для большинства сценариев нормальный вариант поскольку все таки и пегий твой и ристопии у нас используются в большинстве своим в веб-приложениях и там не то что 30 секунд там и одну секунду никто особо из пользователей ждать не готов поэтому для таких вот синхронных взаимодействий в принципе это достаточно нормальное ограничение и если вы действительно в них упираетесь тот первый повод подумать действительно нужно ли в этом месте использовать синхронные вызовы использовать и синхронный паттерн либо переходить к кому то а синхронному взаимодействию к каким-то очередям к чему-то еще и соответственно в этом случае большинство из таких ограничений она автоматически снимается ну понятно что 15 минут для линды но остается всегда но 15 минут это много и в принципе за 15 минут можно много чего посчитать и сделать поэтому мы плавно переходим к следующему паттерну который из себя представляет реализацию асинхронного взаимодействия в виде веб хуков то есть соответственно при помощи вот хуков мы настраиваем взаимодействия между нашим клиентам и брендом уже не в синхронном режиме в асинхронном то и соответственно у нас какие-то запросы могут приходить от клиента либо мы ему их можем возвращать но здесь опять же есть ряд моментов о которых следует помнить если мы переходим в асинхронный режим и если мы вообще достаточно активно используем лямда функции с бэндом как вот даже в случае с димой и коллегами который работает в традиционной парадигме то часто может случиться такая ситуация что у нас дали нам дано хорошо скейлится в не опять же приходит много запросов и она все эти запросы начинают сыпать в бэг-энд систему справится ли backend с такой нагрузкой ну наверное не всегда есть ряд наверно случаев когда он при помощи нам для функций может быть что называется зада дасин в этом случае у линды есть настройка которая позволяет ограничить количество одновременно запущенных в параллель функций и соответственно при вот таком взаимодействии с брендами традиционными и а не следует помнить и включать ответственно чтобы букет мог справиться с нагрузкой вторая вещь которых как раз хотелось поговорить это действительно переход от такого синхронного взаимодействия к а синхронному и здесь мы часто можем использовать такой паттерн как все равно мы оставляем и пегий твой для получения запросов и работы с ними но из и 5 твоим они напрямую вызываем лямбда-функцию а используем какую-то систему либо потоков либо очередей ну как в моем примере на слайде это может быть kinesis data streams хорошая вещь заключается в том что и 5 где твой нативно интегрируется там с генезисом либо сельскую я сам не для этого не нужно писать код дополнительного кода я могу настройками и пеги это сказать что если у меня запросы приходят на определенные ул например там post запросы то соответственно я все эти запросы там тело сообщения перекладывал в виде сообщений либо видим с отжав kinesis например а дальше у меня есть ли он до функция которая подписана на этот стрим и я туда эти сообщения учитывают и каким-то образом обрабатывать более того если мне например не нужно делать это в каком-то таком приближенным к реальному масштаба временем сценарий я могу накапливать какое-то количество сообщений в генезисе и только потом запускать лямда функции с в бочке режиме то есть для обработки этих запросов и вторая вещь которая сразу здесь нужно вспомнить это а что будет если у меня какие то запросы не обрабатываются что с ними будет нет не так для этого в лямки предусмотрен механизм дидло терплю при помощи которого я могу такие необработанные сообщения складывать в отдельную очередь ну и затем уже построить отдельный процесс по их там анализу обработки так соответствии с тем какие у меня есть требование дальше еще если мы вернемся к безопасности помимо аутентификации и авторизации через к огни то напрямую и пегий твой поддерживает так называемый кастом of the rivers например из у меня есть какая-то своя система аутентификации и авторизации через кастом авто райзера я могу настроить соответственно авторизацию в такой системе и иногда опять же там с зависимости от требований например мне нужно какие-то персональные данные быть вычищать я могу написать ещё дополнительную функцию которая работает с genesys да да это stream и например автоматически удалять либо маскировать определенные данные при поступлении их в соответствующие стримы по-бычьи рекорды я уже говорил но действительно мы можем накапливать какое-то количество сообщений в kinesis и они сразу отправлять их блин да функции той соответственно мы можем использовать опыт такой буфер для накапливания сообщений и для обработки их затем еще помимо kinesis а как я уже говорил можно использовать похоже паттерн но заменить kinesis на другой сервис спс часто бывает возникает задача когда мне нужно например сообщение обрабатывать в определенном порядке так вот например в скс есть режим работы фифа при помощи которого я могу соответственно гарантировать очередность обработки сообщений то есть соответственно если мне так такая функциональность необходимо это может быть но какие-то финансовые транзакции в первую очередь то я тогда буду использовать из псы с крестным также интегрируется с дотла церквью и соответственно мы можем настроить обработку сообщения в определенном порядке и опять же от следовать какие из этих сообщений мы и не смогли по тем или иным причинам обработать это паттерн ребята также использовали у себя и сейчас дима расскажет каким образом они это делали и что у них получилось действительно мы пришли к следующему подходу который оказался паттерном веб хук то есть ну почему пришли понятно из проблем паттерне с то есть мы не укладывались во время жизни лямбды и нужно было думать как получать результат и даже после того как ей клей закрылся то есть поэтому мы разбили процесс получения данных на два шага 1 эта передача запроса в линды 2 получение то есть мы все так же то есть один из дергает река и горит при ссылку и прыгает твой эн эн волк кол делает ему укол как линде и в ответ получает или статус 200 если запрос распознан или статус 501 если он не раз постным то есть и на этом же этапе мы перешли от get запросов к пост за к запросам даже если нам нужно получать данные они по сути своей get но некоторые запросах особо тяжелых у нас было более 2000 параметров ну все-таки эта авиакомпания 400 рейсов в день и нам нужно получать всю комбинацию всех аэропортов самолетов там иногда пассажиров и get запрос не принимает это количество входящих параметров поэтому мы перешли к по постам дальше если этот пост запросам нормально парси целям на входящие параметры он возвращает статус 200 что запрос принятый ли статусным 500 так что не все нормально вот соответственно этот запрос уходит в наши ряды реплику и через веб сервис который мы подняли на стороне 1с возвращает данные но соответственно котором мы вместо с вы мы применяли параметр stor сервис потому что он такой более бесплатный вот там она записывается параметры как раз таки и нашей базы данных 1с по какому адресу отдавать что принципе насколько понял не противоречит но однако противоречит потап но следующее что у нас в общем то вот он хорошо и название мне нравится may be потому что 12 may be мы получим какой-то результат после того как мы получили статус 200 мы дальше только может не догадываться что с этим запросам то есть мы вели такой параметр как уровень логирования которые в параметр столь хранили но и для какого-то дебаггинга регулировали степенью детализации с которой выполнение функции пишет пишется в cloudwatch то есть и в общем то тут нет такого непрерывного отслеживания передачи результатов то есть лямда можешь чистой совестью сказать я все отдала водяные свода ловите на той стороне 1с не принимает где эти данные почему не получились это достаточно сложно было отслеживать тем более как я уже сказал мы применяли ли писи перед то есть то есть у нас два приватных облака которые используют приватные and point и и вот мы достаточно долго искали почему не приходит но на самом деле там можно нужно особым образом составить адрес вызова приватного и pr то есть мы и как я сказал достаточно проблематично здесь был у за счет такого то разрыва в передаче информации ну и это часто мы уже видели те слайды которые рому подсказывал новых хотелось бы выслушать то как на самом деле нужно было делать или что здесь можно улучшить до действительно где хорошие и перейти к такому по синхронному взаимодействуют но как я уже рассказывал и показывал чуть не хватает здесь какой-нибудь службы очередей которая бы выступал буфером для сообщений соответственно я бы что сделал я бы действительно добавил одну очередь после пегие т.м. то есть от туда куда мне приходили бы изначальные запросы вот один с дальше из этой очереди при помощи лямды я бы забирал сообщение обрабатывал их и в случае действительно успешного успешной обработки складывала сообщение в другую очередь либо в топик для того чтобы 1с забирала эти сообщения от туда но также я бы настроил здесь обязательно d2 терплю для тех случаев понятно что не знаю сообщением могут там по каким-то причинам быть не обработан места там некорректный формат битая сообщение не знаю логика поменялось много может быть нюансов но вот в текущей реализации действительно если вы не обработаете это сообщение поскольку вызов все равно по факту синхронны или лямда она поработает и если она упадет то все ну да влогах вы увидите что там произошел сбой произошла какая-то нештатная ситуация но восстановить это сообщение как-то его пополам попытаться обработать еще раз к сожалению не получится поэтому дауд у очередь бы здесь сильно помогли то есть мы бы гарантированно сохраняли это сообщение потом настроили ддт и и гарантированно его бы не потеряли а дальше уже можно было бы там что то с ним делать и как то его обрабатывать ну ну и в отправке тоже есть соответственно у нас желательно иметь какой-то промежуточный хранилище либо очередь либо топик куда лямда будет возвращать ответ опять же здесь не знаю с пилингом что-то случилось с 1с что-то случилось 1с не смог сразу забрать данные у нас в сообщении они будут какое-то время жить очереди соответственно 1с сможет их чуть позже собрать и распарсить обработать уже по своей логике это то что я бы рекомендовал здесь реализовать но давайте двигаться дальше и посмотрим еще на несколько паттернов еще один паттерн который хотелось сегодня обсудить это паттерн работы с данными то есть мы сейчас больше всего говорили про rest api дают наверно такой самый популярный подход который всем понятен из известен и практически все его сейчас в том или ином виде используют но арест есть и ряд ощутимых минусов таких как все таки он не до конца определяет схему обмена данными то есть есть разные интерпретации того как мы rest api можем реализовывать плюс когда у меня идет идет достаточно интенсивный обмен данными с брендом rest api и либо дает сразу много данных которые частично мне не нужны и актер ну на клиенте от фильтровую не использую либо он дает мне мало данных и мне тогда чтобы получить все данные которые нужны для той или иной странички для того или иного приложения нужно делать несколько запросов к и пою соответственно это может быть больше потреблять больше вычислительных ресурсов и так далее для этого есть такой паттерн и теперь как граф и соответственно у нас есть реализация об sing для него здесь основное преимущество как раз заключается в том что мы можем описать схему и потом по этой схеме быстро прошивать именно набор данных который нужен для нашего приложения оно хорошо очень интегрируется с amazon узкими сервисами то есть мы можем напрямую уходить в данной модели но например мы также можем использовать лямда функции для какого-то более сложного сложной логики плюс еще есть один такой интересный сервис если мы например реализуем какой-нибудь нам нужно реализовать последовательность шагов есть сервис степ functions при помощи которую например опять же я могу реализовать там не знаю про прозрачно обработку транзакций либо еще какие то вещи которые требуют последовательность шагов и компенсационную логику опять же с точки зрения безопасности здесь все то же самое об sing он интегрируется с сервисом к гнид а то есть мы можем аутентифицировать и авторизовать пользователей еще о чем хотелось сказать мы сегодня больше говорили про дай нам о тебе как такую основную базу вот ребята у себя использует rds но в принципе у нас есть большое количество баз данных на амазоне и под разные задачи мы можем использовать разные базы и соответственно хорошо бы хотя бы помнить о том что у нас есть в амазоне различные базы хотим посмотреть какая из них когда лучше подходит ну и опять же с точки зрения граф коваль и и 5 есть уже встроены и различные механизмы каширования фильтрации данных т.е. соответственно в чем как раз и как я и говорил при прелесть граф келлер что я могу написать запрос таким образом что мне вернется на клиента только тот набор данных которые мне действительно нужен это можно сделать в рамках одного запроса к источнику что достаточно удобно давайте еще поговорим про такой интересный паттерн как стрингер поскольку как раз он позволяет нам совместить несколько миров в вместе то есть он позволяет нам с одной стороны взять что-то и из например он прием либо из традиционных сервисов и поверх либо вместе с этим использовать уже и сервер лес подход каким образом он реализуется то есть ну начнем давайте с такого примера у меня есть что-то на земле я хочу начать переезжать в облако и начать реализовывать каких но у ног новую функциональность в облаке что я могу сделать ну я могу например в облаке развернуть и 59 вы который будет смотреть на текущую инфраструктуру через две писи и через direct connect и уже тем самым я сделал первый шаг к миграции дальше я могу использовать даже в такой конфигурации сервисы сбора логов и метрик чтобы понять а как у меня сервисы дом с долгом взаимодействуют включительно пример tracing опять же cloudwatch x-ray они умеют работать из он приемными сервисами и соответственно дальше я могу перенаправлять трафик в локальный data center с одной стороны могу использовать опять же кастом of the rivers для аутентификации и авторизации например в active directory либо в каком-то корпоративной директории и дальше я постепенно сон прямо начинаю переносить то что называется left in shift сервис и соответственно если я использую там виртуальные машины я переношу на счету если использую контейнеры их переношу там либо вы сейчас либо в и ксв губернатор но при этом опять же с точки зрения клиента поскольку он ходит через этой битве ничего не меняется а дальше соответственно для кота но функционал для реализации новых паттернов для реализации каких-то новых приложений я уже могу опять же используя тот же самый и 5 гид вейперы направлять вы зовут линда функции писать там бизнес-логику использовать д м э д б и так далее то есть соответственно я постепенно так шагами с одной стороны переношу все в облака с другой добавляю новую функциональность уже с использованием новых сервер лес сервисов дима его команда тоже реализовывали в какой-то мере этот паттерн дима расскажи что вы делали до мы оказывается применяли этот паттерн но опять же как я уже говорил в самом начале то что мы должны работать в амазонии в облаке мы узнали уже по ходу проекта и разработку то мы начинали с тех сред которые были что называется на земле точнее ну местных локальных дата центрах и в базах у клиента то есть и в общем то даже когда мы развернули наш station in warm and в сингапурском дата-центре то есть вся наша инфраструктура которая обслуживала 1с она дает мы достаточно быстро и и перри развернули ту которая являлась прототипом того решения которая сейчас применяется но база данных основным основной самый тяжелый источник он долгое время была еще локальном дата центры и мы были вынуждены строить соединения с этим дата-центром используя сервисы сайту сайт ппн connection и на стороне амазона это было virtual проводка гейплей на стороне дата-центра косой бейкой и в общем-то достаточно долгое время жил жила это было это на среда нужно сказать по моему мои пока на прошлой неделе окончательно уничтожили то есть это была наша основная среда для разработки и тестирования то есть и как вы видите вот вьетнамский дата-центрах локально 7 сионом находится на справа и по мере того как клиент переносил все свои базы в rds сервиса мастерское серверов мы переключали лямды вот это вот длинного пути через видит но через сайту сайт т.п.н. connection соединение через пилинг к базам rds сервисе помимо того у нас такая команда была мультинациональная то есть у нас часть разработчиков и работала в москве часть разработчиков работала во вьетнаме причем и вьетнаме как я уже сказала это у нас был организован sp где по ходу проекта мы набрали вьетнамских разработчики вы научили репозиторий был в москве и в общем-то такой достаточно развесистой архитектуры применялась до недавнего времени к слову сказать ну обеспечивала возможность работать потому что допустим когда мы пытались подключиться к москвич симсе из москвы к семьи дата-центра достаточно часто ширина канала не позволяла работать просто элементарно работать после переезда в amazon это проблемы исчезли вот и плавно в течение года наверное мой вот так вот переключились помазан вот поэтому мы да мы применяли этот паттерн стрингер единственное что вот когда я узнал что мы применяли этот паттерн я решил воспользоваться переводчиков и узнать что это значит и вот единственный вариант перевода который дает переводчик stranglers то душитель вот rumours ты не можешь прокомментировать почему то говорят она это название душить ну это да наверное если дословно переводить то это паттерн душитель но здесь наверное в первую очередь нужно сказать в чем его и идея и подход заключается действительно идея заключается в том что мы постепенно переносим либо реализуем новый функционал на каких-то новых сервисах там нолям зинаидой найма деби на сервер лес сервиса и с другой стороны мы постепенно уменьшаем традиционную архив архитектуру традиционную инфраструктуру уменьшаем количество виртуальных машин переезжаем из локального дата-центра в облако из здесь тоже решаем что из того что у нас есть нужная эко а что можно выключить но при этом с точки зрения клиента это все прозрачно потому что мы используем прокси в виде и пять лет в но мы как бы ужимаем с одной стороны традиционную инфраструктуру традиционные сервисы и расширяем с другой стороны использования новых сервисов новых подходов поэтому да так поттер называется стрингер я бы сказал что он ну как бы ущемляет удушает традиционные подходы и традиционные сервисы в году использованию сервер лес и каких-то новых идей вот да давайте перейдём к заключительной части и здесь хотелось бы поделиться тоже какими-то основными наблюдениями основными идеями по поводу того как лучший что правильнее делать дима что вывод из своего проекта вынесли и о чем хотел сказать я хотел бы поделиться некоторыми советами которые надеюсь многим покажутся очевидными почему надеюсь чтобы не сталкивались с теми же самыми проблемами которая достаточно просто решаются допустим вот так как мы применяли правят и пиа это как я говорил же то есть у нас два облака были объединены в писе пеленгам и в этом случае адрес вызовы нужно составлять ну буквально вручную это конечно документации описано то есть документированный сервисы очень хорошо даже можно я бы назвал это избыточным нужно много чего перечитать но очень детальные в конце концов мы нашли что вот оказывается если применяете приватный the road 53 сам автоматически и вам создаст алиасы пожалуйста дергайте его для вызовов ну конечно нужно еще настроить в обоих облаках рута и был чтобы поближе вы проходили следующее это что выпей gateway нужно делать тепло из 3g после каждого изменения то есть какое-то изменение внесли сам стоишь сам по себе не заработает нужно делать тепло и каждый раз и то к чему мы пришли мы начали применять подход infrastructure скотт применяет terra форм то есть и пример для это помогло нам быстро перри разворачивать нашу без серверную часть но в том числе и серверную та которая монолитная то есть на и ситу там где один из живет в целом там и и облака так передавали как набор terra форм скрипта вы каких-то пакетов установки поэтому вот использование infrastructure is код полностью себя оправдывает хоть и требует времени мы применяли terra форму ну потому что мы у нас были специалисты которые вы узнают если у вас вы применяете другой ну я просто рекомендую применять сам принцип потому что это позволяет сэкономить колоссальное количество усилий при когда что-то изменяется она изменяется практически всегда ну и наверное следующий то есть самый очевидный совет но когда которыми мы долго доходили что если вам нужно узнать как сделать правильно то лучше обратиться системному архитектору то есть можно спросить как просто достаточно просто взять и спросить как сделать правильно это перефразировав анализируя главного героя одного из известных сериалов чтоб better call с позвоните и все будет в порядке обращайтесь к нашим коллегам каламазу даст спасибо дим действительно мы всегда готовы помочь и посмотреть детально на ваше решение на вашу архитектуру подсказать это новые идеи и в завершении сегодняшнего доклада давайте сделаю небольшой такой самаре best practice of the о чем нужно в первую очередь помнить и с точки зрения сервер с приложением серравале раз подходов ну да в принципе такие достаточно очевидные вещи но о них иногда забывают и о них все-таки нужно помнить не старайтесь в лямбду запихать все подряд то есть да прям до можно добавить достаточно большие функции но даже уже появился такой анти паттерн который называется монолит лямбда когда в одну лямда функцию добавляют там чуть ли не целый фреймворк типа спринга либо экспресса и на нем пытаются реализовать практически полностью приложения наверное лучше использовать много функций поменьше они лучше будут масштабироваться они будут быстрее исполняться и принципе это будет гораздо лучше дальше мы уже сегодня говорили про cold start и но с другой стороны мы знаем что инфраструктура для каждой из ваших лямда функций она умирает не сразу после того как функция закончила свое выполнение а какое-то еще время живет понятно что это время нигде не декларированы и принципе на него прям полагаться не надо сильно но например за кэшировать какие-то данные и переиспользовать их пури повторных вызовах функций это в принципе можно сделать и соответственно ускорить повторные обработку определенных данных соответственно но опять же нужно помнить что это нигде не регламентирована и прям полагаться на то что если там допустим в темп директорию сохраним в кэш какие-то данные то при последующем вызове функции они там будут ну наверное не надо то есть если вам нужно было ему хранилища то используйте там дай нам о тебе очереди либо еще что то про ноги соответственно используйте подход struck число logging соответственно пишите не только там очевидные вещи но и предоставлять контекст предоставляйте то в какой функции произошел сбой при каких параметрах вызова чтобы дальше было попроще все это отлаживать и посмотреть дальше соответственно что еще используйте переменное окружение соответственно до через них можно сконфигурировать функции опять же под разные например варианта передавать туда как разные наборы параметров но не передавайте в перемен переменное окружение сенситив данные такие как логин и пароль и используйте для этого секрет менеджер используйте для этого параметр stor используйте для этого какие-то сторонние решения естественно с точки зрения безопасности старайтесь использовать а им роли и разграничивать по максимуму пор доступа к другим сервисам то есть не делайте доступ у всех ко всему понятно что это проще всего и быстрее и часто особенно на первых этапах так делают но старайтесь сразу с первого дня стараться хотя бы как-то ограничивать доступ и использовать роли если использовать какой-то messenger добавляйте d2 терплю чтобы обрабатывать ошибки до дальше соответственно с точки зрения опять же с кейлин га сегодня много раз говорил что даль дам да хорошо скейлится но не забывайте если вы для mta вызываете какие-то традиционные приложения которые может быть не так быстро и хорошо скейлится то умерьте аппетит лямбда функции ограничьте ее чтобы не завалить весь backend еще такой тоже очень важный аспект и котором часто забывают многих сервисов есть и те или иные лимиты многие из этих лимитов их можно увеличивать создав соответствующий ticket но часто просто про них забывают и вспоминают уже в тот момент а когда то что то у нас тут отвалилась больше не обрабатываются запросы адам и забыли лимит поднять какой-нибудь соответственно перед тем как уходить в продакшн ну хотя бы pari pari pari viewty ведь все эти лимиты убедитесь что вы ни в какой из них в ближайшее время не упретесь если есть сомнения то опять же обращайтесь к нам обращаетесь в поддержку мы поможем дни для больших событий также можно использовать дополнительные уровни поддержки такие как infrastructure and management когда наши специалисты дополнительно могут вам помочь посмотреть за тем как у вас происходит выход к новой версии либо там вы ожидаете ли на черную пятницу большое количество пользователей соответственно с этим мы тоже вам можем помочь на этом все спасибо большое за внимание готовы ответить на ваши вопросы ну спасибо большой роману дмитрию за доклад мне кажется наконец-то мы увидели что с можно использовать для построения полноценного production вот на правах членов хочу сказать сразу вопрос вообще насколько управляемые эти приложения получаются насколько пользователь действительно понимает что происходит с его сервер с кодом потому что по факту для него это черный ящик он отправил свои функции куда-то и они там исполняются охватит им не хватит ресурсов какие alert и какие даже борт и вот это все только это все удобно и насколько это все готово к полноценному production ну давайте я наверно скажу действительно если использовать просто лямда тонарма да это черный ящик но я уже в своем докладе тоже много говорил про логирование про tracing то есть естественно мы должны сразу разрабатывать свой сервер с приложения с учетом того что мы будем его использовать продакшене то и соответственно у нас будет какое-то логирование tracing его включать ну то есть по умолчанию для линды логе они сразу в cloudwatch впишутся но они пишутся то есть то что вы там систем out вы видите то и запишется то есть соответственно вам нужно подумать о какую информацию я хочу выводить записывать то есть как раз то что я говорил лучше предоставлять сразу побольше про предоставлять контекст понимает что это за функция с какими параметрами она была вызвана дальше да у меня в cloudwatch и я могу там построить дальше даже горды посмотреть плюс сейчас у нас появилась относительно недавно штука называется сервер лес линз в cloudwatch и которая как раз она больше заточена на мониторинг на понимание того что у вас происходит сервер с функциях кей пассивы тут у нас вопрос от тех кто смотрел спросил кстати отличный вопрос зачем вообще переезжали в ввс какие бизнес смотрели с этим закрывали почему в с вопрос наверно ко мне так как я понимаю я могу отвечать только со стороны подрядчика и догадываться почему бардов менеджмент принял такое такое решение я я думаю что на удивление хоть это авиакомпания low cost карьер с наибольшим количеством перевезенных пассажиров по прошлому году во вьетнаме ну на местных авиарейсов у них очень мало и штат пойти по моему я так вот штатном расписании не видел на человек 20 обслуживает они абсолютно все а идти и перрин перенос в amazon все инфраструктуры позволяет применять сервисы которые как бы управляемые они априори уже много функции решаются сами мама зону скажем rds когда вы применяете там автоматические апдейты автоматически бэкапы то есть кучу всего что можно снять с тех же самых живых сотрудников второе я думаю что применяя вот эти вот себе сервисные функции можно избавиться от необходимости постоянно закупать новое оборудование потому что компания в течение авиакомпания в течение последних семи лет очень сильно росла допустим 2011 году не было пять самолетов и каких-нибудь пять рейсов в день по прошлому году у них 400 перелетах в день эй там под 70 самолетов что это нужно обслуживать booking техобслуживания ну кошмарно много данных я думаю что вот именно переход в amazon он позволяет гибко на это реагировать нужно больше инфраструктуры просто покупаете больше виртуалок для всего чего угодно дмитрий это мне кажется уже достаточно полной и смотрящий в кори и ответ и угадали предположили вы абсолютно верно и дмитрий момент хотелось сказать что тут еще у дмитрий отличный вопрос не планирует ли ребята уезжать сервис на контейнер с учетом всех ограничений и мы предлагаем обсудить именно туда в дискуссионную зону в звонок в сумме мы все этой перенесем спасибо и удачи встретимся с вами скоро зрителя всем пасибо спасибо спасибо бакан"
}